<h2 id="dense-net-paper-summary">Dense Net paper summary</h2>

<p>Short summary of the Dense Net paper, which one the CVPR best paper award in 2017.</p>

<p><img src="https://Cedric-Perauer.github.io/DL_from_Foundations/images/densenet.png" alt="images" /></p>

<h3 id="what-did-they-try-to-accomplish-">What did they try to accomplish ?</h3>
<ul>
  <li>improve CNNs : fight the vanishing gradient problem, improve regularization, remove redundancy (redundant layers/neurons) existing in current CNNs (like ResNets), which in turn recudes the number of parameters</li>
</ul>

<h3 id="key-elements">Key elements</h3>

<h4 id="concat-and-dense-conectivity">Concat and Dense conectivity</h4>
<p>concatenating feature maps, instead of using the classic ResNet skip connection function :</p>

<p><img src="https://Cedric-Perauer.github.io/DL_from_Foundations/images/resnet_formula.png" alt="images" /></p>

<p><img src="https://Cedric-Perauer.github.io/DL_from_Foundations/images/resnet_net.png" alt="images" /></p>

<p>=&gt; <code class="highlighter-rouge">lth</code> layer is used as input to <code class="highlighter-rouge">(l+1)th</code> layer =&gt; <code class="highlighter-rouge">xl = Hl(xl-1)</code></p>

<ul>
  <li>Dense Nets concatenate feature maps of the same size, which means it has <code class="highlighter-rouge">L*(L+1)/(2)</code> connections instead of <code class="highlighter-rouge">L</code> in a normal network, where <code class="highlighter-rouge">L</code> is the number of layers. Consequently every Dense Net layer has access to the feature maps of the preceeding layers :</li>
</ul>

<p><img src="https://Cedric-Perauer.github.io/DL_from_Foundations/images/densenet_formula.png" alt="images" /></p>

<p>The activation function Hl is a composite function with 3x3 convolutions, Batch Norm and ReLu activations.</p>

<h4 id="pooling-transition-layers">Pooling /Transition Layers</h4>
<p>When the size of feature maps changes, concatenation is not viable. The network is divided into several Dense Blocks, in between those 2x2 average pooling with 1x1 conv filters and batch norm are applied forming a “transition layer”.</p>

<h4 id="growth-rate">Growth rate</h4>
<p>The growth rate <code class="highlighter-rouge">k</code> is a hyperparameter which regulates how much a layer contributes to the global state. If each composite function <code class="highlighter-rouge">Hl</code> produces <code class="highlighter-rouge">k</code> feature maps, the <code class="highlighter-rouge">lth</code> layer has <code class="highlighter-rouge">k0 + k * (l-1)</code> feature-maps, where <code class="highlighter-rouge">k0</code> is the number of channels in the input layer. It has to be noted that DenseNets use narrow layers, with <code class="highlighter-rouge">k=12</code>.</p>

<h4 id="bottleneck-layers">Bottleneck layers</h4>
<p>To reduce the amount of input channels (for compute efficiency), bottleneck layers are used with <code class="highlighter-rouge">1x1 convs</code> before the <code class="highlighter-rouge">3x3 convs</code> applied.</p>

<h4 id="compression">Compression</h4>

<p>Compression is used to reduce the number of feature maps at transition layers, if a dense block contains <code class="highlighter-rouge">m</code> feature maps, the transition layer will generate <code class="highlighter-rouge">a*m</code> feature maps,where <code class="highlighter-rouge">0 &lt; a &lt;= 1</code> with <code class="highlighter-rouge">a = 0.5</code> in most cases.</p>

<h4 id="implementation-details">Implementation Details</h4>

<ul>
  <li>Kaiming/He init. is used</li>
  <li>Zero padding is used @ each Dense block</li>
  <li>Global pooling after last Dense block, with Softmax activation</li>
  <li>3 Dense blocks are used with all datasets except for ImageNet</li>
  <li>Weight decay of 10e-4</li>
  <li>Nesterov momentum of 0.9</li>
  <li>ImageNet implementation uses 7x7 convs instead of 3x3</li>
</ul>

<h3 id="results-and-conclusion">Results and Conclusion</h3>

<p><img src="https://Cedric-Perauer.github.io/DL_from_Foundations/images/Dense_results.png" alt="images" /></p>

<ul>
  <li>Bottleneck impact decreases with depth of the network</li>
  <li>not the same regularization issues as with ResNets1</li>
  <li>Dense Net BC with 15.3 Million params outperforms much larger Fractal Net (comparable to ResNet-1001), with DenseNet having 90% fewer parameters</li>
  <li>a DenseNet with as much compute complexity (FLOPS) as ResNet-50 performs on par with  ResNet-101</li>
  <li>DenseNet with 0.8 Million parameters performs as good as ResNet with 10.2 Millon parameters</li>
  <li>Deep Supervision is achieved with a single classifier. This provides easier loss functions and doesn’t need a multi classifier (like Inception).</li>
  <li>The intuition behind the good performance of DenseNets : architecture style is similar to a ResNet trained with stochastic depth, that means redundant layers are dropped from the beginning allowing smaller Networks</li>
</ul>

<h3 id="references-that-are-interesting-to-follow">References that are interesting to follow</h3>
<ul>
  <li><a href="https://github.com/liuzhuang13/DenseNet">DenseNets Implentation Github</a></li>
  <li><a href="https://arxiv.org/pdf/1512.03385.pdf">ResNets paper</a></li>
  <li><a href="https://arxiv.org/abs/1605.07648">Fractal Nets paper</a></li>
  <li><a href="https://arxiv.org/pdf/1502.01852.pdf">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></li>
</ul>

