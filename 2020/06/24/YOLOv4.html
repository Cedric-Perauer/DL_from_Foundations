<h1 id="yolov4-optimal-speed-and-accuracy-of-object-detection-paper-summary">YOLOv4: Optimal Speed and Accuracy of Object Detection Paper Summary</h1>
<p>(Alexey Bochkovskiy,Chien-Yao Wang, Hong-Yuan Mark Liao)</p>

<h2 id="what-did-the-authors-want-to-achieve-">What did the authors want to achieve ?</h2>
<ul>
  <li>fast (real time) object detection, that can be trained on a GPU with 8-16 GB of VRAM</li>
  <li>a model that can be easily trained and used</li>
  <li>add state of the art methods for object detection, building on YOLOv3</li>
  <li>find a good model for both GPU and VPU implementation</li>
</ul>

<p><img src="https://Cedric-Perauer.github.io/DL_from_Foundations/images/yolov4.png" alt="images" /></p>

<h2 id="methods-used">Methods used</h2>

<h3 id="bag-of-freebies-methods-that-only-increase-training-runtime-and-not-inference">Bag of Freebies (methods that only increase training runtime and not inference)</h3>

<ul>
  <li>New Data Augmentation techniques are used, for photometric and geometric variability : 
=&gt; Random Erase, Cutout to leave part of image of certain value
=&gt; Dropout, Dropblock does the same with the net params</li>
  <li>Mixup : mult image augmentation</li>
  <li>Style Transfer GAN for texture stability
    <h4 id="dataset-bias-">Dataset Bias :</h4>
    <p>=&gt; focal loss, data imbalance between different classics
=&gt; one-hot hard representation 
=&gt; soft labels 
BBox regression :</p>
  </li>
  <li>MSE has x and y independent, also Anchors
=&gt; IoU loss =&gt; coverage and area are considered 
=&gt; scale invariant, not the case with traditional methods
=&gt; DIoU and CIoU loss</li>
</ul>

<h3 id="bag-of-specials-methods-that-only-have-a-small-impact-on-inference-speed-but-improve-accuracy-significantly">Bag of specials (methods that only have a small impact on inference speed, but improve accuracy significantly)</h3>

<ul>
  <li>only a small cost of compute during inference, improve accuracy :
    <ul>
      <li>enlarging receptive field : SPP, ASPP, RFB, SPP originates 		  from Spatial Pyramid Matching 
=&gt; extract bag of words features 
 	- SPP infeasible for FCN nets, as it outputs a 1D feature 	  vector =&gt; YOLOv3 concat of max pooling outputs with 		  kxk kernel size =&gt; larger receptive field of the backbone 		  =&gt; 2.7% higher AP50, 0.5 more compute necessary</li>
      <li>ASPP diff. to SPP : max pool of 3x3, dilation of k</li>
      <li>RFB : several dilated kxk convs =&gt; 7% more AP, 5.7% more compute
Attention module :</li>
    </ul>
  </li>
  <li>mainly channelwise and pointwise attention, SE and SAM modules, SAM with no extra cost on GPU =&gt; better for us 
Feature Integration :</li>
  <li>skip connections, hyper-column</li>
  <li>channelwise weighting on multi-scale with FPN methods :
    <ul>
      <li>SFAM, ASFF, BiFPN, …
Activation Functions :
        <ul>
          <li>Mish, Swish, (fully differentiable) PReLu, …
Post-processing :</li>
          <li>NMS “messy” with occlusions =&gt; DIoU NMS distance : center to BBox screening process</li>
          <li>NMS method not necessary in anchor free method</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="architecture-selection">Architecture Selection</h3>

<ul>
  <li>things to think about : a reference that is good for classification is not always good for object detection, due to the detector needing :
    <ul>
      <li>higher input size (for small objects)</li>
      <li>more layers (higher receptive field to cover larger input)</li>
      <li>more parameters (for greater capacity, to detect different sized objects)</li>
    </ul>
  </li>
</ul>

<p>=&gt; a detector needs a backbone with more 3x3 convs and more params</p>

<p><img src="https://Cedric-Perauer.github.io/DL_from_Foundations/images/yolov4comp.png" alt="images" /></p>

<p>Due to that, CSPDarknet53 seems to be the best choice in theory</p>

<p>The following improvements are done :</p>
<ul>
  <li>SPP additional module for larger receptive field (almost no runtime disadvantage)</li>
  <li>PANet path-aggregation neck as param aggregation method instead of FPN from YOLOv3</li>
  <li>YOLOv3 anchor based head is used</li>
  <li>DropBlock regularization method</li>
</ul>

<p>For single GPU training :</p>
<ul>
  <li>synchBN is not considered : goal is to run on single GPU, thanks guys !!</li>
  <li>new data augmentation mosaic (mixes 4 training images), Self Adversarial Training =&gt; detection of objects outside their context<br />
SAT : 
Forward Backward Training : 
1) adversarial attack is performed on input
2) neural net is trained to detect and object on this moded image in a normal way</li>
</ul>

<p>Also :</p>
<ul>
  <li>optimal hyper-params while applying genetic algos</li>
  <li>Cross mini Batch Normalization : mini-batch split within batch</li>
  <li>SAM is modified from spatial-wise to pointwise attention, as can be seen below :</li>
</ul>

<p><img src="https://Cedric-Perauer.github.io/DL_from_Foundations/images/Pan_mod.png" alt="images" /></p>

<h3 id="architecture-summary">Architecture Summary</h3>
<ul>
  <li>Backbone : CSPDarknet53</li>
  <li>Neck : SPP, PAN</li>
  <li>Head : Yolov3</li>
</ul>

<p>Techniques :</p>
<ul>
  <li>Bag of freebies for Backbone : 
CutMix and Mosaic data augmentation, DropBlock reg., class label smoothing</li>
  <li>Bag of freebies for detector : 
CIoU-loss, CmBN, DropBlock reg., Mosaic data augmentation, SAT, Eliminate gird sensitivity, multi anchors for a single ground truth, cosine annealing, hyperparams, random training shapes</li>
  <li>Bag of Specials for Backbone : 
Mish activation, Cross-stage partial connections (CSP), Multi-
input weighted residual connections (MiWRC)</li>
  <li>Bag of specials for detector : 
Mish act., SPP-block, SAM-block, PAN path-aggregation block, DIoU-NMS</li>
</ul>

<h3 id="results">Results</h3>

<p>A FPS /mAP (@Iou50) comparison to other detecors can be seen below :</p>

<p><img src="https://Cedric-Perauer.github.io/DL_from_Foundations/images/Yolo_comp.png" alt="images" /></p>
