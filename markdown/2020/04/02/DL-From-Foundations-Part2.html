<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Fastai DL from the Foundations Softmax, NLL, Training, Module, Dataloader, Sampler, Callbacks | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Fastai DL from the Foundations Softmax, NLL, Training, Module, Dataloader, Sampler, Callbacks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Summaries for Building Fastai Funtions from scratch (Lesson 2)" />
<meta property="og:description" content="Summaries for Building Fastai Funtions from scratch (Lesson 2)" />
<link rel="canonical" href="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/02/DL-From-Foundations-Part2.html" />
<meta property="og:url" content="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/02/DL-From-Foundations-Part2.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-02T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-04-02T00:00:00-05:00","dateModified":"2020-04-02T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/02/DL-From-Foundations-Part2.html"},"description":"Summaries for Building Fastai Funtions from scratch (Lesson 2)","@type":"BlogPosting","url":"https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/02/DL-From-Foundations-Part2.html","headline":"Fastai DL from the Foundations Softmax, NLL, Training, Module, Dataloader, Sampler, Callbacks","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/DL_from_Foundations/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://cedric-perauer.github.io/DL_from_Foundations/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/DL_from_Foundations/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Fastai DL from the Foundations Softmax, NLL, Training, Module, Dataloader, Sampler, Callbacks | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Fastai DL from the Foundations Softmax, NLL, Training, Module, Dataloader, Sampler, Callbacks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Summaries for Building Fastai Funtions from scratch (Lesson 2)" />
<meta property="og:description" content="Summaries for Building Fastai Funtions from scratch (Lesson 2)" />
<link rel="canonical" href="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/02/DL-From-Foundations-Part2.html" />
<meta property="og:url" content="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/02/DL-From-Foundations-Part2.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-02T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-04-02T00:00:00-05:00","dateModified":"2020-04-02T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/02/DL-From-Foundations-Part2.html"},"description":"Summaries for Building Fastai Funtions from scratch (Lesson 2)","@type":"BlogPosting","url":"https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/02/DL-From-Foundations-Part2.html","headline":"Fastai DL from the Foundations Softmax, NLL, Training, Module, Dataloader, Sampler, Callbacks","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://cedric-perauer.github.io/DL_from_Foundations/feed.xml" title="fastpages" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/DL_from_Foundations/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/DL_from_Foundations/search/">Search</a><a class="page-link" href="/DL_from_Foundations/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Fastai DL from the Foundations Softmax, NLL, Training, Module, Dataloader, Sampler, Callbacks</h1><p class="page-description">Summaries for Building Fastai Funtions from scratch (Lesson 2)</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-02T00:00:00-05:00" itemprop="datePublished">
        Apr 2, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      15 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/DL_from_Foundations/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#lesson-2-how-to-train-your-model">Lesson 2 How to train your model</a>
<ul>
<li class="toc-entry toc-h2"><a href="#loss">Loss</a></li>
<li class="toc-entry toc-h2"><a href="#training-loop">Training Loop</a></li>
<li class="toc-entry toc-h2"><a href="#module-and-improved-training-loop">Module and improved training loop</a></li>
<li class="toc-entry toc-h2"><a href="#optimizer">Optimizer</a></li>
<li class="toc-entry toc-h2"><a href="#dataset-and-dataloader">Dataset and DataLoader</a>
<ul>
<li class="toc-entry toc-h3"><a href="#dataset">Dataset</a></li>
<li class="toc-entry toc-h3"><a href="#dataloader">DataLoader</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#sampler">Sampler</a></li>
<li class="toc-entry toc-h2"><a href="#with-val-set">With Val Set</a></li>
<li class="toc-entry toc-h2"><a href="#powerful-training-loops-with-callbacks">Powerful Training Loops with Callbacks</a>
<ul>
<li class="toc-entry toc-h3"><a href="#databunch">Databunch</a></li>
<li class="toc-entry toc-h3"><a href="#add-callbacks">add Callbacks</a></li>
<li class="toc-entry toc-h3"><a href="#callback-hander">Callback Hander</a></li>
<li class="toc-entry toc-h3"><a href="#callback-test">Callback Test</a></li>
<li class="toc-entry toc-h3"><a href="#callback-simplification">Callback Simplification</a></li>
<li class="toc-entry toc-h3"><a href="#annealing">Annealing</a></li>
<li class="toc-entry toc-h3"><a href="#other-scheduler-funcs">other scheduler funcs</a></li>
</ul>
</li>
</ul>
</li>
</ul><h1 id="lesson-2-how-to-train-your-model">
<a class="anchor" href="#lesson-2-how-to-train-your-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lesson 2 How to train your model</h1>

<h2 id="loss">
<a class="anchor" href="#loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loss</h2>

<p>For our Classification task, we will be using Cross Entropy Loss (also called Log Loss) ourselves. 
We define a simple Linear Model for our task. We will be using the Pytorch functions, as we already implemented those in Lesson 1 :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span> 
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">in_dim</span><span class="p">,</span><span class="n">nh</span><span class="p">,</span><span class="n">out_dim</span><span class="p">):</span> 
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span><span class="n">nh</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLu</span><span class="p">(),</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nh</span><span class="p">,</span><span class="n">out_dim</span><span class="p">)]</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span> 
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span> <span class="n">x</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>Since we are using Softmax, we need to compute it’s output first : 
<img src="https://Cedric-Perauer.github.io/DL_from_Foundations/images/Softmax.png" alt="images"></p>

<p>In practice we need it’s log, the code is simple :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">:</span> <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">/</span><span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)))</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
</code></pre></div></div>

<p>Using simple log-math</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>log(a/b) = log(a) - log(b)
</code></pre></div></div>
<p>Which leads to in pseudo code :</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>log(x.exp()) - log(x.exp().sum())
</code></pre></div></div>
<p>We can simplify this log_softmax function, like so :</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">:</span> <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
</code></pre></div></div>
<p><img src="https://Cedric-Perauer.github.io/DL_from_Foundations/images/CE_Loss.png" alt="images"></p>

<p>Using numpy <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing">integer array indexing</a> we can compute our negative log likelihood like so by passing our softmax output :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">negative_log_likelihood</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span><span class="n">target</span><span class="p">):</span> 
    <span class="k">return</span> <span class="o">-</span><span class="nb">input</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">target</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<p>However we can compute the log of the sum of exponentials in a more stable way to avoid an overflow of big activations, with the <a href="https://en.wikipedia.org/wiki/LogSumExp">LogSumExp trick</a> : 
<img src="https://Cedric-Perauer.github.io/DL_from_Foundations/images/Log_trick.png" alt="images"></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">logsumexp</span><span class="p">(</span><span class="n">exp</span><span class="p">):</span> 
    <span class="n">a</span><span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1">#maximum of x(j)  
</span>    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">a</span><span class="p">[:,</span><span class="bp">None</span><span class="p">])</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>
</code></pre></div></div>

<p>Updating our softmax again, this leads to :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>x.logsumexp() is the Pytorch function in this case. 
In order to compare our function with Pytorch, we can use</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_near</span><span class="p">(</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">pred</span><span class="p">),</span> <span class="n">pred</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<p>test_near will throw an AssertionError if they are not equal to each other.</p>

<p>Now we succesfully implemented <code class="highlighter-rouge">F.cross_entropy(pred,y_train)</code>, which is made out of <code class="highlighter-rouge">F.log_softmax</code> and <code class="highlighter-rouge">F.nll_loss</code></p>

<p>The accuracy can be calculated with :</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span> <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">==</span><span class="n">yb</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="training-loop">
<a class="anchor" href="#training-loop" aria-hidden="true"><span class="octicon octicon-link"></span></a>Training Loop</h2>

<p>Basically the training loop repeats over the following steps:</p>
<ul>
  <li>get the output of the model on a batch of inputs</li>
  <li>compare the output to the labels we have and compute a loss</li>
  <li>calculate the gradients of the loss with respect to every parameter of the model</li>
  <li>update said parameters with those gradients to make them a little bit better</li>
</ul>

<p>Now we can implement our Training Loop :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
<span class="c1">#       slice dataset in batches
</span>        <span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span><span class="o">*</span><span class="n">bs</span>
        <span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span><span class="o">+</span><span class="n">bs</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="s">'weight'</span><span class="p">):</span>
                    <span class="n">l</span><span class="o">.</span><span class="n">weight</span> <span class="o">-=</span> <span class="n">l</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
                    <span class="n">l</span><span class="o">.</span><span class="n">bias</span>   <span class="o">-=</span> <span class="n">l</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span>   <span class="o">*</span> <span class="n">lr</span>
                    <span class="n">l</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
                    <span class="n">l</span><span class="o">.</span><span class="n">bias</span>  <span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</code></pre></div></div>

<p>This looks kind of messy. Since our parameters can all be stored in a model class, we can loop over them and update them easily. However we need to implement a dummy Module first :</p>

<h2 id="module-and-improved-training-loop">
<a class="anchor" href="#module-and-improved-training-loop" aria-hidden="true"><span class="octicon octicon-link"></span></a>Module and improved training loop</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DummyModule</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_in</span><span class="p">,</span> <span class="n">nh</span><span class="p">,</span> <span class="n">n_out</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_in</span><span class="p">,</span><span class="n">nh</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nh</span><span class="p">,</span><span class="n">n_out</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">):</span> <span class="c1">#this is called everytime self is assigned 
</span>        <span class="k">if</span> <span class="ow">not</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">"_"</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span> <span class="c1"># just checks if it doesn't start with _ to avoid calling  python _modules recursively 
</span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__setattr__</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">v</span><span class="p">)</span> <span class="c1">#super class is python object 
</span>        
    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="n">f</span><span class="s">'{self._modules}'</span>
    
    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">l</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="k">yield</span> <span class="n">p</span>
</code></pre></div></div>

<p>for simplicity we can now use the Pytorch Module</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span> 
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">layers</span><span class="p">):</span> 
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span> <span class="c1">#initalizes self._modules 
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">layers</span> 
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="n">f</span><span class="s">'layer_{i}'</span><span class="p">,</span><span class="n">l</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">():</span> 
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span> <span class="n">x</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>now we can call the training more conveniently :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
<span class="c1">#       slice dataset in batches
</span>        <span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span><span class="o">*</span><span class="n">bs</span>
        <span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span><span class="o">+</span><span class="n">bs</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">param</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span>
</code></pre></div></div>

<p>We can make it even easier with <code class="highlighter-rouge">nn.ModuleList</code> to recreate <code class="highlighter-rouge">nn.Sequential</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequentialModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="p">:</span> <span class="n">x</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>Let’s replace our previous manually coded optimization step:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">p</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>
    <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>

<p>and instead use just:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="optimizer">
<a class="anchor" href="#optimizer" aria-hidden="true"><span class="octicon octicon-link"></span></a>Optimizer</h2>

<p>By creating our own Optimizer Function</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">),</span><span class="n">lr</span>
        
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span> <span class="n">p</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">lr</span>

    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</code></pre></div></div>

<p>PyTorch already provides optimizers, like optim.SGD and optim.Adam, which also handles more stuff.</p>

<p>Now we can further simplify our Training loop :</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="n">bs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">start_i</span> <span class="o">=</span> <span class="n">i</span><span class="o">*</span><span class="n">bs</span>
        <span class="n">end_i</span> <span class="o">=</span> <span class="n">start_i</span><span class="o">+</span><span class="n">bs</span>
        <span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>

        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>

<p>When implementing stuff yourself, it’s always good to put some tests in. Like checking the accuracy for example.</p>

<h2 id="dataset-and-dataloader">
<a class="anchor" href="#dataset-and-dataloader" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset and DataLoader</h2>

<h3 id="dataset">
<a class="anchor" href="#dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset</h3>

<p>We can further simplify this by converting</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xb</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
<span class="n">yb</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_i</span><span class="p">:</span><span class="n">end_i</span><span class="p">]</span>
</code></pre></div></div>

<p>with a Dataset Class :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataset</span><span class="p">():</span> 
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span>
    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">i</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">train_ds</span><span class="p">,</span><span class="n">valid_ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span><span class="n">Dataset</span><span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>

</code></pre></div></div>
<p>Now we can call our loop like this :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">//</span><span class="n">bs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span> 
        <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span>  <span class="n">train_ds</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">bs</span><span class="p">:</span><span class="n">i</span><span class="o">*</span><span class="n">bs</span><span class="o">+</span><span class="n">bs</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="dataloader">
<a class="anchor" href="#dataloader" aria-hidden="true"><span class="octicon octicon-link"></span></a>DataLoader</h3>

<p>We can make this even easier with a Dataloader to iterate over our Dataset automatically.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Dataloader</span> <span class="p">:</span> 
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">ds</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span> <span class="o">=</span> <span class="n">ds</span><span class="p">,</span><span class="n">bs</span>
    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ds</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">):</span> <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">ds</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">]</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">yield</code> is used, in order to produce a series of values over time. We can use the Dataloader with next and iter :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>

<span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">))</span>
</code></pre></div></div>

<p>now we can put our train loop in a wonderful function :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fit</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="sampler">
<a class="anchor" href="#sampler" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sampler</h2>

<p>In order to have a random order, we can implement a Sampler class :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sampler</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ds</span><span class="p">),</span><span class="n">bs</span><span class="p">,</span><span class="n">shuffle</span>
        
    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idxs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">):</span> <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">idxs</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">bs</span><span class="p">]</span>
</code></pre></div></div>

<p>Now we can change our DataLoader to include our sampler as an argument.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">collate</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
    <span class="n">xs</span><span class="p">,</span><span class="n">ys</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">DataLoader</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ds</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ds</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span> <span class="o">=</span> <span class="n">ds</span><span class="p">,</span><span class="n">sampler</span><span class="p">,</span><span class="n">collate_fn</span>
        
    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="p">:</span> <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">ds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">s</span><span class="p">])</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">collate</code> stacks tensors, we can also add padding, etc …
PyTorch does the same thing as well, but also adds <code class="highlighter-rouge">num_workers</code> which can be used to start several threads and run more efficiently  :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_samp</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate</span><span class="p">,</span><span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_ds</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">valid_samp</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate</span><span class="p">,</span><span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="with-val-set">
<a class="anchor" href="#with-val-set" aria-hidden="true"><span class="octicon octicon-link"></span></a>With Val Set</h2>

<p>Using best practices we should add a val set to store our best model and test during training :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># Handle batchnorm / dropout
</span>        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="c1">#         print(model.training)
</span>        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="n">model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="c1">#         print(model.training)
</span>        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">tot_loss</span><span class="p">,</span><span class="n">tot_acc</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span>
            <span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">:</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
                <span class="n">tot_loss</span> <span class="o">+=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
                <span class="n">tot_acc</span>  <span class="o">+=</span> <span class="n">accuracy</span> <span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">yb</span><span class="p">)</span>
        <span class="n">nv</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">tot_loss</span><span class="o">/</span><span class="n">nv</span><span class="p">,</span> <span class="n">tot_acc</span><span class="o">/</span><span class="n">nv</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tot_loss</span><span class="o">/</span><span class="n">nv</span><span class="p">,</span> <span class="n">tot_acc</span><span class="o">/</span><span class="n">nv</span>
</code></pre></div></div>

<p>I divided it in to a train and val Function for better readability :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">):</span> 
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span> <span class="p">:</span> 
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">val</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">):</span> 
    <span class="n">model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> 
        <span class="n">total_loss</span><span class="p">,</span> <span class="n">total_acc</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span>
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">:</span> 
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">)</span>
            <span class="n">total_acc</span> <span class="o">+=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span><span class="n">yb</span><span class="p">)</span>
    <span class="n">iterations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_dl</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">total_loss</span><span class="o">/</span><span class="n">iterations</span><span class="p">,</span> <span class="n">total_acc</span><span class="o">/</span><span class="n">iterations</span><span class="p">)</span>    
    <span class="k">return</span> <span class="n">total_loss</span><span class="p">,</span> <span class="n">total_acc</span><span class="p">,</span> <span class="n">iterations</span>

<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">loss_func</span><span class="p">,</span><span class="n">opt</span><span class="p">,</span><span class="n">train_dl</span><span class="p">,</span><span class="n">valid_dl</span><span class="p">):</span> 
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span> <span class="p">:</span> 
        <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss_func</span><span class="p">,</span><span class="n">opt</span><span class="p">,</span><span class="n">train_dl</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">,</span><span class="n">acc</span><span class="p">,</span> <span class="n">nv</span> <span class="o">=</span> <span class="n">val</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">loss_func</span><span class="p">,</span><span class="n">valid_dl</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">loss</span><span class="o">/</span><span class="p">(</span><span class="n">nv</span><span class="p">),</span> <span class="n">acc</span><span class="o">/</span><span class="p">(</span><span class="n">nv</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="powerful-training-loops-with-callbacks">
<a class="anchor" href="#powerful-training-loops-with-callbacks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Powerful Training Loops with Callbacks</h2>

<p>In order to customize out training loop in many ways (regularization techniques, visualization, early stopping,…), we want to be able to do so easily without having to write a huge loop function all the time that is hard to read and update. For this fastai uses something called callbacks :</p>

<p><img src="images/callbacks.png" alt=""></p>

<h3 id="databunch">
<a class="anchor" href="#databunch" aria-hidden="true"><span class="octicon octicon-link"></span></a>Databunch</h3>
<p>This can be used to store our info and doesn’t have any real logic.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DataBunch</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">train_dl</span><span class="p">,</span><span class="n">valid_dl</span><span class="p">,</span><span class="n">c</span>
        
    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">train_ds</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_dl</span><span class="o">.</span><span class="n">dataset</span>
        
    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">valid_ds</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_dl</span><span class="o">.</span><span class="n">dataset</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">DataBunch</span><span class="p">(</span><span class="o">*</span><span class="n">get_dls</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">),</span> <span class="n">c</span><span class="p">)</span> <span class="c1">#c is max y value 
</span>
<span class="c1">#further storage wrappers
</span><span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">nh</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">train_ds</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">nh</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nh</span><span class="p">,</span><span class="n">data</span><span class="o">.</span><span class="n">c</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Learner</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_func</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span><span class="n">opt</span><span class="p">,</span><span class="n">loss_func</span><span class="p">,</span><span class="n">data</span>

</code></pre></div></div>
<p>With our previous training loop modified a bit we can add Callbacks now,  :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">one_batch</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">cb</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">cb</span><span class="o">.</span><span class="n">begin_batch</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span><span class="n">yb</span><span class="p">):</span> <span class="k">return</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">cb</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">loss_func</span><span class="p">(</span><span class="n">cb</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">cb</span><span class="o">.</span><span class="n">after_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span> <span class="k">return</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">cb</span><span class="o">.</span><span class="n">after_backward</span><span class="p">():</span> <span class="n">cb</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">cb</span><span class="o">.</span><span class="n">after_step</span><span class="p">():</span> <span class="n">cb</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">all_batches</span><span class="p">(</span><span class="n">dl</span><span class="p">,</span> <span class="n">cb</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="n">dl</span><span class="p">:</span>
        <span class="n">one_batch</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">,</span> <span class="n">cb</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">cb</span><span class="o">.</span><span class="n">do_stop</span><span class="p">():</span> <span class="k">return</span>

<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">learn</span><span class="p">,</span> <span class="n">cb</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">cb</span><span class="o">.</span><span class="n">begin_fit</span><span class="p">(</span><span class="n">learn</span><span class="p">):</span> <span class="k">return</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">cb</span><span class="o">.</span><span class="n">begin_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span> <span class="k">continue</span>
        <span class="n">all_batches</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">cb</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">cb</span><span class="o">.</span><span class="n">begin_validate</span><span class="p">():</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="n">all_batches</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">valid_dl</span><span class="p">,</span> <span class="n">cb</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">cb</span><span class="o">.</span><span class="n">do_stop</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">cb</span><span class="o">.</span><span class="n">after_epoch</span><span class="p">():</span> <span class="k">break</span>
    <span class="n">cb</span><span class="o">.</span><span class="n">after_fit</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="add-callbacks">
<a class="anchor" href="#add-callbacks" aria-hidden="true"><span class="octicon octicon-link"></span></a>add Callbacks</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Callback</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">begin_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learn</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn</span> <span class="o">=</span> <span class="n">learn</span>
        <span class="k">return</span> <span class="bp">True</span>
    <span class="k">def</span> <span class="nf">after_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">True</span>
    <span class="k">def</span> <span class="nf">begin_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span>
        <span class="k">return</span> <span class="bp">True</span>
    <span class="k">def</span> <span class="nf">begin_validate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">True</span>
    <span class="k">def</span> <span class="nf">after_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">True</span>
    <span class="k">def</span> <span class="nf">begin_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">xb</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">yb</span> <span class="o">=</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span>
        <span class="k">return</span> <span class="bp">True</span>
    <span class="k">def</span> <span class="nf">after_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="k">return</span> <span class="bp">True</span>
    <span class="k">def</span> <span class="nf">after_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">True</span>
    <span class="k">def</span> <span class="nf">after_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">True</span> 
</code></pre></div></div>

<h3 id="callback-hander">
<a class="anchor" href="#callback-hander" aria-hidden="true"><span class="octicon octicon-link"></span></a>Callback Hander</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CallbackHandler</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">cbs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cbs</span> <span class="o">=</span> <span class="n">cbs</span> <span class="k">if</span> <span class="n">cbs</span> <span class="k">else</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">begin_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learn</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">in_train</span> <span class="o">=</span> <span class="n">learn</span><span class="p">,</span><span class="bp">True</span>
        <span class="n">learn</span><span class="o">.</span><span class="n">stop</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">cb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cbs</span><span class="p">:</span> <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="ow">and</span> <span class="n">cb</span><span class="o">.</span><span class="n">begin_fit</span><span class="p">(</span><span class="n">learn</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">after_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_train</span>
        <span class="k">for</span> <span class="n">cb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cbs</span><span class="p">:</span> <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="ow">and</span> <span class="n">cb</span><span class="o">.</span><span class="n">after_fit</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">res</span>
    
    <span class="k">def</span> <span class="nf">begin_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_train</span><span class="o">=</span><span class="bp">True</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">cb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cbs</span><span class="p">:</span> <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="ow">and</span> <span class="n">cb</span><span class="o">.</span><span class="n">begin_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">begin_validate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_train</span><span class="o">=</span><span class="bp">False</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">cb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cbs</span><span class="p">:</span> <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="ow">and</span> <span class="n">cb</span><span class="o">.</span><span class="n">begin_validate</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">after_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">cb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cbs</span><span class="p">:</span> <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="ow">and</span> <span class="n">cb</span><span class="o">.</span><span class="n">after_epoch</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">res</span>
    
    <span class="k">def</span> <span class="nf">begin_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">cb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cbs</span><span class="p">:</span> <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="ow">and</span> <span class="n">cb</span><span class="o">.</span><span class="n">begin_batch</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">after_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_train</span>
        <span class="k">for</span> <span class="n">cb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cbs</span><span class="p">:</span> <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="ow">and</span> <span class="n">cb</span><span class="o">.</span><span class="n">after_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">after_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">cb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cbs</span><span class="p">:</span> <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="ow">and</span> <span class="n">cb</span><span class="o">.</span><span class="n">after_backward</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="k">def</span> <span class="nf">after_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">for</span> <span class="n">cb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cbs</span><span class="p">:</span> <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="ow">and</span> <span class="n">cb</span><span class="o">.</span><span class="n">after_step</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">res</span>
    
    <span class="k">def</span> <span class="nf">do_stop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">stop</span>
        <span class="k">finally</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">stop</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div>

<h3 id="callback-test">
<a class="anchor" href="#callback-test" aria-hidden="true"><span class="octicon octicon-link"></span></a>Callback Test</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TestCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">begin_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">learn</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">begin_fit</span><span class="p">(</span><span class="n">learn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iters</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="bp">True</span>
        
    <span class="k">def</span> <span class="nf">after_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iters</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iters</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iters</span><span class="o">&gt;=</span><span class="mi">10</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">stop</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">return</span> <span class="bp">True</span>
</code></pre></div></div>

<p>These methods are checked for in our <code class="highlighter-rouge">one_batch</code> function and are executed in our training loop.</p>

<h3 id="callback-simplification">
<a class="anchor" href="#callback-simplification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Callback Simplification</h3>

<p>We do on not need to implement each function seperately by using a <strong>call</strong> function :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cb_name</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">cb</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cbs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">_order</span><span class="p">):</span>
            <span class="n">f</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">cb</span><span class="p">,</span> <span class="n">cb_name</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">f</span> <span class="ow">and</span> <span class="n">f</span><span class="p">():</span> <span class="k">return</span> <span class="bp">True</span>
        <span class="k">return</span> <span class="bp">False</span>
</code></pre></div></div>

<p>This allows is to recreate the Calbacks in a better way :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#export
</span><span class="kn">import</span> <span class="nn">re</span>

<span class="n">_camel_re1</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="s">'(.)([A-Z][a-z]+)'</span><span class="p">)</span>
<span class="n">_camel_re2</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="s">'([a-z0-9])([A-Z])'</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">camel2snake</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
    <span class="n">s1</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">_camel_re1</span><span class="p">,</span> <span class="s">r'\1_\2'</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">_camel_re2</span><span class="p">,</span> <span class="s">r'\1_\2'</span><span class="p">,</span> <span class="n">s1</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">Callback</span><span class="p">():</span>
    <span class="n">_order</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">def</span> <span class="nf">set_runner</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="o">=</span><span class="n">run</span>
    <span class="k">def</span> <span class="nf">__getattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span> <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">r'Callback$'</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">camel2snake</span><span class="p">(</span><span class="n">name</span> <span class="ow">or</span> <span class="s">'callback'</span><span class="p">)</span>

<span class="c1">#export
</span><span class="k">class</span> <span class="nc">TrainEvalCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">begin_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">n_epochs</span><span class="o">=</span><span class="mf">0.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">n_iter</span><span class="o">=</span><span class="mi">0</span>
    
    <span class="k">def</span> <span class="nf">after_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_train</span><span class="p">:</span> <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">n_epochs</span> <span class="o">+=</span> <span class="mf">1.</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">iters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">n_iter</span>   <span class="o">+=</span> <span class="mi">1</span>
        
    <span class="k">def</span> <span class="nf">begin_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">n_epochs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">in_train</span><span class="o">=</span><span class="bp">True</span>

    <span class="k">def</span> <span class="nf">begin_validate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="o">.</span><span class="n">in_train</span><span class="o">=</span><span class="bp">False</span>

<span class="k">class</span> <span class="nc">TestCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">after_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_eval</span><span class="o">.</span><span class="n">n_iters</span><span class="o">&gt;=</span><span class="mi">10</span><span class="p">:</span> <span class="k">return</span> <span class="bp">True</span>

<span class="c1">#export
</span><span class="k">class</span> <span class="nc">Runner</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cbs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">cb_funcs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">cbs</span> <span class="o">=</span> <span class="n">listify</span><span class="p">(</span><span class="n">cbs</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">cbf</span> <span class="ow">in</span> <span class="n">listify</span><span class="p">(</span><span class="n">cb_funcs</span><span class="p">):</span>
            <span class="n">cb</span> <span class="o">=</span> <span class="n">cbf</span><span class="p">()</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cb</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">cb</span><span class="p">)</span>
            <span class="n">cbs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cb</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">cbs</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,[</span><span class="n">TrainEvalCallback</span><span class="p">()]</span><span class="o">+</span><span class="n">cbs</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">opt</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">opt</span>
    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span>
    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">loss_func</span>
    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="o">.</span><span class="n">data</span>

    <span class="k">def</span> <span class="nf">one_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">xb</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">yb</span> <span class="o">=</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">(</span><span class="s">'begin_batch'</span><span class="p">):</span> <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xb</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">(</span><span class="s">'after_pred'</span><span class="p">):</span> <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pred</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">yb</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">(</span><span class="s">'after_loss'</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_train</span><span class="p">:</span> <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">(</span><span class="s">'after_backward'</span><span class="p">):</span> <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">(</span><span class="s">'after_step'</span><span class="p">):</span> <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">all_batches</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dl</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iters</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dl</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">xb</span><span class="p">,</span><span class="n">yb</span> <span class="ow">in</span> <span class="n">dl</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">:</span> <span class="k">break</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">one_batch</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">(</span><span class="s">'after_batch'</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="o">=</span><span class="bp">False</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">learn</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">learn</span> <span class="o">=</span> <span class="n">epochs</span><span class="p">,</span><span class="n">learn</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">cb</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cbs</span><span class="p">:</span> <span class="n">cb</span><span class="o">.</span><span class="n">set_runner</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">(</span><span class="s">'begin_fit'</span><span class="p">):</span> <span class="k">return</span>
            <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="n">epoch</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">(</span><span class="s">'begin_epoch'</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_batches</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">train_dl</span><span class="p">)</span>

                <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> 
                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">(</span><span class="s">'begin_validate'</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_batches</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">valid_dl</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="p">(</span><span class="s">'after_epoch'</span><span class="p">):</span> <span class="k">break</span>
            
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">(</span><span class="s">'after_fit'</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learn</span> <span class="o">=</span> <span class="bp">None</span>
</code></pre></div></div>

<p>In order to track our stats we will add a third Callback to see them, it will make us of a class that computes these.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AvgStats</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">in_train</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">in_train</span> <span class="o">=</span> <span class="n">listify</span><span class="p">(</span><span class="n">metrics</span><span class="p">),</span><span class="n">in_train</span>
    
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tot_loss</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span><span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tot_mets</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>
        
    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">all_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tot_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">tot_mets</span>
    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">avg_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="p">[</span><span class="n">o</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_stats</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span><span class="p">:</span> <span class="k">return</span> <span class="s">""</span>
        <span class="k">return</span> <span class="n">f</span><span class="s">"{'train' if self.in_train else 'valid'}: {self.avg_stats}"</span>

    <span class="k">def</span> <span class="nf">accumulate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run</span><span class="p">):</span>
        <span class="n">bn</span> <span class="o">=</span> <span class="n">run</span><span class="o">.</span><span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tot_loss</span> <span class="o">+=</span> <span class="n">run</span><span class="o">.</span><span class="n">loss</span> <span class="o">*</span> <span class="n">bn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">+=</span> <span class="n">bn</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tot_mets</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">m</span><span class="p">(</span><span class="n">run</span><span class="o">.</span><span class="n">pred</span><span class="p">,</span> <span class="n">run</span><span class="o">.</span><span class="n">yb</span><span class="p">)</span> <span class="o">*</span> <span class="n">bn</span>

<span class="k">class</span> <span class="nc">AvgStatsCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_stats</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_stats</span> <span class="o">=</span> <span class="n">AvgStats</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span><span class="bp">True</span><span class="p">),</span><span class="n">AvgStats</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span><span class="bp">False</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">begin_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_stats</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">valid_stats</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">after_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">stats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_stats</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_train</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_stats</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="n">stats</span><span class="o">.</span><span class="n">accumulate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">run</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">after_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_stats</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_stats</span><span class="p">)</span>
</code></pre></div></div>

<p>As this Callback alread tracks all our Stats, we can easily cerate a new Callback to save the best model based on validation loss at a given epoch and introduce early stopping. This can be done by inheriting from <code class="highlighter-rouge">AvgStatsCallback</code> which already has handy <code class="highlighter-rouge">begin_epoch</code> and <code class="highlighter-rouge">after_loss</code> functions that we can use.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Early_save</span><span class="p">(</span><span class="n">AvgStatsCallback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span><span class="n">early_stopping_iter</span><span class="p">,</span><span class="n">loss_stop</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_stats</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_stats</span> <span class="o">=</span> <span class="n">AvgStats</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span><span class="bp">True</span><span class="p">),</span><span class="n">AvgStats</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lowest_val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">"inf"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cur_val_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">"inf"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_array</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_iter</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_stop</span> <span class="o">=</span> <span class="n">loss_stop</span>
           
    <span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cur_val_loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_val_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_stats</span><span class="o">.</span><span class="n">avg_stats</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_val_loss</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">lowest_val_loss</span> <span class="p">:</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">lowest_val_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cur_val_loss</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span><span class="s">"best_model.pt"</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Saving Model with val loss of :{:.4f}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lowest_val_loss</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">early_stopping</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cur_val_loss</span><span class="p">)</span> 
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_array</span><span class="o">.</span><span class="n">maxlen</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_array</span><span class="p">)):</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_array</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span> <span class="c1">#check diff between losses
</span>                <span class="n">diff</span> <span class="o">+=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_array</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_array</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">diff</span> <span class="o">/=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_array</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span><span class="p">(</span><span class="n">diff</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_stop</span><span class="p">):</span> 
                <span class="k">return</span> <span class="s">"stop"</span>
    
    <span class="k">def</span> <span class="nf">after_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_stats</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_stats</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_model</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">()</span><span class="o">==</span><span class="s">"stop"</span><span class="p">:</span> 
            <span class="k">return</span> <span class="bp">True</span>
        <span class="k">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">valid_stats</span><span class="o">.</span><span class="n">avg_stats</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        
</code></pre></div></div>

<p>It keeps track of the last 3 losses and will stop training if the loss difference is too small. It will also save a model that performs best on validation with the handy <code class="highlighter-rouge">torch.save</code> function.</p>

<p>The Class now also keeps track of the lowest validation loss overall and can save the best model based on validation loss. Early stopping was implemented by tracking the last n elements, with <code class="highlighter-rouge">n=early_stopping_iter</code> in this case. We are storing it in a deque data structures. The <code class="highlighter-rouge">early_stopping</code> function will return a string that will then lead to our <code class="highlighter-rouge">after_epoch</code> function returning True which will stop training, as we have :</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="bp">self</span><span class="p">(</span><span class="s">'after_epoch'</span><span class="p">):</span> 
        <span class="k">break</span>
</code></pre></div></div>
<p>in our training loop.</p>

<p>Now we can finally call our methods and start trainig.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="o">*</span><span class="n">get_model</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="n">stats</span> <span class="o">=</span> <span class="n">AvgStatsCallback</span><span class="p">([</span><span class="n">accuracy</span><span class="p">])</span>
<span class="n">run</span> <span class="o">=</span> <span class="n">Runner</span><span class="p">(</span><span class="n">cbs</span><span class="o">=</span><span class="n">stats</span><span class="p">)</span>
<span class="n">run</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">learn</span><span class="p">)</span>
<span class="n">loss</span><span class="p">,</span><span class="n">acc</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">valid_stats</span><span class="o">.</span><span class="n">avg_stats</span>
</code></pre></div></div>

<p>Using partial we can create a Function that can create a callback function :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="n">acc_cbf</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">AvgStatsCallback</span><span class="p">,</span><span class="n">accuracy</span><span class="p">)</span>
<span class="n">run</span> <span class="o">=</span> <span class="n">Runner</span><span class="p">(</span><span class="n">cb_funcs</span><span class="o">=</span><span class="n">acc_cbf</span><span class="p">)</span>
</code></pre></div></div>

<p>This way we can create Callback funcs easily, e.g. by using a list.</p>

<h3 id="annealing">
<a class="anchor" href="#annealing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Annealing</h3>

<p>We define two new callbacks: the Recorder to save track of the loss and our scheduled learning rate, and a ParamScheduler that can schedule any hyperparameter as long as it’s registered in the state_dict of the optimizer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Recorder</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">begin_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrs</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">=</span> <span class="p">[],[]</span>

    <span class="k">def</span> <span class="nf">after_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_train</span><span class="p">:</span> <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s">'lr'</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>        

    <span class="k">def</span> <span class="nf">plot_lr</span>  <span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lrs</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">plot_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">ParamScheduler</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="n">_order</span><span class="o">=</span><span class="mi">1</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pname</span><span class="p">,</span> <span class="n">sched_func</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">pname</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">sched_func</span> <span class="o">=</span> <span class="n">pname</span><span class="p">,</span><span class="n">sched_func</span>

    <span class="k">def</span> <span class="nf">set_param</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">pg</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pname</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sched_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_epochs</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">)</span>
            
    <span class="k">def</span> <span class="nf">begin_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> 
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_train</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">sched_lin</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_inner</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span> <span class="k">return</span> <span class="n">start</span> <span class="o">+</span> <span class="n">pos</span><span class="o">*</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">partial</span><span class="p">(</span><span class="n">_inner</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">annealer</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_inner</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span> <span class="k">return</span> <span class="n">partial</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_inner</span>

<span class="o">@</span><span class="n">annealer</span> 
<span class="k">def</span> <span class="nf">sched_lin</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span> <span class="k">return</span> <span class="n">start</span> <span class="o">+</span> <span class="n">pos</span><span class="o">*</span><span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span>

</code></pre></div></div>

<p>The Decorator annealer passes sched_lin in annealer and replaces sched_lin() definition with what annealer returns.</p>

<h3 id="other-scheduler-funcs">
<a class="anchor" href="#other-scheduler-funcs" aria-hidden="true"><span class="octicon octicon-link"></span></a>other scheduler funcs</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">annealer</span>
<span class="k">def</span> <span class="nf">sched_cos</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span> <span class="k">return</span> <span class="n">start</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">pos</span><span class="p">)))</span> <span class="o">*</span> <span class="p">(</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="o">@</span><span class="n">annealer</span>
<span class="k">def</span> <span class="nf">sched_no</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span>  <span class="k">return</span> <span class="n">start</span>
<span class="o">@</span><span class="n">annealer</span>
<span class="k">def</span> <span class="nf">sched_exp</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">pos</span><span class="p">):</span> <span class="k">return</span> <span class="n">start</span> <span class="o">*</span> <span class="p">(</span><span class="n">end</span><span class="o">/</span><span class="n">start</span><span class="p">)</span> <span class="o">**</span> <span class="n">pos</span>

<span class="k">def</span> <span class="nf">cos_1cycle_anneal</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">high</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">sched_cos</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">high</span><span class="p">),</span> <span class="n">sched_cos</span><span class="p">(</span><span class="n">high</span><span class="p">,</span> <span class="n">end</span><span class="p">)]</span>

<span class="c1">#This monkey-patch is there to be able to plot tensors
</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">=</span> <span class="nb">property</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</code></pre></div></div>

<p>plot them</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">annealings</span> <span class="o">=</span> <span class="s">"NO LINEAR COS EXP"</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>

<span class="n">fns</span> <span class="o">=</span> <span class="p">[</span><span class="n">sched_no</span><span class="p">,</span> <span class="n">sched_lin</span><span class="p">,</span> <span class="n">sched_cos</span><span class="p">,</span> <span class="n">sched_exp</span><span class="p">]</span>
<span class="k">for</span> <span class="n">fn</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">fns</span><span class="p">,</span> <span class="n">annealings</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">p</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>

<p>Combine Schedulers with a function :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">combine_scheds</span><span class="p">(</span><span class="n">pcts</span><span class="p">,</span> <span class="n">scheds</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">sum</span><span class="p">(</span><span class="n">pcts</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.</span>
    <span class="n">pcts</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">listify</span><span class="p">(</span><span class="n">pcts</span><span class="p">))</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="nb">all</span><span class="p">(</span><span class="n">pcts</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">pcts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pcts</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">_inner</span><span class="p">(</span><span class="n">pos</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">pos</span> <span class="o">&gt;=</span> <span class="n">pcts</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span>
        <span class="n">actual_pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">pos</span><span class="o">-</span><span class="n">pcts</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">pcts</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">pcts</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">scheds</span><span class="p">[</span><span class="n">idx</span><span class="p">](</span><span class="n">actual_pos</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_inner</span>

<span class="n">sched</span> <span class="o">=</span> <span class="n">combine_scheds</span><span class="p">([</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="n">sched_cos</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">),</span> <span class="n">sched_cos</span><span class="p">(</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)])</span> 
</code></pre></div></div>

<p>Here is an example: use 30% of the budget to go from 0.3 to 0.6 following a cosine, then the last 70% of the budget to go from 0.6 to 0.2, still following a cosine. Train for a long time @ high lr, then switch to lower lr with cosine 1 cycle schedules.</p>

<p>Now we can train with our Callbacks and cosine scheduling.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cbfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">Recorder</span><span class="p">,</span>
        <span class="n">partial</span><span class="p">(</span><span class="n">AvgStatsCallback</span><span class="p">,</span><span class="n">accuracy</span><span class="p">),</span>
        <span class="n">partial</span><span class="p">(</span><span class="n">ParamScheduler</span><span class="p">,</span> <span class="s">'lr'</span><span class="p">,</span> <span class="n">sched</span><span class="p">)]</span>
    
<span class="n">learn</span> <span class="o">=</span> <span class="n">create_learner</span><span class="p">(</span><span class="n">get_model_func</span><span class="p">(</span><span class="mf">0.3</span><span class="p">),</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="n">run</span> <span class="o">=</span> <span class="n">Runner</span><span class="p">(</span><span class="n">cb_funcs</span><span class="o">=</span><span class="n">cbfs</span><span class="p">)</span>

<span class="n">run</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">learn</span><span class="p">)</span>
</code></pre></div></div>

<p>These Callbacks can also be used to move compute to our GPU !</p>

  </div><a class="u-url" href="/DL_from_Foundations/markdown/2020/04/02/DL-From-Foundations-Part2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/DL_from_Foundations/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/DL_from_Foundations/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/DL_from_Foundations/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/Cedric-Perauer" title="Cedric-Perauer"><svg class="svg-icon grey"><use xlink:href="/DL_from_Foundations/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/DL_from_Foundations/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
