<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Fastai DL from the Foundations Lesson 1 | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Fastai DL from the Foundations Lesson 1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Summaries for Building Fastai Funtions from scratch" />
<meta property="og:description" content="Summaries for Building Fastai Funtions from scratch" />
<link rel="canonical" href="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/01/DL-From-Foundations-Part1.html" />
<meta property="og:url" content="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/01/DL-From-Foundations-Part1.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-01T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-04-01T00:00:00-05:00","dateModified":"2020-04-01T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/01/DL-From-Foundations-Part1.html"},"description":"Summaries for Building Fastai Funtions from scratch","@type":"BlogPosting","url":"https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/01/DL-From-Foundations-Part1.html","headline":"Fastai DL from the Foundations Lesson 1","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/DL_from_Foundations/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://cedric-perauer.github.io/DL_from_Foundations/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/DL_from_Foundations/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Fastai DL from the Foundations Lesson 1 | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Fastai DL from the Foundations Lesson 1" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Summaries for Building Fastai Funtions from scratch" />
<meta property="og:description" content="Summaries for Building Fastai Funtions from scratch" />
<link rel="canonical" href="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/01/DL-From-Foundations-Part1.html" />
<meta property="og:url" content="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/01/DL-From-Foundations-Part1.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-01T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-04-01T00:00:00-05:00","dateModified":"2020-04-01T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/01/DL-From-Foundations-Part1.html"},"description":"Summaries for Building Fastai Funtions from scratch","@type":"BlogPosting","url":"https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/01/DL-From-Foundations-Part1.html","headline":"Fastai DL from the Foundations Lesson 1","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://cedric-perauer.github.io/DL_from_Foundations/feed.xml" title="fastpages" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/DL_from_Foundations/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/DL_from_Foundations/about/">About Me</a><a class="page-link" href="/DL_from_Foundations/search/">Search</a><a class="page-link" href="/DL_from_Foundations/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Fastai DL from the Foundations Lesson 1</h1><p class="page-description">Summaries for Building Fastai Funtions from scratch</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-01T00:00:00-05:00" itemprop="datePublished">
        Apr 1, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/DL_from_Foundations/categories/#markdown">markdown</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#fastai-dl-from-the-foundations-lesson-1">Fastai DL from the Foundations Lesson 1</a>
<ul>
<li class="toc-entry toc-h5"><a href="#the-code-is-based-on-the-code-in-the-fastai-course-v3-check-out-their-repo-which-also-includes-part-1-which-is-realy-focused-on-the-practical-side-of-things-thank-you-to-jeremy-rachel-and-the-fastai-team-for-this-great-course">The Code is based on the code in the fastai course (v3), check out their repo which also includes part 1 which is realy focused on the practical side of things. Thank you to Jeremy, Rachel and the fastai Team for this great course.</a></li>
<li class="toc-entry toc-h2"><a href="#fundamentals">Fundamentals</a>
<ul>
<li class="toc-entry toc-h3"><a href="#vision">Vision</a></li>
<li class="toc-entry toc-h3"><a href="#nlp">NLP</a></li>
<li class="toc-entry toc-h3"><a href="#tabular-data">Tabular Data</a></li>
<li class="toc-entry toc-h3"><a href="#broadcasting">Broadcasting</a></li>
<li class="toc-entry toc-h3"><a href="#einstein-summation">Einstein Summation</a></li>
<li class="toc-entry toc-h3"><a href="#pytorch-op">Pytorch Op</a></li>
<li class="toc-entry toc-h3"><a href="#matmul-summary">Matmul Summary</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#now-lets-use-it-to-init-our-weights-and-code-relu">Now let’s use it to init our weights and code RELU</a>
<ul>
<li class="toc-entry toc-h3"><a href="#init">Init</a>
<ul>
<li class="toc-entry toc-h4"><a href="#kaiming-init-code-">Kaiming init code :</a></li>
<li class="toc-entry toc-h4"><a href="#relu-can-be-implemented-easily-it-clamps-values-below-0-and-is-linear-otherwise-">ReLu can be implemented easily, it clamps values below 0 and is linear otherwise :</a></li>
<li class="toc-entry toc-h4"><a href="#therfore-kaiming-init-with-relu-can-be-implemented-like-this-">Therfore Kaiming init with ReLU can be implemented like this :</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#warning-">Warning !!</a></li>
<li class="toc-entry toc-h3"><a href="#loss-function-mse">Loss Function MSE</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#gradient-and-backward-pass">Gradient and Backward Pass</a>
<ul>
<li class="toc-entry toc-h4"><a href="#lets-start-with-mse-">Let’s start with MSE :</a></li>
<li class="toc-entry toc-h3"><a href="#relu-">Relu :</a></li>
<li class="toc-entry toc-h3"><a href="#linear-layers">Linear Layers</a></li>
<li class="toc-entry toc-h3"><a href="#forward-and-backward-pass">Forward and Backward Pass</a></li>
<li class="toc-entry toc-h3"><a href="#in-order-to-check-our-results-we-can-use-pytorch--">In order to check our results, we can use Pytorch  :</a></li>
<li class="toc-entry toc-h3"><a href="#refactor">Refactor</a></li>
<li class="toc-entry toc-h3"><a href="#further-refactor">further Refactor</a></li>
<li class="toc-entry toc-h3"><a href="#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul><h1 id="fastai-dl-from-the-foundations-lesson-1">
<a class="anchor" href="#fastai-dl-from-the-foundations-lesson-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fastai DL from the Foundations Lesson 1</h1>
<p>The idea of this Repo is to manage and document the Code for the fastai Deep Learning from the foundations course and include Papers and Summaries of them when it is helpful to do so. The course focuses on building large components of the fastai library and Pytorch from scratch to allow deep understanding of the fastai and Pytorch frameworks, which enables the creation of own algorithms and makes debugging easier.</p>

<p><img src="https://github.com/Cedric-Perauer/Fastai_DL_from_the_foundations/blob/master/images/1_RwzUcBlGybc9YFBMyYCTWw.png" alt=""></p>

<h5 id="the-code-is-based-on-the-code-in-the-fastai-course-v3-check-out-their-repo-which-also-includes-part-1-which-is-realy-focused-on-the-practical-side-of-things-thank-you-to-jeremy-rachel-and-the-fastai-team-for-this-great-course">
<a class="anchor" href="#the-code-is-based-on-the-code-in-the-fastai-course-v3-check-out-their-repo-which-also-includes-part-1-which-is-realy-focused-on-the-practical-side-of-things-thank-you-to-jeremy-rachel-and-the-fastai-team-for-this-great-course" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Code is based on the code in the fastai course (v3), check out <a href="https://github.com/fastai/course-v3">their repo</a> which also includes part 1 which is realy focused on the practical side of things. Thank you to Jeremy, Rachel and the fastai Team for this great course.</h5>

<p>In order to understand the material of part 2, you should be familiar with the following concepts, depending on each category :</p>
<h2 id="fundamentals">
<a class="anchor" href="#fundamentals" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fundamentals</h2>
<ul>
  <li><a href="https://course.fast.ai/">Affine functions &amp; non-linearities</a></li>
  <li><a href="https://course.fast.ai/">Parameters &amp; activations</a></li>
  <li><a href="https://course.fast.ai/">Random weight initalization and transfer learning</a></li>
  <li><a href="https://course.fast.ai/videos/?lesson=5">Stochastic gradient descent, Momentum and ADAM (a combination of RMSprop and Momentum)</a></li>
  <li><a href="https://course.fast.ai/videos/?lesson=5">Regularization techniques, specifically batch norm, dropout, weight decay and data augmentation</a></li>
  <li>
<a href="https://course.fast.ai/videos/?lesson=4">Embeddings</a>
    <h3 id="vision">
<a class="anchor" href="#vision" aria-hidden="true"><span class="octicon octicon-link"></span></a>Vision</h3>
  </li>
  <li><a href="https://course.fast.ai/videos/?lesson=1">Image classification and Regression Lesson 1</a></li>
  <li><a href="https://course.fast.ai/videos/?lesson=2">Image classification and Regression Lesson 2</a></li>
  <li><a href="https://course.fast.ai/videos/?lesson=6">Conv Nets</a></li>
  <li><a href="https://course.fast.ai/videos/?lesson=7">Residual and dense blocks</a></li>
  <li><a href="https://course.fast.ai/videos/?lesson=3">Segmentation : U-Net</a></li>
  <li>
<a href="https://course.fast.ai/videos/?lesson=7">GANs</a>
    <h3 id="nlp">
<a class="anchor" href="#nlp" aria-hidden="true"><span class="octicon octicon-link"></span></a>NLP</h3>
  </li>
  <li>
<a href="https://course.fast.ai/videos/?lesson=4">Language models &amp; NLP</a>
    <h3 id="tabular-data">
<a class="anchor" href="#tabular-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tabular Data</h3>
  </li>
  <li><a href="https://course.fast.ai/videos/?lesson=4">Continious &amp; categorical variables</a></li>
  <li><a href="https://course.fast.ai/videos/?lesson=4">Collaborative filtering</a></li>
</ul>

<p>##  Lesson 1</p>

<p>As we already know DL is mainly based on Linear Algebra, so let’s implement some simple Matrix Multiplication !
 We already know that <code class="highlighter-rouge">np.matmul</code> can be used for this, bur let’s do it ourselves.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">ar</span><span class="p">,</span><span class="n">ac</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="c1"># n_rows * n_cols 
</span>    <span class="n">br</span><span class="p">,</span><span class="n">bc</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">assert</span> <span class="n">ac</span><span class="o">==</span><span class="n">br</span> <span class="c1">#check for right dimensions =&gt; output dim is ar,bc
</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ar</span><span class="p">,</span> <span class="n">bc</span><span class="p">)</span> 
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ar</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bc</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ac</span><span class="p">):</span> <span class="c1"># or br
</span>                <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">c</span>
 
</code></pre></div></div>
<p>This is a very simple and inefficient implementation, which runs in 572 ms on my CPU with matrix dimensions 5x784 multiplied by 784x10. As expected the output array has 5 rows, if we used MNIST (50k) rows onw forward pass would take more than an hour which is unacceptable.</p>

<p>To improve this we can pass the Code down to a lower level language (Pytorch uses <a href="https://pytorch.org/cppdocs/">ATen</a> a Tensor library for this). This can be done with elementwise multiplication (also works on Tensors with rank &gt; 1) :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">ar</span><span class="p">,</span><span class="n">ac</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">br</span><span class="p">,</span><span class="n">bc</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">assert</span> <span class="n">ac</span><span class="o">==</span><span class="n">br</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ar</span><span class="p">,</span> <span class="n">bc</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ar</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">bc</span><span class="p">):</span>
            <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[:,</span><span class="n">j</span><span class="p">])</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span> <span class="c1">#row by column 
</span>    <span class="k">return</span> <span class="n">c</span>
</code></pre></div></div>

<p><img src="images/frobenius.png" alt=""></p>

<p>This is essentially using the above formula and executing it in C code, with a runtime of : 802 µs ± 60.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each). Which is about 714 times faster than the first implementation ! Wooho we are done !</p>

<h3 id="broadcasting">
<a class="anchor" href="#broadcasting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Broadcasting</h3>

<p>Hold on not so fast ! We can still do better by removing the inner loop with Broadcasting. Broadcasting “broadcasts” the smaller array across the larger one, so they have compatible shapes, operations are vecorized so that loops are executed in C without any overhead. You can see the broadcasted version of a vector by calling :</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&lt;</span><span class="n">smaller_array</span><span class="o">&gt;.</span><span class="n">expand_as</span><span class="p">(</span><span class="o">&lt;</span><span class="n">larger_array</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div></div>
<p>after expansion you can call :</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&lt;</span><span class="n">smaller_array</span><span class="o">&gt;.</span><span class="n">storage</span><span class="p">()</span>
</code></pre></div></div>

<p>and you will see that no additional memory is needed. With this our matmul looks like this :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">ar</span><span class="p">,</span><span class="n">ac</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">br</span><span class="p">,</span><span class="n">bc</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">assert</span> <span class="n">ac</span><span class="o">==</span><span class="n">br</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ar</span><span class="p">,</span> <span class="n">bc</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ar</span><span class="p">):</span>
        <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>   <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span> <span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#unsqueeze is used to unsqueeze a to rank 2
</span>    <span class="k">return</span> <span class="n">c</span>
</code></pre></div></div>
<p>This code executes in 172 µs ± 14.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each). Which means we are 3325.81 faster than in the beginning, nice.</p>

<h3 id="einstein-summation">
<a class="anchor" href="#einstein-summation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Einstein Summation</h3>

<p>A compact representation to combine products and sums in a general way.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span> <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'ik,kj-&gt;ij'</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<p>This speeds up the code a little more (factor 3575 compared to the start), but more improvements can be made.</p>

<p>It uses this string “mini language” (sort of like regex) to specify the multiply, it is a little bit annoying and languages like Swift will hopefully allow us to get rid of this.</p>

<h3 id="pytorch-op">
<a class="anchor" href="#pytorch-op" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pytorch Op</h3>

<p>Pushes the code to BLAS, Hardware optimized code. We can not specify this with Pytorch, with Swift this could be optimized by the programmer more easily. A classic operation is the @ operator, it can do more than matmul (such as Batch wise, Tensor Reductions,…).</p>

<h3 id="matmul-summary">
<a class="anchor" href="#matmul-summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Matmul Summary</h3>

<table>
  <thead>
    <tr>
      <th>Algorithm</th>
      <th>Runtime on CPU</th>
      <th>Factor improvement</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Naive Loops</td>
      <td>572 ms</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Loops + elementwise row/column multiply</td>
      <td>802 µs</td>
      <td>714</td>
    </tr>
    <tr>
      <td>Brodacasting</td>
      <td>172 µs</td>
      <td>3326</td>
    </tr>
    <tr>
      <td>Einstein Summation</td>
      <td>160 µs</td>
      <td>3575</td>
    </tr>
    <tr>
      <td>Pytorch’s function (uses HW specific BLAS)</td>
      <td>86 µs</td>
      <td>6651</td>
    </tr>
  </tbody>
</table>

<h2 id="now-lets-use-it-to-init-our-weights-and-code-relu">
<a class="anchor" href="#now-lets-use-it-to-init-our-weights-and-code-relu" aria-hidden="true"><span class="octicon octicon-link"></span></a>Now let’s use it to init our weights and code RELU</h2>

<h3 id="init">
<a class="anchor" href="#init" aria-hidden="true"><span class="octicon octicon-link"></span></a>Init</h3>

<p>We create a 2layer Net, with a hidden layer of size 50.</p>

<p>m is the 2nd dimension size of our input.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nh</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">nh</span><span class="p">)</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nh</span><span class="p">)</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">nh</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">nh</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Randn gives us weights with mean 0 and std of 1. Just using random numbers the mean and std of our output vector will be way off. In order to avoid this we divide by sqrt(m), which will keep our output mean and std in bounds.</p>

<p>Another common initalization method is Xavier Initalization. Check out <a href="https://www.youtube.com/watch?v=gYpoJMlgyXA&amp;list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&amp;index=5">Andrej Karpathy’s lecture</a> (starting at about 45:00) for a good explanation 
Even more advanced methods like <a href="https://arxiv.org/abs/1901.09321">Fixup initalization</a> can be used. The authors of the paper are able to learn deep nets (up to 10k layers) as stable without normalization when using Fixup.</p>

<p>Problem : if the variance halves each layer, the activation will vanish after some time, leading to dead neurons.</p>

<p>Due to this the <a href="https://arxiv.org/abs/1502.01852">2015 ImageNet ResNet winners, see 2.2 in the paper</a> suggested this :
 Up to that point init was done with random weights from Gaussian distributions, which used fixed std deviations (for example 0.01). These methods however did not allow deeper models (more than 8 layers) to converge in most cases. Due to this, in the older days models like VGG16 had to train the first 8 layers at first, in order to then initalize the next ones. As we can imagine this takes longer to train, but also may lead to a poorer local optimum. Unfortunately the Xavier init paper does not talk about non-linarities, but should not be used with ReLu like functions, as the ReLu function will half the distribution (values smaller than zero are = 0) at every step. 
 <img src="images/Stanford.png" alt=""></p>

<p>Looking at the distributions in the plots, you can see that the rapid decrease of the std. deviation leads to ReLu neurons activating less and less.</p>

<p>The Kaiming init paper investigates the variance at each layer and ends up suggesting the following : 
<img src="images/resnet_init.png" alt="">
essentially it just adds the 2 in the numerator to avoid the halfing of the variance due at each step.</p>

<p>A direct comparison in the paper on a 22 layer model shows the benefit, even though Xavier converges as well, Kaiming init does so significantly faster. With a deeper 30-layer model the advantage of Kaiming is even more evident. 
<img src="images/He.png" alt=""></p>

<h4 id="kaiming-init-code-">
<a class="anchor" href="#kaiming-init-code-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Kaiming init code :</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">nh</span><span class="p">)</span><span class="o">*</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">m</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="relu-can-be-implemented-easily-it-clamps-values-below-0-and-is-linear-otherwise-">
<a class="anchor" href="#relu-can-be-implemented-easily-it-clamps-values-below-0-and-is-linear-otherwise-" aria-hidden="true"><span class="octicon octicon-link"></span></a>ReLu can be implemented easily, it clamps values below 0 and is linear otherwise :</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span>
</code></pre></div></div>

<p>Leaky ReLu avoids 0-ing the gradient by using a small negative slope below 0 (0.01 usually).</p>

<h4 id="therfore-kaiming-init-with-relu-can-be-implemented-like-this-">
<a class="anchor" href="#therfore-kaiming-init-with-relu-can-be-implemented-like-this-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Therfore Kaiming init with ReLU can be implemented like this :</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">nh</span><span class="p">)</span><span class="o">*</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="o">/</span><span class="n">m</span> <span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">lin</span><span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">))</span>
<span class="n">t1</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span><span class="n">t1</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="warning-">
<a class="anchor" href="#warning-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Warning !!</h3>
<p>The Pytorch source code for <code class="highlighter-rouge">torch.nn.Conv2d</code>
uses a kaiming init with :</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</code></pre></div></div>
<p>.sqrt(5) is mysterious and does not seem to be a good idea</p>

<h3 id="loss-function-mse">
<a class="anchor" href="#loss-function-mse" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loss Function MSE</h3>
<p>Now that we have done almost one forward pass,we still need to implement an error function. MSE Error, popular for regression tasks, can be implemented like this :</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">targ</span><span class="p">):</span> <span class="k">return</span> <span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">targ</span><span class="p">)</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>

<p>.squeeze() is used to get rid of a trainiling (,1) in this case.</p>

<h2 id="gradient-and-backward-pass">
<a class="anchor" href="#gradient-and-backward-pass" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient and Backward Pass</h2>
<p>Mathematically the Backward Pass uses the chain rule to compute all of the gradients. 
<img src="images/gradient.png" alt=""></p>

<p>In order to Backprop effectively, we need to calc the gradients of all of our components. In our case these are our loss, activation functions (only ReLu, which is easy) and our linear layers.</p>

<p>I suggest <a href="https://www.youtube.com/watch?v=i94OvYb6noo&amp;t=1472s">CS 231n by Andrej Karpathy</a> for mathematical explanation of Backprop.</p>

<h4 id="lets-start-with-mse-">
<a class="anchor" href="#lets-start-with-mse-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Let’s start with MSE :</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mse_grad</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">targ</span><span class="p">):</span> 
    <span class="c1"># grad of loss with respect to output of previous layer
</span>    <span class="n">inp</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">-</span> <span class="n">targ</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">inp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="relu-">
<a class="anchor" href="#relu-" aria-hidden="true"><span class="octicon octicon-link"></span></a>Relu :</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">relu_grad</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
    <span class="c1"># grad of relu with respect to input activations
</span>    <span class="n">inp</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="n">inp</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">g</span>
</code></pre></div></div>
<p>Very simple, the gradient is either 0 or 1. In the Leaky Relu Case it’s either -0.01 or 1.</p>

<h3 id="linear-layers">
<a class="anchor" href="#linear-layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Layers</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">lin_grad</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="c1"># grad of matmul with respect to input
</span>    <span class="n">inp</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">g</span> <span class="o">@</span> <span class="n">w</span><span class="o">.</span><span class="n">t</span><span class="p">()</span> <span class="c1">#matrix prod with the transpose 
</span>    <span class="n">w</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">b</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="forward-and-backward-pass">
<a class="anchor" href="#forward-and-backward-pass" aria-hidden="true"><span class="octicon octicon-link"></span></a>Forward and Backward Pass</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward_and_backward</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">targ</span><span class="p">):</span>
    <span class="c1"># forward pass:
</span>    <span class="n">l1</span> <span class="o">=</span> <span class="n">inp</span> <span class="o">@</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">l2</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">l1</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">l2</span> <span class="o">@</span> <span class="n">w2</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="c1"># we don't actually need the loss in backward!
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">targ</span><span class="p">)</span>
    
    <span class="c1"># backward pass, just reverse order:
</span>    <span class="n">mse_grad</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">targ</span><span class="p">)</span>
    <span class="n">lin_grad</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
    <span class="n">relu_grad</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">)</span>
    <span class="n">lin_grad</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="in-order-to-check-our-results-we-can-use-pytorch--">
<a class="anchor" href="#in-order-to-check-our-results-we-can-use-pytorch--" aria-hidden="true"><span class="octicon octicon-link"></span></a>In order to check our results, we can use Pytorch  :</h3>

<p>We can control our results with Pytorch auto_grad() function</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xt2</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">w12</span> <span class="o">=</span> <span class="n">w1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">w22</span> <span class="o">=</span> <span class="n">w2</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b12</span> <span class="o">=</span> <span class="n">b1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b22</span> <span class="o">=</span> <span class="n">b2</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>.requires_grad(True) turns a tensor in to an autograd so it can keep track of each step</p>

<h3 id="refactor">
<a class="anchor" href="#refactor" aria-hidden="true"><span class="octicon octicon-link"></span></a>Refactor</h3>

<p>It’s always good to refactor our code. This can be done by creating classes and using our functions. One for forward and one for backward pass.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Relu</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inp</span> <span class="o">=</span> <span class="n">inp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span><span class="o">-</span><span class="mf">0.5</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">inp</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inp</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">g</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Lin</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">w</span><span class="p">,</span><span class="n">b</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inp</span> <span class="o">=</span> <span class="n">inp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">inp</span><span class="o">@</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inp</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">g</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="c1"># Creating a giant outer product, just to sum it, is inefficient!
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inp</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Mse</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">targ</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inp</span> <span class="o">=</span> <span class="n">inp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">targ</span> <span class="o">=</span> <span class="n">targ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">-</span> <span class="n">targ</span><span class="p">)</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inp</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inp</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">targ</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">targ</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<p>Lastly we create a model class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Model</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">Lin</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span><span class="n">b1</span><span class="p">),</span> <span class="n">Relu</span><span class="p">(),</span> <span class="n">Lin</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span><span class="n">b2</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">Mse</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">targ</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span> <span class="n">x</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">targ</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span> <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>

<p>The execution times is to slow and we want to avoid the <strong>call</strong>() declarations so we define a module class</p>

<h3 id="further-refactor">
<a class="anchor" href="#further-refactor" aria-hidden="true"><span class="octicon octicon-link"></span></a>further Refactor</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Module</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">args</span> <span class="o">=</span> <span class="n">args</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="s">'not implemented'</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">bwd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Relu</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span> <span class="k">return</span> <span class="n">inp</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="mf">0.</span><span class="p">)</span><span class="o">-</span><span class="mf">0.5</span>
    <span class="k">def</span> <span class="nf">bwd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span> <span class="n">inp</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="n">inp</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="nb">float</span><span class="p">()</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">g</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Lin</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">w</span><span class="p">,</span><span class="n">b</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span> <span class="k">return</span> <span class="n">inp</span><span class="o">@</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
    
    <span class="k">def</span> <span class="nf">bwd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">inp</span><span class="p">):</span>
        <span class="n">inp</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">g</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"bi,bj-&gt;ij"</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">g</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Mse</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">targ</span><span class="p">):</span> <span class="k">return</span> <span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="o">-</span> <span class="n">targ</span><span class="p">)</span><span class="o">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">bwd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">targ</span><span class="p">):</span> <span class="n">inp</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">-</span><span class="n">targ</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">targ</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Model</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">Lin</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span><span class="n">b1</span><span class="p">),</span> <span class="n">Relu</span><span class="p">(),</span> <span class="n">Lin</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span><span class="n">b2</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">Mse</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">targ</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span> <span class="n">x</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">targ</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span> <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>

<p>Now we can call the forward and backprop passes for our model easily.</p>

<h3 id="summary">
<a class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h3>

<p>To summarize we implemented nn.Linear and nn.Module and will be able to write the train loop next lesson !</p>

  </div><a class="u-url" href="/DL_from_Foundations/markdown/2020/04/01/DL-From-Foundations-Part1.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/DL_from_Foundations/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/DL_from_Foundations/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/DL_from_Foundations/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/DL_from_Foundations/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/DL_from_Foundations/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
