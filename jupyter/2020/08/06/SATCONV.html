<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Stand-Alone Self-Attention in Vision Models Summary | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Stand-Alone Self-Attention in Vision Models Summary" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introducing Self-Attention as a stand-alone layer in Convolutional Architectures by Prajit Ramachandran,Niki Parmar,Ashish Vaswani,Jonathon Shlens,Anselm Levskaya,Irwan Bello" />
<meta property="og:description" content="Introducing Self-Attention as a stand-alone layer in Convolutional Architectures by Prajit Ramachandran,Niki Parmar,Ashish Vaswani,Jonathon Shlens,Anselm Levskaya,Irwan Bello" />
<link rel="canonical" href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html" />
<meta property="og:url" content="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:image" content="https://cedric-perauer.github.io/DL_from_Foundations/images/SATCONV.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-06T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"Stand-Alone Self-Attention in Vision Models Summary","dateModified":"2020-08-06T00:00:00-05:00","description":"Introducing Self-Attention as a stand-alone layer in Convolutional Architectures by Prajit Ramachandran,Niki Parmar,Ashish Vaswani,Jonathon Shlens,Anselm Levskaya,Irwan Bello","datePublished":"2020-08-06T00:00:00-05:00","@type":"BlogPosting","image":"https://cedric-perauer.github.io/DL_from_Foundations/images/SATCONV.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html"},"url":"https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/DL_from_Foundations/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://cedric-perauer.github.io/DL_from_Foundations/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/DL_from_Foundations/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Stand-Alone Self-Attention in Vision Models Summary | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Stand-Alone Self-Attention in Vision Models Summary" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introducing Self-Attention as a stand-alone layer in Convolutional Architectures by Prajit Ramachandran,Niki Parmar,Ashish Vaswani,Jonathon Shlens,Anselm Levskaya,Irwan Bello" />
<meta property="og:description" content="Introducing Self-Attention as a stand-alone layer in Convolutional Architectures by Prajit Ramachandran,Niki Parmar,Ashish Vaswani,Jonathon Shlens,Anselm Levskaya,Irwan Bello" />
<link rel="canonical" href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html" />
<meta property="og:url" content="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:image" content="https://cedric-perauer.github.io/DL_from_Foundations/images/SATCONV.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-06T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"Stand-Alone Self-Attention in Vision Models Summary","dateModified":"2020-08-06T00:00:00-05:00","description":"Introducing Self-Attention as a stand-alone layer in Convolutional Architectures by Prajit Ramachandran,Niki Parmar,Ashish Vaswani,Jonathon Shlens,Anselm Levskaya,Irwan Bello","datePublished":"2020-08-06T00:00:00-05:00","@type":"BlogPosting","image":"https://cedric-perauer.github.io/DL_from_Foundations/images/SATCONV.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html"},"url":"https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://cedric-perauer.github.io/DL_from_Foundations/feed.xml" title="fastpages" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/DL_from_Foundations/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/DL_from_Foundations/search/">Search</a><a class="page-link" href="/DL_from_Foundations/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Stand-Alone Self-Attention in Vision Models Summary</h1><p class="page-description">Introducing Self-Attention as a stand-alone layer in Convolutional Architectures by Prajit Ramachandran,Niki Parmar,Ashish Vaswani,Jonathon Shlens,Anselm Levskaya,Irwan Bello</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-08-06T00:00:00-05:00" itemprop="datePublished">
        Aug 6, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/DL_from_Foundations/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#What-did-the-authors-want-to-achieve-?">What did the authors want to achieve ? </a></li>
<li class="toc-entry toc-h2"><a href="#Methods">Methods </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Conv-Block-deficits">Conv Block deficits </a></li>
<li class="toc-entry toc-h3"><a href="#Self-Attention">Self-Attention </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Replacing-Spatial-Convolutions">Replacing Spatial Convolutions </a></li>
<li class="toc-entry toc-h4"><a href="#Replacing-the-Convolutional-Stem-(intial-layers-of-the-CNN)">Replacing the Convolutional Stem (intial layers of the CNN) </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Results">Results </a>
<ul>
<li class="toc-entry toc-h3"><a href="#ImageNet">ImageNet </a></li>
<li class="toc-entry toc-h3"><a href="#Coco-Object-Detection">Coco Object Detection </a></li>
<li class="toc-entry toc-h3"><a href="#Where-is-stand-alone-attention-most-useful-?">Where is stand-alone attention most useful ? </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Stem">Stem </a></li>
<li class="toc-entry toc-h4"><a href="#Full-Net">Full Net </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Which-attention-features-are-important-?">Which attention features are important ? </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-08-06-SATCONV.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://arxiv.org/pdf/1906.05909.pdf">Paper Link</a></p>
<h2 id="What-did-the-authors-want-to-achieve-?">
<a class="anchor" href="#What-did-the-authors-want-to-achieve-?" aria-hidden="true"><span class="octicon octicon-link"></span></a>What did the authors want to achieve ?<a class="anchor-link" href="#What-did-the-authors-want-to-achieve-?"> </a>
</h2>
<p>Combat the deficits of Conolutional Architectures, (local connectivity, failing to reason globally) by introducing Attention to as a stand-alone layer. The authors prove that this can be both more accurate and more efficient at the same time. Architectures that are attention only, and a mixed version of convolutional and attention architectures are introduced and compared to the vanilla convolutional implementations.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Methods">
<a class="anchor" href="#Methods" aria-hidden="true"><span class="octicon octicon-link"></span></a>Methods<a class="anchor-link" href="#Methods"> </a>
</h2>
<p><br><br></p>
<h3 id="Conv-Block-deficits">
<a class="anchor" href="#Conv-Block-deficits" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conv Block deficits<a class="anchor-link" href="#Conv-Block-deficits"> </a>
</h3>
<p>Capturing long range interactions is challenging for convolutions as they do not scale well with large receptives. Since attention has been used to tackle long range dependencies in sequence modeling, since architectures like SE Nets model attention on a chanel wise basis successfully. However in these cases attention was only an add-on to a traditional architecture style. In this paper the authors propose to use attention mechanisms as stand alone layers.</p>
<h3 id="Self-Attention">
<a class="anchor" href="#Self-Attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>Self-Attention<a class="anchor-link" href="#Self-Attention"> </a>
</h3>
<p>Attention was originally introduced in order to allow for summarization from variable length source sentences. Attention focuses on the important parts of the input and thereby can serve as a primary representation learning mechanism and fully replace recurrence. The word self means that it just considers a single context (query,keys and values are extracted from the same image). The breakthrough in this paper is the use of self-attention layers instead of convolutional layers.</p>
<p>In this work, already existing mechanisms are used which are not optimized for the image domain. Therefore it is permutation equivariant and has limited expression capability for vision tasks.
<br><br></p>
<p><img src="/DL_from_Foundations/images/copied_from_nb/images/sat_conv.png" alt=""> 
<br><br>
The process is as follows : 
1) given a pixel $x_{i,j} \in R^{d_{in}}$ in positions $ab \in N_{k}(i,j)$ a local kernel with kernel size $k$ is extracted. $x_{i,j}$ is the middle of the kernel, which is called <em>memory block</em>. Prior work only performed global attention, which can only be done with a downsized sample as it is very compute expensive. 
<br><br></p>
<p>2) Single headed attention is computed : 
<br><br>
$y_{i,j} = \sum_{ab \in N_{k}(i,j)}  softmax_{a,b}(q_{ij}^{T}k_{ab})v_{ab}$
<br><br>
where the ${queries}$ $q_{ij} = W_Q x_{ij}$<br>
<br><br>
${keys}$ $k_{ab} = W_K x_{ab}$
<br><br></p>
<p>and ${values}$ $v_{ab} = W_V x_{ab}$ 
<br><br></p>
<p>are linear transformations of the pixel in position and it's neighbors in the kernel. 
<br><br>
$\texttt{softmax}_{a b}$ is a softmax, which is applied to all logits computed in the neighborhood of $ij$. 
$W_Q, W_K, W_V \in \mathbb{R}^{d_{out} \times d_{in}}$ are learned transforms. 
<br><br></p>
<p>Local self-attention is similar to convolution in the way that it aggregates spatial information in the neighborhoods, multi attention heads are used to learn unique representations of the input. This is done by partitioning pixel features into $N$ groups and then computing single-headed attention on each one seperately with the transforms $W_Q, W_K, W_V \in \mathbb{R}^{d_{out} \times d_{in}}$. The outputs of the heads are then concatenated. 
<br><br>
2D relative pose embeddings,relative attention is used : 
<br><br></p>
<p>1) relative attention computes relative distances of the pixel to each one in the neighborhood : row $(a-i)$ and column offset $(b-j)$<br>
2) row and column offset are associated with an embedding and concatenated into a vector $r_{a-i,b-j}$</p>
<p>3) Spatial-relative attention is then defined as : 
<br><br></p>
<p>$y_{ij} = \sum_{a,b\in N_{k}(i, j)} softmax_{ab}(q_{ij}^{T}k_{ab}+q_{ij}^{T}r_{a-i,b-j})v_{ab}$</p>
<p><br><br>
The logit measuring similarity between the query and an element results from the content of the element and the relative distance of the element from the query. By inlcuding this spatial information, self-attention also has translation equivariance, just like conv layers. Unlike conv layers, self-attentions parameter count is independent of its spatial extent. 
The compute cost also grows slower : 
For example, if $d_{in} = d_{out} = 128$, a convolution layer with $k = 3$ has the same computational cost as an attention layer with $k = 19$.</p>
<p>Using this as their basis, a fully attentional architecture is created in two steps:</p>
<h4 id="Replacing-Spatial-Convolutions">
<a class="anchor" href="#Replacing-Spatial-Convolutions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Replacing Spatial Convolutions<a class="anchor-link" href="#Replacing-Spatial-Convolutions"> </a>
</h4>
<p>A spatial conv is defined to have spatial extent k &gt; 1, which also includes 1x1 convolutions. These can be viewed as fully connected layers. 
Here the authors want to replace conv blocks in a straightforward way, specificially focusing on ResNet. Therefore the 3x3 convolution in Path B is swapped with a self-attention layer as defined above. All the other blocks are not changed, this might be supobtimal but promises potential improvements using architecture search.</p>
<h4 id="Replacing-the-Convolutional-Stem-(intial-layers-of-the-CNN)">
<a class="anchor" href="#Replacing-the-Convolutional-Stem-(intial-layers-of-the-CNN)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Replacing the Convolutional Stem (intial layers of the CNN)<a class="anchor-link" href="#Replacing-the-Convolutional-Stem-(intial-layers-of-the-CNN)"> </a>
</h4>
<p>This part focuses on replacing the inital layers, as they are the most compute expensive due to the large input size of the image. In the OG ResNet the input is a 7x7 kernel with stride 2, followed by 3x3 max pooling with stride 2. At the beginning RGB pixels are individually uninformative and information is heavily spatially correlated through low level features such as edges. Edge detectors are difficult to learn for self-attention due to spatial correlation, convs learn these easily through distance based weight parameterization. The authors inject spatially-varying linear transforms into the pointwise 1x1 softmax convolution.</p>
<p>$\tilde{v}_{a b} = \left(\sum_m  p(a, b, m) W_V^{m}\right) x_{a b}$</p>
<p>The results is similar to convolutions, weights are learned based on a local neighborhood basis. So in total the stem consists of spatially aware value features, followed by max-pooling. A more detailed explanation of this can be found in the appendix of the paper (page 14/15).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Results">
<a class="anchor" href="#Results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results<a class="anchor-link" href="#Results"> </a>
</h2>
<p>Implementation details for both classification and object detection are in the appendix.</p>
<h3 id="ImageNet">
<a class="anchor" href="#ImageNet" aria-hidden="true"><span class="octicon octicon-link"></span></a>ImageNet<a class="anchor-link" href="#ImageNet"> </a>
</h3>
<p>The multi head self-attention layer uses a spatial width of k=7 and 8 attention heads. The position-aware attention stem as described above is used.The stem performs self-attention within each 4×4 block of the original image, followed by batch normalization and a 4×4 max pool operation. Results are below : 
<img src="/DL_from_Foundations/images/copied_from_nb/images/sat_imagenet.png" alt="images"></p>
<h3 id="Coco-Object-Detection">
<a class="anchor" href="#Coco-Object-Detection" aria-hidden="true"><span class="octicon octicon-link"></span></a>Coco Object Detection<a class="anchor-link" href="#Coco-Object-Detection"> </a>
</h3>
<p>Here Retina Net is used with a classification backbone, followed by an FPN, the network has 2 detection heads. Results are in the table below : 
<img src="/DL_from_Foundations/images/copied_from_nb/images/sat_coco.png" alt="images"></p>
<p>We can see that using attention based backbone we can achieve results on par with a conv backbone, but with 22% less parameters. This can be extended by additionaly making the FPN and the detection heads attention-based and thereby reducing paraneter count by 34% and more importantly FLOPS by 39%.</p>
<h3 id="Where-is-stand-alone-attention-most-useful-?">
<a class="anchor" href="#Where-is-stand-alone-attention-most-useful-?" aria-hidden="true"><span class="octicon octicon-link"></span></a>Where is stand-alone attention most useful ?<a class="anchor-link" href="#Where-is-stand-alone-attention-most-useful-?"> </a>
</h3>
<p><img src="/DL_from_Foundations/images/copied_from_nb/images/sat_usful.png" alt="images"></p>
<p>Results are schown in the tables above.</p>
<h4 id="Stem">
<a class="anchor" href="#Stem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stem<a class="anchor-link" href="#Stem"> </a>
</h4>
<p>The basic results for the stem is that, convolutions perform very well here, as described above self-attention can not easily learn edges due to the high spatial correlation which is captured very well by conv layers though.</p>
<h4 id="Full-Net">
<a class="anchor" href="#Full-Net" aria-hidden="true"><span class="octicon octicon-link"></span></a>Full Net<a class="anchor-link" href="#Full-Net"> </a>
</h4>
<p>The authors basically state what has been described above, conv layers capture low level features very well, while attention is able to model global relations effectively. Therefore an optimal architecture should contain both attention and convolutional layers.</p>
<h3 id="Which-attention-features-are-important-?">
<a class="anchor" href="#Which-attention-features-are-important-?" aria-hidden="true"><span class="octicon octicon-link"></span></a>Which attention features are important ?<a class="anchor-link" href="#Which-attention-features-are-important-?"> </a>
</h3>
<ul>
<li>
<p>Effect of spatial extent of self-attention (Table 4) : 
The value of spatial extent k should generally be larger (for example k=11), the exact optimal setting depends on hyperaparameter choices.</p>
</li>
<li>
<p>Importance of positional information (Table 5 + 6)
3 types of encoding were used : no positional encoding, sinusodial encoing and absolute pixel position. Relativ encoding performs 2% better than absolute one. Removing content-content interaction only descreases accuracy by 0.5%. Therefore the positional encoding seems to be very important and can be a strong focus of their future research.</p>
</li>
<li>
<p>Importance of spatially-aware attention stem (Table 7) 
Using stand-alone attention in the stem with spatially-aware values, it outperforms vanilla stand-alone attention by 1.4%, while having similar FLOPS. Using a spatial convolution to the values instead of spatially-aware point-wise transformations (see above), leads to more FLOPS and slightly worse results. A future goal of the authors is to unify attention used across the stem and main</p>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="Cedric-Perauer/DL_from_Foundations"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/DL_from_Foundations/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/DL_from_Foundations/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/DL_from_Foundations/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/Cedric-Perauer" title="Cedric-Perauer"><svg class="svg-icon grey"><use xlink:href="/DL_from_Foundations/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/DL_from_Foundations/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
