<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Summary Mixup Augmentation Technique | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Summary Mixup Augmentation Technique" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Suggests a multi class augmentation technique to improve generalization, memorization of corrupted labels and improves robustness against adversarial examples" />
<meta property="og:description" content="Suggests a multi class augmentation technique to improve generalization, memorization of corrupted labels and improves robustness against adversarial examples" />
<link rel="canonical" href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/25/Mixup.html" />
<meta property="og:url" content="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/25/Mixup.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:image" content="https://cedric-perauer.github.io/DL_from_Foundations/images/mixup_tables.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-25T00:00:00-05:00" />
<script type="application/ld+json">
{"dateModified":"2020-04-25T00:00:00-05:00","datePublished":"2020-04-25T00:00:00-05:00","description":"Suggests a multi class augmentation technique to improve generalization, memorization of corrupted labels and improves robustness against adversarial examples","image":"https://cedric-perauer.github.io/DL_from_Foundations/images/mixup_tables.png","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/25/Mixup.html"},"url":"https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/25/Mixup.html","headline":"Summary Mixup Augmentation Technique","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/DL_from_Foundations/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://cedric-perauer.github.io/DL_from_Foundations/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/DL_from_Foundations/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Summary Mixup Augmentation Technique | fastpages</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Summary Mixup Augmentation Technique" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Suggests a multi class augmentation technique to improve generalization, memorization of corrupted labels and improves robustness against adversarial examples" />
<meta property="og:description" content="Suggests a multi class augmentation technique to improve generalization, memorization of corrupted labels and improves robustness against adversarial examples" />
<link rel="canonical" href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/25/Mixup.html" />
<meta property="og:url" content="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/25/Mixup.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:image" content="https://cedric-perauer.github.io/DL_from_Foundations/images/mixup_tables.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-25T00:00:00-05:00" />
<script type="application/ld+json">
{"dateModified":"2020-04-25T00:00:00-05:00","datePublished":"2020-04-25T00:00:00-05:00","description":"Suggests a multi class augmentation technique to improve generalization, memorization of corrupted labels and improves robustness against adversarial examples","image":"https://cedric-perauer.github.io/DL_from_Foundations/images/mixup_tables.png","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/25/Mixup.html"},"url":"https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/25/Mixup.html","headline":"Summary Mixup Augmentation Technique","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://cedric-perauer.github.io/DL_from_Foundations/feed.xml" title="fastpages" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/DL_from_Foundations/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/DL_from_Foundations/about/">About Me</a><a class="page-link" href="/DL_from_Foundations/search/">Search</a><a class="page-link" href="/DL_from_Foundations/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Summary Mixup Augmentation Technique</h1><p class="page-description">Suggests a multi class augmentation technique to improve generalization, memorization of corrupted labels and improves robustness against adversarial examples</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-25T00:00:00-05:00" itemprop="datePublished">
        Apr 25, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/DL_from_Foundations/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/Cedric-Perauer/DL_from_Foundations/tree/master/_notebooks/2020-04-25-Mixup.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/DL_from_Foundations/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/Cedric-Perauer/DL_from_Foundations/master?filepath=_notebooks%2F2020-04-25-Mixup.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/DL_from_Foundations/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/Cedric-Perauer/DL_from_Foundations/blob/master/_notebooks/2020-04-25-Mixup.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/DL_from_Foundations/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Problems-of-existing-rechniques-:">Problems of existing rechniques : </a></li>
<li class="toc-entry toc-h2"><a href="#Key-elements">Key elements </a></li>
<li class="toc-entry toc-h2"><a href="#Results-and-Conclusion">Results and Conclusion </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Classification">Classification </a></li>
<li class="toc-entry toc-h3"><a href="#Speech-Data">Speech Data </a></li>
<li class="toc-entry toc-h3"><a href="#Memorization-of-corrupted-labels-(Table-2)">Memorization of corrupted labels (Table 2) </a></li>
<li class="toc-entry toc-h3"><a href="#Robustness-to-adversarial-example-(Table-3)">Robustness to adversarial example (Table 3) </a></li>
<li class="toc-entry toc-h3"><a href="#Tabular-Data-(Table-4)">Tabular Data (Table 4) </a></li>
<li class="toc-entry toc-h3"><a href="#Stabilize-GAN-Training-(Table-5)">Stabilize GAN Training (Table 5) </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-25-Mixup.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Problems-of-existing-rechniques-:">
<a class="anchor" href="#Problems-of-existing-rechniques-:" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problems of existing rechniques :<a class="anchor-link" href="#Problems-of-existing-rechniques-:"> </a>
</h2>
<ul>
<li>ERM memorizes, does not generalize -&gt; vulnerable to adversarials</li>
<li>Classic data augmentation is used to define neighbors in a single class, this is mostly hand crafted by humans and does not consider multi class combinations
## What did the authors want to achieve ?</li>
<li>improve on undesirable memorization of corrupted labels and sensitivity to adversarial examples </li>
<li>stabilize training (especially of GANs)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Key-elements">
<a class="anchor" href="#Key-elements" aria-hidden="true"><span class="octicon octicon-link"></span></a>Key elements<a class="anchor-link" href="#Key-elements"> </a>
</h2>
<p>The Expected Risk (ER), assuming that the joint distribution between a random input $X$ and a label $Y$ $P(X,Y)$ is an empirical distribution :<br>
$P_{\delta}(x,y) = \frac{1}{n}\sum_{i=1}^{n}\delta(x=x_{i},y=y_{i})$</p>
<p>We can infer the approximation of the expected risk $R_{\delta}(f)$ since $dP_{\delta}(x,y) = 1$, and the loss $l(f(x),y) = \sum_{i=1}^{n}l(f(x_{i}),y_{i})$  :</p>
<p>$R_{\delta}(f) = \int l(f(x),y)*dP_{\delta}(x,y)=\sum_{i=1}^{n}l(f(x_{i}),y_{i})$</p>
<p>The Empirical Risk Minimization (ERM) (Vapnik,1998) is known as learning our function $f$ by minimizing the loss. $P_{\delta}$ is a naive estimation as it is one of many possible choices. The paper also mentions Vicinal Risk Minimization (Chapelle et al., 2000) which assumes the distribution P to be a sum of all the inputs and labels over a vicinity distribution $v(\tilde{x},\tilde{y}|x_{i},y_{i})$ and measures the probability of finding the virtual feature target  pair $(\tilde{x},\tilde{y})$ in the vicinity of the training feature-target pair $(x_{i},y_{i})$. The approach considered Gaussian vicinities, which is equal to augmenting the training data with additive Gaussian noise. Considering a Dataset of size m, we can infer the empirical vicinal risk :</p>
<p>$R_{v}(f) = \frac{1}{m}\sum_{i=1}^{m}l(f(\tilde{(x_{i}}),\tilde{y_{i}})$</p>
<p>This paper introduces a different generic vicinal distribution, called mixup : 
<img src="/DL_from_Foundations/images/copied_from_nb/images/mixup_mu.png" alt=""></p>
<p>With $\lambda = [0,1]$ and $(\tilde{x_{i}},\tilde{y_{i}})$ &amp; $(\tilde{x_{j}},\tilde{y_{j}}) being 2 random target vectors$. The mixup parameter $\alpha$ controls the strength of interpolation between the pair, as $\alpha \rightarrow  0$ it increasingly recovers to the ERM principle.</p>
<p>The implementation from the paper is relatively straightforward :</p>
<div class="highlight"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">),</span> <span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">y2</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">loader1</span><span class="p">,</span> <span class="n">loader2</span><span class="p">):</span>
<span class="n">lam</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">lam</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">lam</span><span class="p">)</span> <span class="o">*</span> <span class="n">x2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">lam</span> <span class="o">*</span> <span class="n">y1</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">lam</span><span class="p">)</span> <span class="o">*</span> <span class="n">y2</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
<p>What does it do ? 
Mixup makes the model behave linearly in-between classes/examples, this reduces the amount of oscillations when facing a new example that is outside of the training examples. This happens linearily and is simple and therefore a good bias from the Occam's razor point of view ( "Entities should not be multiplied without necessity.").</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Results-and-Conclusion">
<a class="anchor" href="#Results-and-Conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results and Conclusion<a class="anchor-link" href="#Results-and-Conclusion"> </a>
</h2>
<p>The authors prove that mixup is a very effective technique across all domains.</p>
<h3 id="Classification">
<a class="anchor" href="#Classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Classification<a class="anchor-link" href="#Classification"> </a>
</h3>
<p><img src="/DL_from_Foundations/images/copied_from_nb/images/mixup_class.png" alt="">
Experiments were made on both ImageNet and CIFAR-10/100 using different alphas for each dataset. $\alpha[0.1,0.4]$ for ImageNet and $\alpha = 1$ for CIFAR-10/100. Comparisons were made using different DenseNet and ResNet models. You can refer to the paper for the exact hyperparameters used in each case. As we can see in the graph above, miuxp outperforms their non-mixup counterparts. Learning rate decays were used @ epochs 10/60/120/180 with an inital value of 0.1 and training was done for a total of 300 epochs.</p>
<h3 id="Speech-Data">
<a class="anchor" href="#Speech-Data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Speech Data<a class="anchor-link" href="#Speech-Data"> </a>
</h3>
<p>The google command dataset was used which contains 30 classes of 65000 examples. LeNet and VGG-11 were used with ADAM and a learning rate of 0.001 and mixup variants with alphas of 0.1 and 0.2 were compared to the ERM counterparts. Mixup was able to outperform ERM with VGG-11 which had the lowest error of all models (3.9 percent) on the validation set. LeNet however performed better without mixup applied. Comparing these outcomes, the authors infer that mixuxp works particularly well with higher capacity models.</p>
<p><img src="/DL_from_Foundations/images/copied_from_nb/images/mixup_tables.png" alt=""></p>
<h3 id="Memorization-of-corrupted-labels-(Table-2)">
<a class="anchor" href="#Memorization-of-corrupted-labels-(Table-2)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Memorization of corrupted labels (Table 2)<a class="anchor-link" href="#Memorization-of-corrupted-labels-(Table-2)"> </a>
</h3>
<p>Robustness to corrupted labels is compared to ERM and mixup. An updated version of CIFAR is used where 20%,50% and then 80% are replaced by random noise. Usually Dropout was considered to be the best technique for corrupted label learning, so dropout with p ∈ {0.5, 0.7, 0.8, 0.9} and mixup are compared to each other along with a combo of both where α ∈ {1, 2, 4, 8} and p ∈ {0.3, 0.5, 0.7}.  The PreAct ResNet-18 (He et al., 2016) model implemented in (Liu, 2017) was used. The model was trained for 200 epochs.</p>
<h3 id="Robustness-to-adversarial-example-(Table-3)">
<a class="anchor" href="#Robustness-to-adversarial-example-(Table-3)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Robustness to adversarial example (Table 3)<a class="anchor-link" href="#Robustness-to-adversarial-example-(Table-3)"> </a>
</h3>
<p>Several methods were proposed, such as penalizing the norm of the Jacobian of the model to control the Lipschitz constand of it. Other approaches perform data augmentation on adversarial examples. All of these methods add a lot of compute overhead to ERM. Here the authors prove that mixup can improve on ERM significantly without adding significant compute overhead by penalizing the norm of the gradient of the loss wrt a given input along the most plausible directions (the directions of other training points). ResNet101 models were trained 2 models were trained on ImageNet with ERM and one was trained with mixup. White box attacks are tested at first . For that, the model itself is used to generate adversarial examples fusing FGSM or iterative FGSM methods, 10 iterations with equal step sizes are used, $\epsilon=4$ was set as maximum perturbation for each pixel. Secondly black box attacks are done, this is achieved by using the first ERM model to produce adversarials with FGSM and I-FGSM. Then the robustness of the second ERM model and the mixp model to these exampels is tested.</p>
<p>As we can see in table 3, mixup outperforms ERM in both cases significantly, being p to 2.7 times better when it comes to Top-1 error in the FGSM white box attack category.</p>
<p><img src="/DL_from_Foundations/images/copied_from_nb/images/mixup_tables2.png" alt=""></p>
<h3 id="Tabular-Data-(Table-4)">
<a class="anchor" href="#Tabular-Data-(Table-4)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tabular Data (Table 4)<a class="anchor-link" href="#Tabular-Data-(Table-4)"> </a>
</h3>
<p>A series of experiments was performed on the UCI dataset. A FCN with 2 hidden layers &amp; 128 ReLU units was trained for 10 epochs with Adam and a batch size of 16. Table 4 shows that mixup improves test error significantly.</p>
<h3 id="Stabilize-GAN-Training-(Table-5)">
<a class="anchor" href="#Stabilize-GAN-Training-(Table-5)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stabilize GAN Training (Table 5)<a class="anchor-link" href="#Stabilize-GAN-Training-(Table-5)"> </a>
</h3>
<p>When training GANs, the mathematical goal is to solve the optimization problem :</p>
<p><img src="/DL_from_Foundations/images/copied_from_nb/images/gan_1.png" alt=""></p>
<p>the binary cross entropy loss is used in this case. Solving this equation is a very difficult optimization problem (Goodfellow et al., 2016) as the discriminator often provides the generator with vanishing gradients. Using mixup, the optimization problem looks like this : 
<img src="/DL_from_Foundations/images/copied_from_nb/images/gan_mix.png" alt=""></p>
<p>The models are fully connected, with 3 hidden layers and 512 ReLU units. The generator accepts 2D Gaussian noise vectors, 20000 mini-batches of size 128 are used for training with the discriminator being trained for 5 epochs before every generator iteration. Table 5 shows the improved performance of mixup.</p>
<p>A part of these mostly supervised use cases (except for GANs), the authors believe that other non straightforward use cases such as segmentation or different unsupervised techniques should be promising areas of future research.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="Cedric-Perauer/DL_from_Foundations"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/DL_from_Foundations/jupyter/2020/04/25/Mixup.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/DL_from_Foundations/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/DL_from_Foundations/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/DL_from_Foundations/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/DL_from_Foundations/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/DL_from_Foundations/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
