<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://cedric-perauer.github.io/DL_from_Foundations/feed.xml" rel="self" type="application/atom+xml" /><link href="https://cedric-perauer.github.io/DL_from_Foundations/" rel="alternate" type="text/html" /><updated>2020-09-13T17:13:38-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/feed.xml</id><title type="html">fastpages</title><subtitle>An easy to use blogging platform with support for Jupyter Notebooks.</subtitle><entry><title type="html">Stand-Alone Self-Attention in Vision Models Summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html" rel="alternate" type="text/html" title="Stand-Alone Self-Attention in Vision Models Summary" /><published>2020-08-06T00:00:00-05:00</published><updated>2020-08-06T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-08-06-SATCONV.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.05909.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Combat the deficits of Conolutional Architectures, (local connectivity, failing to reason globally) by introducing Attention to as a stand-alone layer. The authors prove that this can be both more accurate and more efficient at the same time. Architectures that are attention only, and a mixed version of convolutional and attention architectures are introduced and compared to the vanilla convolutional implementations.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Methods&quot;&gt;Methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Methods&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Conv-Block-deficits&quot;&gt;Conv Block deficits&lt;a class=&quot;anchor-link&quot; href=&quot;#Conv-Block-deficits&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Capturing long range interactions is challenging for convolutions as they do not scale well with large receptives. Since attention has been used to tackle long range dependencies in sequence modeling, since architectures like SE Nets model attention on a chanel wise basis successfully. However in these cases attention was only an add-on to a traditional architecture style. In this paper the authors propose to use attention mechanisms as stand alone layers.&lt;/p&gt;
&lt;h3 id=&quot;Self-Attention&quot;&gt;Self-Attention&lt;a class=&quot;anchor-link&quot; href=&quot;#Self-Attention&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Attention was originally introduced in order to allow for summarization from variable length source sentences. Attention focuses on the important parts of the input and thereby can serve as a primary representation learning mechanism and fully replace recurrence. The word self means that it just considers a single context (query,keys and values are extracted from the same image). The breakthrough in this paper is the use of self-attention layers instead of convolutional layers.&lt;/p&gt;
&lt;p&gt;In this work, already existing mechanisms are used which are not optimized for the image domain. Therefore it is permutation equivariant and has limited expression capability for vision tasks.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/sat_conv.png&quot; alt=&quot;&quot; /&gt; 
&lt;br /&gt;&lt;br /&gt;
The process is as follows : 
1) given a pixel $x_{i,j} \in R^{d_{in}}$ in positions $ab \in N_{k}(i,j)$ a local kernel with kernel size $k$ is extracted. $x_{i,j}$ is the middle of the kernel, which is called &lt;em&gt;memory block&lt;/em&gt;. Prior work only performed global attention, which can only be done with a downsized sample as it is very compute expensive. 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2) Single headed attention is computed : 
&lt;br /&gt;&lt;br /&gt;
$y_{i,j} = \sum_{ab \in N_{k}(i,j)}  softmax_{a,b}(q_{ij}^{T}k_{ab})v_{ab}$
&lt;br /&gt;&lt;br /&gt;
where the ${queries}$ $q_{ij} = W_Q x_{ij}$&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;
${keys}$ $k_{ab} = W_K x_{ab}$
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;and ${values}$ $v_{ab} = W_V x_{ab}$ 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;are linear transformations of the pixel in position and it's neighbors in the kernel. 
&lt;br /&gt;&lt;br /&gt;
$\texttt{softmax}_{a b}$ is a softmax, which is applied to all logits computed in the neighborhood of $ij$. 
$W_Q, W_K, W_V \in \mathbb{R}^{d_{out} \times d_{in}}$ are learned transforms. 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Local self-attention is similar to convolution in the way that it aggregates spatial information in the neighborhoods, multi attention heads are used to learn unique representations of the input. This is done by partitioning pixel features into $N$ groups and then computing single-headed attention on each one seperately with the transforms $W_Q, W_K, W_V \in \mathbb{R}^{d_{out} \times d_{in}}$. The outputs of the heads are then concatenated. 
&lt;br /&gt;&lt;br /&gt;
2D relative pose embeddings,relative attention is used : 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;1) relative attention computes relative distances of the pixel to each one in the neighborhood : row $(a-i)$ and column offset $(b-j)$&lt;br /&gt;
2) row and column offset are associated with an embedding and concatenated into a vector $r_{a-i,b-j}$&lt;/p&gt;
&lt;p&gt;3) Spatial-relative attention is then defined as : 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;$y_{ij} = \sum_{a,b\in N_{k}(i, j)} softmax_{ab}(q_{ij}^{T}k_{ab}+q_{ij}^{T}r_{a-i,b-j})v_{ab}$&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
The logit measuring similarity between the query and an element results from the content of the element and the relative distance of the element from the query. By inlcuding this spatial information, self-attention also has translation equivariance, just like conv layers. Unlike conv layers, self-attentions parameter count is independent of its spatial extent. 
The compute cost also grows slower : 
For example, if $d_{in} = d_{out} = 128$, a convolution layer with $k = 3$ has the same computational cost as an attention layer with $k = 19$.&lt;/p&gt;
&lt;p&gt;Using this as their basis, a fully attentional architecture is created in two steps:&lt;/p&gt;
&lt;h4 id=&quot;Replacing-Spatial-Convolutions&quot;&gt;Replacing Spatial Convolutions&lt;a class=&quot;anchor-link&quot; href=&quot;#Replacing-Spatial-Convolutions&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;A spatial conv is defined to have spatial extent k &amp;gt; 1, which also includes 1x1 convolutions. These can be viewed as fully connected layers. 
Here the authors want to replace conv blocks in a straightforward way, specificially focusing on ResNet. Therefore the 3x3 convolution in Path B is swapped with a self-attention layer as defined above. All the other blocks are not changed, this might be supobtimal but promises potential improvements using architecture search.&lt;/p&gt;
&lt;h4 id=&quot;Replacing-the-Convolutional-Stem-(intial-layers-of-the-CNN)&quot;&gt;Replacing the Convolutional Stem (intial layers of the CNN)&lt;a class=&quot;anchor-link&quot; href=&quot;#Replacing-the-Convolutional-Stem-(intial-layers-of-the-CNN)&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;This part focuses on replacing the inital layers, as they are the most compute expensive due to the large input size of the image. In the OG ResNet the input is a 7x7 kernel with stride 2, followed by 3x3 max pooling with stride 2. At the beginning RGB pixels are individually uninformative and information is heavily spatially correlated through low level features such as edges. Edge detectors are difficult to learn for self-attention due to spatial correlation, convs learn these easily through distance based weight parameterization. The authors inject spatially-varying linear transforms into the pointwise 1x1 softmax convolution.&lt;/p&gt;
&lt;p&gt;$\tilde{v}_{a b} = \left(\sum_m  p(a, b, m) W_V^{m}\right) x_{a b}$&lt;/p&gt;
&lt;p&gt;The results is similar to convolutions, weights are learned based on a local neighborhood basis. So in total the stem consists of spatially aware value features, followed by max-pooling. A more detailed explanation of this can be found in the appendix of the paper (page 14/15).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Results&quot;&gt;Results&lt;a class=&quot;anchor-link&quot; href=&quot;#Results&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Implementation details for both classification and object detection are in the appendix.&lt;/p&gt;
&lt;h3 id=&quot;ImageNet&quot;&gt;ImageNet&lt;a class=&quot;anchor-link&quot; href=&quot;#ImageNet&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The multi head self-attention layer uses a spatial width of k=7 and 8 attention heads. The position-aware attention stem as described above is used.The stem performs self-attention within each 4×4 block of the original image, followed by batch normalization and a 4×4 max pool operation. Results are below : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/sat_imagenet.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Coco-Object-Detection&quot;&gt;Coco Object Detection&lt;a class=&quot;anchor-link&quot; href=&quot;#Coco-Object-Detection&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Here Retina Net is used with a classification backbone, followed by an FPN, the network has 2 detection heads. Results are in the table below : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/sat_coco.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that using attention based backbone we can achieve results on par with a conv backbone, but with 22% less parameters. This can be extended by additionaly making the FPN and the detection heads attention-based and thereby reducing paraneter count by 34% and more importantly FLOPS by 39%.&lt;/p&gt;
&lt;h3 id=&quot;Where-is-stand-alone-attention-most-useful-?&quot;&gt;Where is stand-alone attention most useful ?&lt;a class=&quot;anchor-link&quot; href=&quot;#Where-is-stand-alone-attention-most-useful-?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/sat_usful.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Results are schown in the tables above.&lt;/p&gt;
&lt;h4 id=&quot;Stem&quot;&gt;Stem&lt;a class=&quot;anchor-link&quot; href=&quot;#Stem&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The basic results for the stem is that, convolutions perform very well here, as described above self-attention can not easily learn edges due to the high spatial correlation which is captured very well by conv layers though.&lt;/p&gt;
&lt;h4 id=&quot;Full-Net&quot;&gt;Full Net&lt;a class=&quot;anchor-link&quot; href=&quot;#Full-Net&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The authors basically state what has been described above, conv layers capture low level features very well, while attention is able to model global relations effectively. Therefore an optimal architecture should contain both attention and convolutional layers.&lt;/p&gt;
&lt;h3 id=&quot;Which-attention-features-are-important-?&quot;&gt;Which attention features are important ?&lt;a class=&quot;anchor-link&quot; href=&quot;#Which-attention-features-are-important-?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Effect of spatial extent of self-attention (Table 4) : 
The value of spatial extent k should generally be larger (for example k=11), the exact optimal setting depends on hyperaparameter choices.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Importance of positional information (Table 5 + 6)
3 types of encoding were used : no positional encoding, sinusodial encoing and absolute pixel position. Relativ encoding performs 2% better than absolute one. Removing content-content interaction only descreases accuracy by 0.5%. Therefore the positional encoding seems to be very important and can be a strong focus of their future research.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Importance of spatially-aware attention stem (Table 7) 
Using stand-alone attention in the stem with spatially-aware values, it outperforms vanilla stand-alone attention by 1.4%, while having similar FLOPS. Using a spatial convolution to the values instead of spatially-aware point-wise transformations (see above), leads to more FLOPS and slightly worse results. A future goal of the authors is to unify attention used across the stem and main&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/SATCONV.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/SATCONV.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ADABOUND Summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/31/ADABOUND.html" rel="alternate" type="text/html" title="ADABOUND Summary" /><published>2020-07-31T00:00:00-05:00</published><updated>2020-07-31T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/31/ADABOUND</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/31/ADABOUND.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-31-ADABOUND.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.09843&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In recent years, a lot of different first order methods (ADAGRAD, RMSProp, ADAM, AMSGRAD) have been introduced for training deep neural nets in order to combat the uniform gradient scaling of SGD and it's slower training speed. Experiments and the release of state of the art CV and NLP models - which use traditional SGD (often with momentum) -  prove that these optimizers generalize poorly in comparison to classic SGD. Sometimes they don't even converge, which is largely because of unstable and extreme learning rates.&lt;/p&gt;
&lt;p&gt;The authors therefore provide new, dynamically bounded versions of AMSGRAD and ADAM, called AMSBOUND and ADABOUND. These new techniques can be seen as adaptive methods during early training, which smoothly transform into SGD/SGD with momentum as training time increases. Their method is inspired by gradient clipping.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Methods&quot;&gt;Methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Methods&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Analysis-of-already-existing-adaptive-methods&quot;&gt;Analysis of already existing adaptive methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Analysis-of-already-existing-adaptive-methods&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/frame_adabound.png&quot; alt=&quot;image&quot; /&gt; 
The figure above shows the generic training loop that was used by the authors, where :&lt;/p&gt;
&lt;p&gt;$g_{t}$ represents the gradient at step $t$&lt;br /&gt;
$\alpha_{t}$ represents the learning rate at step $t$, which is gradually being                           reduced for theoretical proof of convergence&lt;br /&gt;
$m_{t}$ represents the momentum term at step $t$, a function of the gradients&lt;br /&gt;
$V_{t}$ represents the scaling term at step $t$, a function of the gradients&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/adabound_algos.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The table above shows different optimizers w.r.t to the generic training loop. $\phi_{t}$ and $\psi_{t}$ are the respective functions used for computing $m_{t}$ and $V_{t}$ respectively. 
&lt;br /&gt;&lt;br /&gt;
The authors mainly focus on ADAM in their research, as it is very general so it also applies to other adaptive methods (RMSProp, ADAGRAD). Note that ADAM is a combination of RMSprop and momentum, with the distinction that momentum is incorporated directly in the first-order moment estimate (with the use of exponential weighting) and also the additional use of weight decay.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;
At the time of writing, AMSGRAD was a recently released adaptive method, which was supposed to perform as well as SGD. However AMSGRAD does not yield evident improvement when compared to ADAM. AMSGRAD uses a smaller learning rate than ADAM and the AMSGRAD researchers consider that large learning rates are probLematic. Due to the smaller learning rate being used in AMSGRAD, the authors suspect that small learning rates might be a problem for ADAM as well.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;To prove their theory, the authors sample learning rates of several weights and biases for ResNet-34 on CIFAR-10 with ADAM. They randomly select 9 3x3 conv kernels from different layers and biases from the last fully connected layer. Because parameters of the same layer very similar, 9 weights sampled from 9 kernels and one bias are considered. These weights and the bias are visualized in a heatmap. They find that tiny learning rates of smaller than 0.01 and larger ones greater than 1000 lead to the model being closer to convergence. Therefore it is empirically proven, that the learning rate can be both too large and too small in extreme cases. AMSGRAD combats the impact of large learning rates, but it does neglect the other side of the spectrum.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;In a second experiment, the authors consider a set of linear functions with different gradients. In their example (see Page 4, upper part of the page), once every $C$ steps a gradient of -1 (moves into the wrong direction) occurs and at the next step a gradient of 2 occurs. Even though 2 is larger than 1 in terms of the abolute value, the larger gradient can not counteract the -1 gradient as the learning rate is scaled down in this later time step leading to x diverging.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3 theorems formalize the intuition : 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Theorem-1&quot;&gt;Theorem 1&lt;a class=&quot;anchor-link&quot; href=&quot;#Theorem-1&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;There is an online convex optimization problem where for any initial step size α, ADAM
has non-zero average regret. (ADAM does not converge) 
This problem does not occur with SGD, as for a large amount of possible choices for α, the average regret for SGD converges to 0. This is mostly a problem at the end of training, where a lot of gradients are close to zero and the average of 2n order momentum is very various beacuse of the exponential moving average. In this case correct signals occur rarely, which may lead to the algorithm not being able to converge (refer to the example with the gradients of -1 and 2 earlier). Their results show that for any case where $\beta_{1} &amp;lt; \sqrt{\beta_{2}}$ , ADAM does not converge regardless of the step size. Note the standard values are usually $\beta_{1}=0.9$ and $\beta_{2}=0.999$, which proves that $\beta_{1} &amp;lt; \sqrt{\beta_{2}}$, because $0.9 &amp;lt; 0.9995$.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Theorem-2&quot;&gt;Theorem 2&lt;a class=&quot;anchor-link&quot; href=&quot;#Theorem-2&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
Refers to the earlier point, that there is an online convex optimzation setting, where ADAM does not converge when $\beta_{1} &amp;lt; \sqrt{\beta_{2}}$ for  $\beta_{1}$,$\beta_{2}$ $\in [0,1)$ 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Theorem-3&quot;&gt;Theorem 3&lt;a class=&quot;anchor-link&quot; href=&quot;#Theorem-3&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
Just like theorem 2, but in a stochastic convex optimization setting  : There is a stochatic convex optimzation setting, where ADAM does not converge when $\beta_{1} &amp;lt; \sqrt{\beta_{2}}$ for  $\beta_{1}$,$\beta_{2}$ $\in [0,1)$ 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;By formulating these 3 theorems, it is proven that the impact of extreme learning rates will lead to a lacking generalization ability and will not be able to solve the problem.&lt;/p&gt;
&lt;h3 id=&quot;Adabound&quot;&gt;Adabound&lt;a class=&quot;anchor-link&quot; href=&quot;#Adabound&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;As mentioned in the beginning of the paper, the goal should be to achieve SGD performance with ADAM training speed. 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/ADABOUND_algo.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;The above algorithm is essentially ADAM with clipping in order to constrain the learning rate to be in $[\eta_{l},\eta_{u}]$. For ADAM $\eta_{l} = 0$ and $\eta_{u} = \infty$ and for SGD(optionally with Momentum) with a learning rate $\alpha$, $\eta_{l} = \eta_{u} = \alpha$. The bound are functions of step $t$, with the lower bound $\eta_{l}(t)$ is a non-decreasing function that starts from zero and converges to SGD's $\alpha$  and the upper bound $\eta_{u}(t)$ being a function that starts from $\infty$ and converges to $\alpha$ as well. 
The functions are : 
$\eta_{l}(t) = 0.1 - \frac{0.1}{(1-\beta_{2})t + 1}$&lt;br /&gt;
$\eta_{u}(t) = 0.1 + \frac{0.1}{(1-\beta_{2})t}$&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;We therefore reach the SGD state in the end as bound become increasingly restricted with higher step size $t$. The authors empasize that this method is better than using a mixed ADAM and SGD approach with a &quot;hard switch&quot; (Kesker &amp;amp; Socher 2017) which also introduces a hard to tune hyperparameter. The result is proven for Adabound in theorem 4, which also shows that ADABOUND is upper bounded by $O(\sqrt{T})$ : 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/theorem4.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
The authors provide extensive and very understandable proofs in their appendix.&lt;/p&gt;
&lt;h3 id=&quot;Implementation&quot;&gt;Implementation&lt;a class=&quot;anchor-link&quot; href=&quot;#Implementation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
An implementation of ADABOUND and ADABOUNDW by the authors can be found &lt;a href=&quot;https://github.com/Luolc/AdaBound/blob/master/adabound/adabound.py&quot;&gt;here&lt;/a&gt;. We can see that the implementation is very similar to the standard ADAM optimizer in the Pytorch source code. The difference is the if/else condition in the end which keeps track of the maximum 2nd moment running average, as well as the bounding at the end of the &lt;code&gt;step&lt;/code&gt; function. ADABOUNDW also adds weight decay at the end.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Results&quot;&gt;Results&lt;a class=&quot;anchor-link&quot; href=&quot;#Results&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Ada_exp.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;The authors compare ADABOUND and AMSBOUND to ADAM, AMSGRAD, ADAGRAD and SGD on the tasks above with the respective architectures. Hyperparameters are tuned for each optimizer seperately using a logarithmic grid which can be extrapolated if an extreme value is determined to perform the best.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/ada_CNN.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;The above diagrams show the performance of ADABOUND/AMSBOUND when compared to traditional methods. With DenseNet-121 on CIFAR-10 ADABOUND is about 2% better than it's base methods during test. With ResNet, they even outperform SGD (with momentum) @ test time by 1%. Much more improvment in deep conv nets is oberved when compared to perceptrons.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/ada_RNN.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;For RNNs ADAM does have the best initial progress but stagnates very quickly, with AMSBOUND/ADABOUND being smoother than SGD (with momentum). Compared to ADAM it outperforms it by 1.1% in the 1 layer LSTM case and 2.8% in the 3 Layer case. This further proves that increasingly more complex architectures benefit even more from AMSBOUND/ADABOUND. This is due to extreme learning rates being more likely to occur with complex models.&lt;/p&gt;
&lt;p&gt;The authors summarize that while AMSBOUND/ADABOUND perform well with complex architectures, there is still potential for shallower architectures. It is also unclear why SGD performs so well in most cases. Other ways for improvment should also be considered, the authors propose weight decay as a potential often (AMSBOUNDW).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/adabound.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/adabound.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Mish Paper Summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/24/Mish.html" rel="alternate" type="text/html" title="Mish Paper Summary" /><published>2020-07-24T00:00:00-05:00</published><updated>2020-07-24T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/24/Mish</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/24/Mish.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-24-Mish.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1908.08681.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Propose a new activation function which replaces upon the known standards such as ReLU and Swish. The function proposed is called Mish activation and is defined by : $f(x) = x * tanh(softplus(x))$
Recall that Sofplus is defined as $f(x) = ln(1+e^{x})$
The authors show that it can be more effective than ReLU and Swish for Computer Vision tasks. 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Mish.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Methods&quot;&gt;Methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Methods&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Mish-Activation&quot;&gt;Mish Activation&lt;a class=&quot;anchor-link&quot; href=&quot;#Mish-Activation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;As explained in the intro, Mish is a novel activation function. It is inspired by ReLU and Swish and has a bounded bottom value of $~ -0.31$&lt;br /&gt;
The derivative is defined as  :
$f^{'}(x) = \frac{e^{x} * w}{\delta^{2}}$&lt;br /&gt;
With $w=4*(x+1) + 4e^{2x} + e^{3x}  +e^{x} * (4x+6)$&lt;br /&gt;
and $\delta = 2e^{2x} + e^{2x} + 2$&lt;br /&gt;
It also has a self gating property, which means that it simply takes a scalar as input and allows it to easily replace ReLU in existing networks. A plot including Mish and Swish derivatives is shown below : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Mish_derivative.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Code&quot;&gt;Code&lt;a class=&quot;anchor-link&quot; href=&quot;#Code&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We can implement Mish in Pytorch the following way :&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MishImplementation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;nd&quot;&gt;@staticmethod&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_for_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softplus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# x * tanh(ln(1 + exp(x)))&lt;/span&gt;
   &lt;span class=&quot;nd&quot;&gt;@staticmethod&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saved_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sofplus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Credits go to the author of the paper and the implementation above that is used in YOLOv3 by Ultralytics.&lt;/p&gt;
&lt;h3 id=&quot;Explanation&quot;&gt;Explanation&lt;a class=&quot;anchor-link&quot; href=&quot;#Explanation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Mish_prop.png&quot; alt=&quot;image&quot; /&gt; 
The authors explain why Mish does improve upon current results in this section and emphasize the advantageous properties of Mish.&lt;/p&gt;
&lt;p&gt;Like Relu and Swish, Mish is unbounded above, which prevents saturation and therefore vanishing gradients. The about -0.31 bound below adds strong regularization  properties. Not killing gradients when x is below 0 improves gradient flow and therefore improves expressivity. Famously ReLU is not differentiable at 0, the smoothness  of Mish makes it continuously differentiable. The smoother function allow for smoother loss functions and therefore better optimization. The authors summarize these properties and the table above.&lt;/p&gt;
&lt;p&gt;The authors generally recommend to use a higher amount of epochs with Mish activation. This obviously introduces some overhead during training.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Results&quot;&gt;Results&lt;a class=&quot;anchor-link&quot; href=&quot;#Results&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Hyperparameter-study&quot;&gt;Hyperparameter study&lt;a class=&quot;anchor-link&quot; href=&quot;#Hyperparameter-study&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The author studies the difference between ReLU, Swish and Mish in Section 3.1 by considering fully connected nets with different layer amounts batch norm, dropout and no residual connections, plots are shown for every category. The most important takeaway is that Mish is better than current SOTA for optimizing larger and wider networks. It has to be criticized that the author is not using residual connections here, as this might increase the advantage of Mish even more than it would in a real setting with a skip connection network.
They also show that larger batch sizes benefit from Mish, it is also more stable for different initalizations and slightly more robust to noise.&lt;/p&gt;
&lt;p&gt;The results are replicated in experiments with a 6-layer CNN. Here Mish outperforms Swish with 75.6% to 71.7% on CIFAR-100. Swish did not seem to learn for the first 10 epochs here due to dead gradients.&lt;/p&gt;
&lt;p&gt;The author also shows that Mish outperforms Swish when using Cosine Annealing and outperforms Swish by about 9% when using Mixup with $\alpha=0.2$ to compare the two methods.&lt;br /&gt;
Statistical comparison shows that Mish has highest mean test accuracy and lowest mean standard deviation when compared to ReLU and Swish.&lt;/p&gt;
&lt;p&gt;It is also mentioned that Mish is slightly less efficient on GPU than the other two mentioned activation functions.&lt;/p&gt;
&lt;h3 id=&quot;Different-Architectures-with-Mish&quot;&gt;Different Architectures with Mish&lt;a class=&quot;anchor-link&quot; href=&quot;#Different-Architectures-with-Mish&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The author compares Mish by training with different networks and soley replacing ReLU/Swish with Mish, while leaving the hyperparameters unchanged. The superior performance of Mish during CIFAR100 can be seen in the table below : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Mish_results.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/Mish.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/Mish.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Efficient Det paper summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/15/EfficientDet.html" rel="alternate" type="text/html" title="Efficient Det paper summary" /><published>2020-07-15T00:00:00-05:00</published><updated>2020-07-15T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/15/EfficientDet</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/15/EfficientDet.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-15-EfficientDet.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Recently, the SOTA in high accuracy in the fields of object detection and semantic segmentation were mostly achieved by scaling up architectures (e.g. AmoebaNet and NAS-FPN). 
These models are not easily deployable, especially in runtime/compute constrained applications such as autonomous driving. While existing worked has achieved faster rutime through one-stage/anchor-free detectors or model compression, they usually sacrifice accuracy for runtime. The goal is therefore to create an architecture that combines the best of both worlds, and achieve both high accuracy and better efficiency. The authors consider a wide range of compute that someone might have at hand during inference (3B to 300B FLOPS).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Methods&quot;&gt;Methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Methods&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Challenges&quot;&gt;Challenges&lt;a class=&quot;anchor-link&quot; href=&quot;#Challenges&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The authors define 2 main challenges :&lt;/p&gt;
&lt;p&gt;1) efficient multi-scale feature fusion&lt;/p&gt;
&lt;p&gt;The authors do consider recent developments like PANet and NAS-FPN (both improvments of the original FPN approach). Most of these works only sum up the features without weighting them, even though the resolutions are different. That's why the authors of the paper propose a weighted bi-directional feature pyramid network (BiFPN), it introduces weights that can learn the importance of a different input features. It does this while also applying top-down and bottom-up bath augmentation as proposed in the PANet paper.&lt;/p&gt;
&lt;p&gt;2) model scaling&lt;/p&gt;
&lt;p&gt;In the past, the main method to improve performance, was using larger and therefore more powerful backbones. In this paper the authors use NAS to jointly scale resolution of the input, depth and width of the net as well as sclaing the feature network and box/class prediction network. It thus follows the ideas of EfficientNet, which also turns out to be their backbone choice. The architectures that are proposed is therefore a combo of EfficientNet, BiPFN and compound scaling. These models are called EfficientDet, in honor of their Backbone.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/FPNs.png&quot; alt=&quot;image&quot; /&gt; 
$Figure$ $1$&lt;/p&gt;
&lt;h3 id=&quot;BiFPN&quot;&gt;BiFPN&lt;a class=&quot;anchor-link&quot; href=&quot;#BiFPN&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;h4 id=&quot;Problem&quot;&gt;Problem&lt;a class=&quot;anchor-link&quot; href=&quot;#Problem&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;BiFPN aims to aggregate features at different resolutions, as FPN-ish methods downscale the feature level with a resolution of $1/2^{i}$ ($i$ being the layer number) , up-/downsampling is used in order to match features of different resolution.&lt;/p&gt;
&lt;h4 id=&quot;Cross-Scale-Connections&quot;&gt;Cross-Scale Connections&lt;a class=&quot;anchor-link&quot; href=&quot;#Cross-Scale-Connections&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In their research the authors compare PANet (introduces bottom-up aggregation) with FPN and NAS-FPN and find that PANet achieves the best accuracy at the cost of some compute overhead. The authors improve upon these cross-scale connections by removing and thereby simplifying nodes with only one input edge, as these have less contribution to the feature net. Secondly they connect an extra edge from input to output node if they are at the same level (fusion with low compute cost), also see $Figure$ $1$. Furthermore, each bidirectional layer (top-down and bottom-up bath) is repeated multiple times to enable higher level fusion. For concrete implementation details (# of layers), refer to section 4.2 in the paper.&lt;/p&gt;
&lt;h4 id=&quot;Weighted-Feature-Fusion&quot;&gt;Weighted Feature Fusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Weighted-Feature-Fusion&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;As explained earlier, the different resolutions that the feature maps have should be considered and therefore weithed during aggregation. In this work the authors use Pyramid Attention Network, which uses global self-attention upsampling to recover the location of the pixels. In order to weigh each input seperately, an additional learnable weight is added for each input. The authors consider three different fusion approaches :&lt;/p&gt;
&lt;p&gt;1) Unbounded Fusion : $0 = \sum\limits_{i} w_{i} * I_{i}$
=&amp;gt; could lead to instability, so weight norm is applied here&lt;/p&gt;
&lt;p&gt;2) Softmax-based function : $O = \sum\limits_{i} \dfrac{e^{w_{i}}}{\sum\limits_{j} e^{w_{j}}} * I_{i}$&lt;br /&gt;
=&amp;gt; here the idea is to normalize the probablities between 0 and 1, weighting the importance of each input that way. However the softmax introduces extra slowdown on the GPU hardware, so 3) is proposed :&lt;/p&gt;
&lt;p&gt;3) Fast normalized fusion : $O = \sum\limits_{i} \dfrac{w_{i}}{\sum\limits_{j} {w_{j}} + \epsilon} * I_{i}$&lt;/p&gt;
&lt;p&gt;This function has the same learning charaterstics as 2), but it runs about 30% faster on GPU  Depthwise sep. convs are used for better runtime.&lt;/p&gt;
&lt;h3 id=&quot;Architecture&quot;&gt;Architecture&lt;a class=&quot;anchor-link&quot; href=&quot;#Architecture&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/eff_arch.png&quot; alt=&quot;image&quot; /&gt;
$Figure$ $2$&lt;/p&gt;
&lt;p&gt;To create the final architecture a new compound scaling method is introduced which scales Backbone,BiFPN, class/box net and resolution jointly. A heuristics used as object detectors have even more possible configs as classification nets.&lt;/p&gt;
&lt;h4 id=&quot;Backbone&quot;&gt;Backbone&lt;a class=&quot;anchor-link&quot; href=&quot;#Backbone&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In order to use ImageNet pretraining, the checkpoints of EfficientNet-B0 to B6 are used.&lt;/p&gt;
&lt;h4 id=&quot;BiFPN-Net&quot;&gt;BiFPN Net&lt;a class=&quot;anchor-link&quot; href=&quot;#BiFPN-Net&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The BiFPN depth is linearly increased. The width is scaled exponentially, a grid search is done and 1.35 is used as a base :&lt;/p&gt;
&lt;p&gt;$W_{BiFPN} = 64 * (1.35^{\theta})$&lt;br /&gt;
$D_{BiFPN} = 3 + \theta$&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/efficient_nas.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Box/class-prediction-network&quot;&gt;Box/class prediction network&lt;a class=&quot;anchor-link&quot; href=&quot;#Box/class-prediction-network&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Width is fixed to be same as BiFPN, but depth is increased differently : 
$D_{box} = D_{class} = 3 + [\theta/3]$&lt;/p&gt;
&lt;h4 id=&quot;Image-Resolution&quot;&gt;Image Resolution&lt;a class=&quot;anchor-link&quot; href=&quot;#Image-Resolution&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Image res. is increased using equation : $R_{input} = 512 + \theta * 128$&lt;br /&gt;
128 is used as the features are used in level 3-7 and $2^{7} = 128$&lt;/p&gt;
&lt;p&gt;Heuristics based scaling might not be optimal and could be improved.&lt;/p&gt;
&lt;p&gt;The scaling results can be seen below : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/eff_nas.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The advantage of compound scaling is shown in $Figure$ $2$.&lt;/p&gt;
&lt;h4 id=&quot;Some-Implementation-Details&quot;&gt;Some Implementation Details&lt;a class=&quot;anchor-link&quot; href=&quot;#Some-Implementation-Details&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;SGD with momentum 0.9 and weight decay 4e-5   &lt;/li&gt;
&lt;li&gt;Synch BN w/BN decay of 0.99 end epsilon 1e-3   &lt;/li&gt;
&lt;li&gt;swish activation with weight decay of 0.9998 &lt;/li&gt;
&lt;li&gt;focal-loss with α = 0.25 and γ = 1.5, and aspect ratio {1/2, 1, 2}&lt;/li&gt;
&lt;li&gt;RetinaNnet preprocessing with training-time flip-
ping and scaling&lt;/li&gt;
&lt;li&gt;soft NMS is used for D7, standard NMS for the others&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Results&quot;&gt;Results&lt;a class=&quot;anchor-link&quot; href=&quot;#Results&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/eff_det.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;As the graph above shows, EfficientDet-D0 performs about on par with YOLOv3. It does use 28x fewer flops, which does not directly show in the runtime (also due to the optimized TensorRT implementation that YOLOv3 uses). Overall it is about 4-9x smaller and 13-42x less FLOP hungry than other detectors. In total they are about 4.1x faster on GPU and even 10x faster on CPU. This is probably due to the fact that the CPU can not hide the extra compute requierements in FLOPS as efficiently as the latency hiding GPU architecture.&lt;/p&gt;
&lt;p&gt;The authors compete on a Segmentation task and outperform DeepLabV3+ by 1.7% on COCO Segmentation with 9.8x fewer FLOPS.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/eff_det.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/eff_det.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SE Net Summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/03/SENET.html" rel="alternate" type="text/html" title="SE Net Summary" /><published>2020-07-03T00:00:00-05:00</published><updated>2020-07-03T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/03/SENET</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/03/SENET.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-03-SENET.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;SENET-Paper-Summary&quot;&gt;SENET Paper Summary&lt;a class=&quot;anchor-link&quot; href=&quot;#SENET-Paper-Summary&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;(Jie HU, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu)&lt;/p&gt;
&lt;h3 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;So far most research was focused on the feature hierarchy (global receptive fields are obtained through multi-scale operations, interleaving layers, ...) the authors here want to focus on channel relationship instead. They propose a new blok type called the &quot;Squeeze and excitation block&quot;, which models dependencies between channels. They achieve new SOTA results on ImageNet and won the competition in 2017.&lt;/p&gt;
&lt;h3 id=&quot;Methods-Proposed&quot;&gt;Methods Proposed&lt;a class=&quot;anchor-link&quot; href=&quot;#Methods-Proposed&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Squeeze-and-Excitation Block :&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/se_block.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;p&gt;feature maps $U$ are first passed through a $squeeze$ operation, this results in a descriptor. It stacks the maps by it's spatial Dimensions (HxW), so the total size is (HxWxnumber of channels), essentially it produces an Embedding.&lt;/p&gt;
&lt;p&gt;After that an $excitation$ operation is applied, it is a self gating mechanism. It uses the emedding as an input and outputs per channel weights. This operation is then applied to the feature maps $U$. SE Blocks can be stacked like Dense blocks, Residual blocks, etc.&lt;/p&gt;
&lt;p&gt;In the early layers, SE blocks are class agnostic, while they are very class specific in the later ones. Therfore the advantage feature recalibration can be accumulated in the net.&lt;/p&gt;
&lt;h4 id=&quot;Squeeze&quot;&gt;Squeeze&lt;a class=&quot;anchor-link&quot; href=&quot;#Squeeze&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Each filter only has a local receptive field, which is only focusing on that region. The researchers therefore propose the squeeze operation to create one descriptor of global information. It is generated by shrinking the input $U$ (using global avg. pooling) through it's spatial dimensions :&lt;/p&gt;
&lt;p&gt;$z_{c} = F_{sq}(u_{c}) = \dfrac{1}{H*W} * \sum\limits_{i=1}^H \sum\limits_{j=1}^W u_{c}(i,j) $&lt;/p&gt;
&lt;p&gt;The output $z$ can be interpreted as a collection of the local descriptors, together they describe the whole image.&lt;/p&gt;
&lt;h4 id=&quot;Excitation&quot;&gt;Excitation&lt;a class=&quot;anchor-link&quot; href=&quot;#Excitation&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The excitation block is responsible for capturing channel-wise dependencies. The authors choose a function that is 1) flexibel (learn non-linear interactions between channels) and 2)  has a non-mutually exclusive relationship, because they want to have the ability to emphasize multiple channels and thereby avoid using a one-hot activation. The authors propose a gating mechanism to do this :&lt;/p&gt;
&lt;p&gt;$s = F_{ex}(z,W) = \sigma(g,(z,W)) = \sigma(W_{2}\delta(W_{1}z))$&lt;/p&gt;
&lt;p&gt;Here $\delta$ is the ReLU activation function&lt;br /&gt;
$W_{1}  \epsilon R^{\frac{C}{R} * C} $&lt;br /&gt;
$W_{2}  \epsilon R^{C * \frac{C}{R} } $&lt;/p&gt;
&lt;p&gt;In order to limit the compute complexity, the authors make the gating mechanism consist of a bottleneck with 2 FC layers around the non-linear activation function. It uses a reduction ration r (for good choice of this refer to 6.1 in the paper), a ReLU and then an increasing layer for dimensionality. The final output is then obtained by rescaling $U$ with an activation (scalar multiplies channel-wise). In a way SE blocks produce their own self attention on channels that are not locally confined, by mapping the descriptor produced by squeeze to channel weights.&lt;/p&gt;
&lt;p&gt;SE blocks can be nicely implemented into existing architectures such as Inception, ResNet or ResNeXt blocks. DOW&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/SE_kinds.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Compute-complexity&quot;&gt;Compute complexity&lt;a class=&quot;anchor-link&quot; href=&quot;#Compute-complexity&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Due to the global avg pooling in the squeeze part, only two FC layers and channelwise scaling in the excitation part, only 0.26% inference compute increase compared to a normal ResNet-50 (with image size 224x224) is observed. SE blocks increases ResNet parameter size by about ~10% (2.5 Million parameters).&lt;/p&gt;
&lt;h3 id=&quot;Results&quot;&gt;Results&lt;a class=&quot;anchor-link&quot; href=&quot;#Results&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;the authors prove that Squeeze and Excitation do both improve the performance and can be easily added to most architectures. It yields an improvement independent of architecture, but the exact type of SE block used should be researched depending on the base architecture.&lt;/li&gt;
&lt;li&gt;Later SE layers learn close to the Identity mapping, so &lt;/li&gt;
&lt;li&gt;2-3% mAP improvement compared to ResNet backbone Faster-RCNN&lt;/li&gt;
&lt;li&gt;25% improvement on ImageNet top-5 error&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/se_block.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/se_block.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">PANet Paper Summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/27/PANet.html" rel="alternate" type="text/html" title="PANet Paper Summary" /><published>2020-06-27T00:00:00-05:00</published><updated>2020-06-27T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/27/PANet</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/27/PANet.html">&lt;h2 id=&quot;panet-paper-summary&quot;&gt;PANet Paper Summary&lt;/h2&gt;

&lt;h3 id=&quot;what-did-the-authors-want-to-achieve-&quot;&gt;What did the authors want to achieve ?&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/PAN.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;improve Computer Vision tasks object detection and mainly instance segmentation&lt;/li&gt;
  &lt;li&gt;build on top of Fast/ Faster /Mask RCNN and improve info propagation&lt;/li&gt;
  &lt;li&gt;design an architecture that can deal with blurry and heavy occlusion of the new datasets back then (2018) , like COCO 2017&lt;/li&gt;
  &lt;li&gt;use in-net feature hierarchy : top down path with lateral connections is augmented to emphasize strong sementical features&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;methods-used&quot;&gt;Methods used&lt;/h3&gt;
&lt;h4 id=&quot;findings&quot;&gt;Findings&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Mask RCNN : long path from low-level to topmost features, which makes it difficult to access localization information&lt;/li&gt;
  &lt;li&gt;Mask RCNN : only single view, multi view preferd to gather diverse information&lt;/li&gt;
  &lt;li&gt;Mask RCNN : predictions based on pooled feature grides that are assigned heuristically, can be updated since lower level info can be important for final prediction&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;contributions&quot;&gt;Contributions&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;PANet is proposed for instance segementation&lt;/li&gt;
  &lt;li&gt;1) bottom-up path augmentation to shorten information path and improve feature pyramid with accurate low-level information 
=&amp;gt; new : propagate low-level features to enhance the feature hierarchy for instance recogniton&lt;/li&gt;
  &lt;li&gt;2) Adaptive feature pooling is introduced to recover broken information between each proposal and all feature levels&lt;/li&gt;
  &lt;li&gt;3) for multi view : augmentation of mask prediction with small fc layers : more diverse info, masks have better quality&lt;/li&gt;
  &lt;li&gt;1) &amp;amp; 2) are both used for detection and segmentation and lead to improvements of both tasks&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;framework&quot;&gt;Framework&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/bottom_up.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bottom-up Path Augmentation&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Intuition : bottom up is augmented to easily propagate lower layers&lt;/li&gt;
  &lt;li&gt;We know that high layers respond to whole objects, lower layers to fine features&lt;/li&gt;
  &lt;li&gt;Localization can be enhanced with top-down paths (FPN)&lt;/li&gt;
  &lt;li&gt;Here a path from low levels to higher ones is built, based on higher layers respose to edges and instance parts which helps localization error&lt;/li&gt;
  &lt;li&gt;Approach follow FPN, also using ResNet : layers with same spatial size are in same feature space (b) in figure 1)&lt;/li&gt;
  &lt;li&gt;As shown in figure 2), each feature map takes a higher resolution feature map $N_{i}$ and a coarser map $P_{i+1}$ and generates a new one using a 3x3 conv with stride 2 for size reduction on each $N_{i}$ map. After that each element of $P_{i+1}$ and the down sampled map are added using lateral connection. The fused map is convoluted using another 3x3 kernel to generate $N_{i+1}$, the whole process is iterated until $P_{5}$ is reached. All convs are followed by a ReLU.&lt;/li&gt;
  &lt;li&gt;up to 0.9 mAP improvement with large input sizes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/mask_pred_pan.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Adaptive Feature Pooling&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Idea : Adapative feature pooling allows each propsal to access info from all levels&lt;/li&gt;
  &lt;li&gt;In FPN, small proposals are assigned to low level features and high proposals to higher level ones . This can be non-optimal, as e.g. 2 examples with 10-pixel difference can be assigned to different levels even though they are rather similar. Also features may not correlate strongly with the layer they belong to.&lt;/li&gt;
  &lt;li&gt;High-level features have a larger receptive field and a more global context, whereas lower ones have fine details and high-localization accuracy. Therefore pooling from all levels and all proposals is fused for prediction. The authors call it adaptive feature pooling. For fusion max operations are used. For each level a ratio of kept features is calculated, surprisingly almost 70% are from other higher levels. The findings show that, features from multi levels together are helpful for accurate prediction. An intuition that is similar to DenseNet.This also supports bottom-up augmentation.&lt;/li&gt;
  &lt;li&gt;The exact process can be seen in figure 1 c), at first each proposal is mapped to different feature levels. Then ROIAlign is used to pool grids from each level. After that fusion of feature grids from different levels is performed using an element-wise max or sum. The focus is on in net feature hierarchy, instead of using different levels from image pyramids. It is comparable to L2 norm, where concat and dimension reduction are used.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fully Connected Fusion&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Results have shown that MLP is also useful for pixelwise mask prediction. FC layers have different properties compared to FCN, FCN shares parameters and predicts based on local receptifve field. FC layers are localy sensitive since they do not use param sharing. They can adapt to different spatial locations. Using these ideas, it is helpful to differentiate parts in an image and use this information by fusing them.&lt;/li&gt;
  &lt;li&gt;Mask branch operates on pooled features and mainly consists of a FCN net with 4 conv (each 3x3 with 256 filters) and 1 deconv layer (upsample by 2). A shortcut from layer conv3 to fc is also implemented. The fc layer predicts a class agnostic foreground/background mask. It’s efficient and allows for better generality.&lt;/li&gt;
  &lt;li&gt;up to 0.7 mAP improvement for all scales&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Others&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MultiGPU Sync Batch Norm&lt;/li&gt;
  &lt;li&gt;Heavier head : effective for box AP&lt;/li&gt;
  &lt;li&gt;multi scale training&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Total improvement in AP is 4.4 over baseline, half of it is due to Synch BN and multi scale training&lt;/p&gt;

&lt;p&gt;Results&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Ablation study was done for architecture design&lt;/li&gt;
  &lt;li&gt;Winner of 2017 COCO Segmentation, SOTA performance in segmentation (Cityscapes) and detection&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">PANet Paper Summary</summary></entry><entry><title type="html">YOLOv4, a summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/24/YOLOv4.html" rel="alternate" type="text/html" title="YOLOv4, a summary" /><published>2020-06-24T00:00:00-05:00</published><updated>2020-06-24T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/24/YOLOv4</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/24/YOLOv4.html">&lt;h1 id=&quot;yolov4-optimal-speed-and-accuracy-of-object-detection-paper-summary&quot;&gt;YOLOv4: Optimal Speed and Accuracy of Object Detection Paper Summary&lt;/h1&gt;
&lt;p&gt;(Alexey Bochkovskiy,Chien-Yao Wang, Hong-Yuan Mark Liao)&lt;/p&gt;

&lt;h2 id=&quot;what-did-the-authors-want-to-achieve-&quot;&gt;What did the authors want to achieve ?&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;fast (real time) object detection, that can be trained on a GPU with 8-16 GB of VRAM&lt;/li&gt;
  &lt;li&gt;a model that can be easily trained and used&lt;/li&gt;
  &lt;li&gt;add state of the art methods for object detection, building on YOLOv3&lt;/li&gt;
  &lt;li&gt;find a good model for both GPU and VPU implementation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/yolov4.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;methods-used&quot;&gt;Methods used&lt;/h2&gt;

&lt;h3 id=&quot;bag-of-freebies-methods-that-only-increase-training-runtime-and-not-inference&quot;&gt;Bag of Freebies (methods that only increase training runtime and not inference)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;New Data Augmentation techniques are used, for photometric and geometric variability : 
=&amp;gt; Random Erase, Cutout to leave part of image of certain value
=&amp;gt; Dropout, Dropblock does the same with the net params&lt;/li&gt;
  &lt;li&gt;Mixup : mult image augmentation&lt;/li&gt;
  &lt;li&gt;Style Transfer GAN for texture stability
    &lt;h4 id=&quot;dataset-bias-&quot;&gt;Dataset Bias :&lt;/h4&gt;
    &lt;p&gt;=&amp;gt; focal loss, data imbalance between different classics
=&amp;gt; one-hot hard representation 
=&amp;gt; soft labels 
BBox regression :&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;MSE has x and y independent, also Anchors
=&amp;gt; IoU loss =&amp;gt; coverage and area are considered 
=&amp;gt; scale invariant, not the case with traditional methods
=&amp;gt; DIoU and CIoU loss&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bag-of-specials-methods-that-only-have-a-small-impact-on-inference-speed-but-improve-accuracy-significantly&quot;&gt;Bag of specials (methods that only have a small impact on inference speed, but improve accuracy significantly)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;only a small cost of compute during inference, improve accuracy :
    &lt;ul&gt;
      &lt;li&gt;enlarging receptive field : SPP, ASPP, RFB, SPP originates 		  from Spatial Pyramid Matching 
=&amp;gt; extract bag of words features 
 	- SPP infeasible for FCN nets, as it outputs a 1D feature 	  vector =&amp;gt; YOLOv3 concat of max pooling outputs with 		  kxk kernel size =&amp;gt; larger receptive field of the backbone 		  =&amp;gt; 2.7% higher AP50, 0.5 more compute necessary&lt;/li&gt;
      &lt;li&gt;ASPP diff. to SPP : max pool of 3x3, dilation of k&lt;/li&gt;
      &lt;li&gt;RFB : several dilated kxk convs =&amp;gt; 7% more AP, 5.7% more compute
Attention module :&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;mainly channelwise and pointwise attention, SE and SAM modules, SAM with no extra cost on GPU =&amp;gt; better for us                  &lt;br /&gt;
Feature Integration :&lt;/li&gt;
  &lt;li&gt;skip connections, hyper-column&lt;/li&gt;
  &lt;li&gt;channelwise weighting on multi-scale with FPN methods :
    &lt;ul&gt;
      &lt;li&gt;SFAM, ASFF, BiFPN, …
Activation Functions :
        &lt;ul&gt;
          &lt;li&gt;Mish, Swish, (fully differentiable) PReLu, …
Post-processing :&lt;/li&gt;
          &lt;li&gt;NMS “messy” with occlusions =&amp;gt; DIoU NMS distance : center to BBox screening process&lt;/li&gt;
          &lt;li&gt;NMS method not necessary in anchor free method&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture-selection&quot;&gt;Architecture Selection&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;things to think about : a reference that is good for classification is not always good for object detection, due to the detector needing :
    &lt;ul&gt;
      &lt;li&gt;higher input size (for small objects)&lt;/li&gt;
      &lt;li&gt;more layers (higher receptive field to cover larger input)&lt;/li&gt;
      &lt;li&gt;more parameters (for greater capacity, to detect different sized objects)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;=&amp;gt; a detector needs a backbone with more 3x3 convs and more params&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/yolov4comp.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Due to that, CSPDarknet53 seems to be the best choice in theory&lt;/p&gt;

&lt;p&gt;The following improvements are done :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SPP additional module for larger receptive field (almost no runtime disadvantage)&lt;/li&gt;
  &lt;li&gt;PANet path-aggregation neck as param aggregation method instead of FPN from YOLOv3&lt;/li&gt;
  &lt;li&gt;YOLOv3 anchor based head is used&lt;/li&gt;
  &lt;li&gt;DropBlock regularization method&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For single GPU training :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;synchBN is not considered : goal is to run on single GPU, thanks guys !!&lt;/li&gt;
  &lt;li&gt;new data augmentation mosaic (mixes 4 training images), Self Adversarial Training =&amp;gt; detection of objects outside their context&lt;br /&gt;
SAT : 
Forward Backward Training : 
1) adversarial attack is performed on input
2) neural net is trained to detect and object on this moded image in a normal way&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;optimal hyper-params while applying genetic algos&lt;/li&gt;
  &lt;li&gt;Cross mini Batch Normalization : mini-batch split within batch&lt;/li&gt;
  &lt;li&gt;SAM is modified from spatial-wise to pointwise attention, as can be seen below :&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/Pan_mod.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;architecture-summary&quot;&gt;Architecture Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Backbone : CSPDarknet53&lt;/li&gt;
  &lt;li&gt;Neck : SPP, PAN&lt;/li&gt;
  &lt;li&gt;Head : Yolov3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Techniques :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bag of freebies for Backbone : 
CutMix and Mosaic data augmentation, DropBlock reg., class label smoothing&lt;/li&gt;
  &lt;li&gt;Bag of freebies for detector : 
CIoU-loss, CmBN, DropBlock reg., Mosaic data augmentation, SAT, Eliminate gird sensitivity, multi anchors for a single ground truth, cosine annealing, hyperparams, random training shapes&lt;/li&gt;
  &lt;li&gt;Bag of Specials for Backbone : 
Mish activation, Cross-stage partial connections (CSP), Multi-
input weighted residual connections (MiWRC)&lt;/li&gt;
  &lt;li&gt;Bag of specials for detector : 
Mish act., SPP-block, SAM-block, PAN path-aggregation block, DIoU-NMS&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;A FPS /mAP (@Iou50) comparison to other detecors can be seen below :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/Yolo_comp.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">YOLOv4: Optimal Speed and Accuracy of Object Detection Paper Summary (Alexey Bochkovskiy,Chien-Yao Wang, Hong-Yuan Mark Liao)</summary></entry><entry><title type="html">Summary Gradient Descent Optimization Algorithms</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/27/Gradient.html" rel="alternate" type="text/html" title="Summary Gradient Descent Optimization Algorithms" /><published>2020-04-27T00:00:00-05:00</published><updated>2020-04-27T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/27/-Gradient</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/27/Gradient.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-27- Gradient.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;give an overview of the different optimizers that exist and the challenges faced&lt;/li&gt;
&lt;li&gt;analyze additional methods that can be used to improve gradient descent&lt;/li&gt;
&lt;li&gt;the paper is more geared towards the mathematical understandinf of these optimizers as it does not make performance comparisons  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Key-elements-(Methods-in-this-case)&quot;&gt;Key elements (Methods in this case)&lt;a class=&quot;anchor-link&quot; href=&quot;#Key-elements-(Methods-in-this-case)&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In the following all sorts of SGD variants and optimization algorithms will be adressed. The following parameters are universal in the upcoming formulas:&lt;/p&gt;
&lt;p&gt;θ : parameters&lt;br /&gt;
 η : learning rate&lt;br /&gt;
J(θ) : objective function depening on models parameters&lt;/p&gt;
&lt;h2 id=&quot;SGD-Based&quot;&gt;SGD Based&lt;a class=&quot;anchor-link&quot; href=&quot;#SGD-Based&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Batch-Gradient-Descent&quot;&gt;Batch Gradient Descent&lt;a class=&quot;anchor-link&quot; href=&quot;#Batch-Gradient-Descent&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;h4 id=&quot;Formula-:&quot;&gt;Formula :&lt;a class=&quot;anchor-link&quot; href=&quot;#Formula-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;$θ = θ − η · ∇θJ(θ)$&lt;/p&gt;
&lt;h4 id=&quot;Code-:&quot;&gt;Code :&lt;a class=&quot;anchor-link&quot; href=&quot;#Code-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the vanilla version, it updates all parameters in one update. It is redundant and very slow and can be impossible to compute for large datasets due to memory limitations. It also does not allow for online training.&lt;/p&gt;
&lt;h3 id=&quot;Stochastic-Gradient-Descent-(SGD)&quot;&gt;Stochastic Gradient Descent (SGD)&lt;a class=&quot;anchor-link&quot; href=&quot;#Stochastic-Gradient-Descent-(SGD)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;SGD performs a parameter update for each training example ${x^i}$ and ${y^i}$ respectively and therefore allows online training as well as faster training and the ability to train larger datasets.&lt;/p&gt;
&lt;h4 id=&quot;Formula-:&quot;&gt;Formula :&lt;a class=&quot;anchor-link&quot; href=&quot;#Formula-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;$θ = θ − η · ∇θJ(θ; {x^i};{y^i})$&lt;/p&gt;
&lt;h4 id=&quot;Code-:&quot;&gt;Code :&lt;a class=&quot;anchor-link&quot; href=&quot;#Code-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;SGD also has the potential to reach better local minima, convergance is compilcated however but can be combated with weight decay,almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively. As is common for most batch gradient descent techniques the data is also shuffled in the beginning, the advantages of shuffling are discussed in the additional strategies section.&lt;/p&gt;
&lt;h3 id=&quot;Mini-Batch-Gradient-Descent-(most-commonly-refered-to-as-the-actual-SGD)&quot;&gt;Mini-Batch Gradient Descent (most commonly refered to as the actual SGD)&lt;a class=&quot;anchor-link&quot; href=&quot;#Mini-Batch-Gradient-Descent-(most-commonly-refered-to-as-the-actual-SGD)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Mini-Batch Gradient Descent  takes the best of both worlds and performs updates on smaller batches (common sizes range between 16 and 1024).&lt;/p&gt;
&lt;h4 id=&quot;Formula-:&quot;&gt;Formula :&lt;a class=&quot;anchor-link&quot; href=&quot;#Formula-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;$θ = θ − η · ∇θJ(θ; {x^{i:i+n}};{y^{i:i+n}})$&lt;/p&gt;
&lt;h4 id=&quot;Code-:&quot;&gt;Code :&lt;a class=&quot;anchor-link&quot; href=&quot;#Code-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_batches&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It is usually refered to as SGD. This technique reduces the variance, which leads to better convergence. It can also make use of highly computationally optimized matrix operations to make computing the gradient of a mini-batch very fast.&lt;/p&gt;
&lt;h4 id=&quot;Challenges-of-SGD&quot;&gt;Challenges of SGD&lt;a class=&quot;anchor-link&quot; href=&quot;#Challenges-of-SGD&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Vanilla SGD does not guarantee good convergence, so the following challenges have to be adressed :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;choosing the learning rate is very difficult, small values can lead to very slow convergence while large ones can lead to divergence&lt;/li&gt;
&lt;li&gt;techniques that combat this like annealing and scheduling of the $\eta$ are used to combat this, however they do not adjust to the dataset used and have to be specified in advance&lt;/li&gt;
&lt;li&gt;the learning rate is fixed for all parameters, however we would like to updates each parameter depending on the occurence of features (e.g. higher lr for rarer features) &lt;/li&gt;
&lt;li&gt;minimizing non-convex functions, where the convex criterion :       &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$f(\lambda*x_{1}+(1-\lambda)*x_{2}) {\leq } \lambda*f(x_{1}) + (1-\lambda) * f(x_{2})$&lt;/p&gt;
&lt;p&gt;is not fullfilled, often leads to convergence in local minima. Often this is due to saddle points as Dauphin et al. found out.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Gradient-descent-optimization-algorithms&quot;&gt;Gradient descent optimization algorithms&lt;a class=&quot;anchor-link&quot; href=&quot;#Gradient-descent-optimization-algorithms&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In the following widely used optimization algorithms will be adressed. The following parameters are universal in the upcoming formulas:&lt;/p&gt;
&lt;p&gt;θ : parameters&lt;br /&gt;
 η : learning rate&lt;br /&gt;
J(θ) : objective function depening on models parameters&lt;br /&gt;
 $γ$ : fraction that was introduced with Momentum (usually set to 0.9)&lt;br /&gt;
 $t$ (index) : time step&lt;br /&gt;
 $\epsilon$ : smoothing term to avoid division by zero, introduced by Adagrad (usuall set to 1e-8)&lt;/p&gt;
&lt;h3 id=&quot;Momentum&quot;&gt;Momentum&lt;a class=&quot;anchor-link&quot; href=&quot;#Momentum&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Momentum.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;SGD has trouble navigating ravines (area where surface curves much more steeply in one dimension than in another), SGD often oscillates in these cases while only making small progress. These are common around local optima. 
Momentum helps accelerate SGD in the relevant direction and dampens these ocsillations by introducing a new $γ$ term that is used to add a fraction of the update vector of the past time step to the current update vector.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Formula-:&quot;&gt;Formula :&lt;a class=&quot;anchor-link&quot; href=&quot;#Formula-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;$vt = γv_{t−1} + η∇θJ(θ)$&lt;br /&gt;
$θ = θ − v_{t}$
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;An analogy to this is a ball rolling dowhill, it becomes faster on the way (until it reaches its maximum speed due to air resistance, i.e. $γ&amp;lt;1$)&lt;/p&gt;
&lt;h3 id=&quot;Nesterov-accelerated-gradient-(NGA)&quot;&gt;Nesterov accelerated gradient (NGA)&lt;a class=&quot;anchor-link&quot; href=&quot;#Nesterov-accelerated-gradient-(NGA)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The idea of NGA is that we do not want to blindly go downhill, but rather know when the uphill section starts so we can slow down again. NGA does take this into account by using our momentum term $γv_{t−1}$ to approximate the next time step using $θ − γv_{t}$. (only the gradient is missing for the full update). This allows us to compute the gradient w.r.t the approximate future position of our parameters rather than the current one. 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Formula&quot;&gt;Formula&lt;a class=&quot;anchor-link&quot; href=&quot;#Formula&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;$vt = γv_{t−1} + η∇θJ(θ-γv_{t−1})$&lt;br /&gt;
$θ = θ − v_{t}$
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Hinton_mom.png&quot; alt=&quot;&quot; /&gt;
NAG first makes a jump in the direction of calculated gradient (brown vector) and then makes a correction (green vector) 
This anticipatory update prevents us from going too fast and results in increased
responsiveness, which has significantly increased the performance of RNNs on a number of tasks. Now we are able to update e&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Results-and-Conclusion&quot;&gt;Results and Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Results-and-Conclusion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The authors were able to give a good overview of different SGD techniques and it's optimizers like Momentum, Nesterov accelerated gradient, Adagrad, Adadelta,
RMSprop, Adam, AdaMax, Nadam, as well as different algorithms to optimize asynchronous SGD. Additionally strategies that can improve upon this like curriculum learning, batch norm and early stopping were deployed.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/resnet_var.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/resnet_var.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Summary LSUV Paper</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/26/LSUV.html" rel="alternate" type="text/html" title="Summary LSUV Paper" /><published>2020-04-26T00:00:00-05:00</published><updated>2020-04-26T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/26/LSUV</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/26/LSUV.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-26-LSUV.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;improve training of deep nets&lt;/li&gt;
&lt;li&gt;generalize Xavier initalization to activations other than ReLU (Kaiming init.), such as tanh and maxout &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Key-elements&quot;&gt;Key elements&lt;a class=&quot;anchor-link&quot; href=&quot;#Key-elements&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;LSUV extends orthogonal initalization and consists of two steps :&lt;/p&gt;
&lt;p&gt;1) Fill the weights with Gaussian noise with unit variance&lt;br /&gt;
2) Decompose to orthonormal basis with QR or SVD decomposition and replace the weights with one of the components.&lt;/p&gt;
&lt;p&gt;LSUV then estimates the variance of each convolution and inner product layer, the variance is scaled to equal one. It is worth mentioning that the batch size is neglactable in wide margins. 
In total LSUV can be seen as orthonormal initialization with batch norm applied at the first mini-batch. The orthonormal initalization of weights matrices de-correlates layer activations, a batch norm similarity is the unit variance normalization procedure. When compared to traditional batch norm, the results are sufficient and computationally more efficient. (Batch Norm adds about 30% in compute complexity to the system).It is not always possible to normalize the variance with the desired precision due to inconsistencies in data variations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/lsuv_algo.png&quot; alt=&quot;&quot; /&gt;
The pseudocode for LSUV can be seen above, in order to restrict the number of maximum trials (avoid infinite loops) a $T_{max}$ is set. 1-5 iterations are required for the desired variance.&lt;/p&gt;
&lt;h3 id=&quot;Implementation&quot;&gt;Implementation&lt;a class=&quot;anchor-link&quot; href=&quot;#Implementation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;An implementation tutorial, powered by fastai can be found &lt;a href=&quot;https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/15/LSUV.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Results-and-Conclusion&quot;&gt;Results and Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Results-and-Conclusion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;CIFAR-10/100&quot;&gt;CIFAR 10/100&lt;a class=&quot;anchor-link&quot; href=&quot;#CIFAR-10/100&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/lsuv_cifar.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;As we can see the FitNet with LSUV outperforms other techniques, but is virtually the same as orthonormal initalization. SGD was used with a learning rate of 0.01 and weight decay @ epoch 10/150/200 for 230 epochs in total.&lt;/p&gt;
&lt;h3 id=&quot;Analysis-of-empircal-results&quot;&gt;Analysis of empircal results&lt;a class=&quot;anchor-link&quot; href=&quot;#Analysis-of-empircal-results&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;For FitNet-1 the authors did not experience any problems with any of the activation functions that they used (ReLU,maxout,tanh) optimizers (SGD,RMSProp) or initalizaton techniques (Xavier,MSRA,Ortho,LSUV). This is most likely due to the fact that CNNs tolerate a wide range of mediocre inits, only the training time increases. However FitNet-4 was much more difficult to optimize.&lt;/p&gt;
&lt;p&gt;Training a FitResNet-4 on CIFAR-10, which tests the initalization with ResNet training &quot;out-of-the-box&quot;, LSUV is proven to be the only initalization technique that leads all nets to converge regardless of the activation function that was used : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/lsuv_he.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;LSUV-compared-to-Batch-Norm&quot;&gt;LSUV compared to Batch Norm&lt;a class=&quot;anchor-link&quot; href=&quot;#LSUV-compared-to-Batch-Norm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;LSUV can be seen as batch norm of layer output done before the start of training. The authors also prove that putting batch norm after the activation function is proven to work for FitNet-4.&lt;/p&gt;
&lt;h3 id=&quot;ImageNet-training&quot;&gt;ImageNet training&lt;a class=&quot;anchor-link&quot; href=&quot;#ImageNet-training&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/LSUV_im.png&quot; alt=&quot;&quot; /&gt;
When training on ImageNet the authors found out that, LSUV reduces the starting flat-loss time from 0.5 epochs to 0.05 for CaffeNet. It also converges faster in the beginning, but is then overtaken by a standard CaffeNet architecture at the 30th epoch and has a 1.3% lower precision in the end. The authors of the paper do not have any explanation for this empirical phenomenon. Especially since in contrast GoogLeNet performed better (0.68 compared to 0.672)&lt;/p&gt;
&lt;h3 id=&quot;LSUV-Timing&quot;&gt;LSUV Timing&lt;a class=&quot;anchor-link&quot; href=&quot;#LSUV-Timing&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The significant part of LSUV is SVD-decomposition of the weight matrices. The compute overhead on top of generating the Gaussian noise (that's almost instant) is about 3.5 Minutes for CaffeNet, which is very small compared to total training time.&lt;/p&gt;
&lt;p&gt;The authors state that the experiments confirm the finding of Romero et al. (2015) that very thin, thus fast and low in parameters, but deep networks obtain comparable or even better performance than wider, but shallower nets. LSUV is fast and the results are almost state-of-the art.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/resnet_var.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/resnet_var.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Summary Mixup Augmentation Technique</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/25/Mixup.html" rel="alternate" type="text/html" title="Summary Mixup Augmentation Technique" /><published>2020-04-25T00:00:00-05:00</published><updated>2020-04-25T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/25/Mixup</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/25/Mixup.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-25-Mixup.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Problems-of-existing-rechniques-:&quot;&gt;Problems of existing rechniques :&lt;a class=&quot;anchor-link&quot; href=&quot;#Problems-of-existing-rechniques-:&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;ERM memorizes, does not generalize -&amp;gt; vulnerable to adversarials&lt;/li&gt;
&lt;li&gt;Classic data augmentation is used to define neighbors in a single class, this is mostly hand crafted by humans and does not consider multi class combinations
## What did the authors want to achieve ?&lt;/li&gt;
&lt;li&gt;improve on undesirable memorization of corrupted labels and sensitivity to adversarial examples &lt;/li&gt;
&lt;li&gt;stabilize training (especially of GANs)&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Key-elements&quot;&gt;Key elements&lt;a class=&quot;anchor-link&quot; href=&quot;#Key-elements&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The Expected Risk (ER), assuming that the joint distribution between a random input $X$ and a label $Y$ $P(X,Y)$ is an empirical distribution :&lt;br /&gt;
$P_{\delta}(x,y) = \frac{1}{n}\sum_{i=1}^{n}\delta(x=x_{i},y=y_{i})$&lt;/p&gt;
&lt;p&gt;We can infer the approximation of the expected risk $R_{\delta}(f)$ since $dP_{\delta}(x,y) = 1$, and the loss $l(f(x),y) = \sum_{i=1}^{n}l(f(x_{i}),y_{i})$  :&lt;/p&gt;
&lt;p&gt;$R_{\delta}(f) = \int l(f(x),y)*dP_{\delta}(x,y)=\sum_{i=1}^{n}l(f(x_{i}),y_{i})$&lt;/p&gt;
&lt;p&gt;The Empirical Risk Minimization (ERM) (Vapnik,1998) is known as learning our function $f$ by minimizing the loss. $P_{\delta}$ is a naive estimation as it is one of many possible choices. The paper also mentions Vicinal Risk Minimization (Chapelle et al., 2000) which assumes the distribution P to be a sum of all the inputs and labels over a vicinity distribution $v(\tilde{x},\tilde{y}|x_{i},y_{i})$ and measures the probability of finding the virtual feature target  pair $(\tilde{x},\tilde{y})$ in the vicinity of the training feature-target pair $(x_{i},y_{i})$. The approach considered Gaussian vicinities, which is equal to augmenting the training data with additive Gaussian noise. Considering a Dataset of size m, we can infer the empirical vicinal risk :&lt;/p&gt;
&lt;p&gt;$R_{v}(f) = \frac{1}{m}\sum_{i=1}^{m}l(f(\tilde{(x_{i}}),\tilde{y_{i}})$&lt;/p&gt;
&lt;p&gt;This paper introduces a different generic vicinal distribution, called mixup : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/mixup_mu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;With $\lambda = [0,1]$ and $(\tilde{x_{i}},\tilde{y_{i}})$ &amp;amp; $(\tilde{x_{j}},\tilde{y_{j}}) being 2 random target vectors$. The mixup parameter $\alpha$ controls the strength of interpolation between the pair, as $\alpha \rightarrow  0$ it increasingly recovers to the ERM principle.&lt;/p&gt;
&lt;p&gt;The implementation from the paper is relatively straightforward :&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loader1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What does it do ? 
Mixup makes the model behave linearly in-between classes/examples, this reduces the amount of oscillations when facing a new example that is outside of the training examples. This happens linearily and is simple and therefore a good bias from the Occam's razor point of view ( &quot;Entities should not be multiplied without necessity.&quot;).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Results-and-Conclusion&quot;&gt;Results and Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Results-and-Conclusion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The authors prove that mixup is a very effective technique across all domains.&lt;/p&gt;
&lt;h3 id=&quot;Classification&quot;&gt;Classification&lt;a class=&quot;anchor-link&quot; href=&quot;#Classification&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/mixup_class.png&quot; alt=&quot;&quot; /&gt;
Experiments were made on both ImageNet and CIFAR-10/100 using different alphas for each dataset. $\alpha[0.1,0.4]$ for ImageNet and $\alpha = 1$ for CIFAR-10/100. Comparisons were made using different DenseNet and ResNet models. You can refer to the paper for the exact hyperparameters used in each case. As we can see in the graph above, miuxp outperforms their non-mixup counterparts. Learning rate decays were used @ epochs 10/60/120/180 with an inital value of 0.1 and training was done for a total of 300 epochs.&lt;/p&gt;
&lt;h3 id=&quot;Speech-Data&quot;&gt;Speech Data&lt;a class=&quot;anchor-link&quot; href=&quot;#Speech-Data&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The google command dataset was used which contains 30 classes of 65000 examples. LeNet and VGG-11 were used with ADAM and a learning rate of 0.001 and mixup variants with alphas of 0.1 and 0.2 were compared to the ERM counterparts. Mixup was able to outperform ERM with VGG-11 which had the lowest error of all models (3.9 percent) on the validation set. LeNet however performed better without mixup applied. Comparing these outcomes, the authors infer that mixuxp works particularly well with higher capacity models.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/mixup_tables.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Memorization-of-corrupted-labels-(Table-2)&quot;&gt;Memorization of corrupted labels (Table 2)&lt;a class=&quot;anchor-link&quot; href=&quot;#Memorization-of-corrupted-labels-(Table-2)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Robustness to corrupted labels is compared to ERM and mixup. An updated version of CIFAR is used where 20%,50% and then 80% are replaced by random noise. Usually Dropout was considered to be the best technique for corrupted label learning, so dropout with p ∈ {0.5, 0.7, 0.8, 0.9} and mixup are compared to each other along with a combo of both where α ∈ {1, 2, 4, 8} and p ∈ {0.3, 0.5, 0.7}.  The PreAct ResNet-18 (He et al., 2016) model implemented in (Liu, 2017) was used. The model was trained for 200 epochs.&lt;/p&gt;
&lt;h3 id=&quot;Robustness-to-adversarial-example-(Table-3)&quot;&gt;Robustness to adversarial example (Table 3)&lt;a class=&quot;anchor-link&quot; href=&quot;#Robustness-to-adversarial-example-(Table-3)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Several methods were proposed, such as penalizing the norm of the Jacobian of the model to control the Lipschitz constand of it. Other approaches perform data augmentation on adversarial examples. All of these methods add a lot of compute overhead to ERM. Here the authors prove that mixup can improve on ERM significantly without adding significant compute overhead by penalizing the norm of the gradient of the loss wrt a given input along the most plausible directions (the directions of other training points). ResNet101 models were trained 2 models were trained on ImageNet with ERM and one was trained with mixup. White box attacks are tested at first . For that, the model itself is used to generate adversarial examples fusing FGSM or iterative FGSM methods, 10 iterations with equal step sizes are used, $\epsilon=4$ was set as maximum perturbation for each pixel. Secondly black box attacks are done, this is achieved by using the first ERM model to produce adversarials with FGSM and I-FGSM. Then the robustness of the second ERM model and the mixp model to these exampels is tested.&lt;/p&gt;
&lt;p&gt;As we can see in table 3, mixup outperforms ERM in both cases significantly, being p to 2.7 times better when it comes to Top-1 error in the FGSM white box attack category.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/mixup_tables2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Tabular-Data-(Table-4)&quot;&gt;Tabular Data (Table 4)&lt;a class=&quot;anchor-link&quot; href=&quot;#Tabular-Data-(Table-4)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;A series of experiments was performed on the UCI dataset. A FCN with 2 hidden layers &amp;amp; 128 ReLU units was trained for 10 epochs with Adam and a batch size of 16. Table 4 shows that mixup improves test error significantly.&lt;/p&gt;
&lt;h3 id=&quot;Stabilize-GAN-Training-(Table-5)&quot;&gt;Stabilize GAN Training (Table 5)&lt;a class=&quot;anchor-link&quot; href=&quot;#Stabilize-GAN-Training-(Table-5)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;When training GANs, the mathematical goal is to solve the optimization problem :&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/gan_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;the binary cross entropy loss is used in this case. Solving this equation is a very difficult optimization problem (Goodfellow et al., 2016) as the discriminator often provides the generator with vanishing gradients. Using mixup, the optimization problem looks like this : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/gan_mix.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The models are fully connected, with 3 hidden layers and 512 ReLU units. The generator accepts 2D Gaussian noise vectors, 20000 mini-batches of size 128 are used for training with the discriminator being trained for 5 epochs before every generator iteration. Table 5 shows the improved performance of mixup.&lt;/p&gt;
&lt;p&gt;A part of these mostly supervised use cases (except for GANs), the authors believe that other non straightforward use cases such as segmentation or different unsupervised techniques should be promising areas of future research.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/mixup_tables.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/mixup_tables.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>