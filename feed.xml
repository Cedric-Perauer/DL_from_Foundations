<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://cedric-perauer.github.io/DL_from_Foundations/feed.xml" rel="self" type="application/atom+xml" /><link href="https://cedric-perauer.github.io/DL_from_Foundations/" rel="alternate" type="text/html" /><updated>2021-01-21T14:44:33-06:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/feed.xml</id><title type="html">fastpages</title><subtitle>An easy to use blogging platform with support for Jupyter Notebooks.</subtitle><entry><title type="html"></title><link href="https://cedric-perauer.github.io/DL_from_Foundations/2021/01/21/2021-01-21-SCALEDYOLOv4.html" rel="alternate" type="text/html" title="" /><published>2021-01-21T14:44:33-06:00</published><updated>2021-01-21T14:44:33-06:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/2021/01/21/2021-01-21-SCALEDYOLOv4</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/2021/01/21/2021-01-21-SCALEDYOLOv4.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-21-SCALEDYOLOv4.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Goal-of-the-Authors&quot;&gt;Goal of the Authors&lt;a class=&quot;anchor-link&quot; href=&quot;#Goal-of-the-Authors&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;design an efficient model scaling technique, according to them : EfficientDet, SpineNet,RegNet 
  scaling is all good work but they do not analyze the relation between parameters like model depth, model witdh,
  input resolution, group width, bottleneck ratio&lt;/li&gt;
&lt;li&gt;the good performance of normal YOLOv4 is due to the CSP Darknet53 Backbone fulffilling optimal parameters as 
found through architecture search (depth of 65, bottleneck ratio of 1 and width growth ratio of 2)   &lt;/li&gt;
&lt;li&gt;based on YOlOv4, they develop YOLOv4-CSP and based on that they scale to YOLO Scaled&lt;/li&gt;
&lt;li&gt;they aim to design a good scaling technique for both large and small models, analyze the relations between scaling 
factors&lt;/li&gt;
&lt;li&gt;FPN is confirmed to be a once for all model element&lt;/li&gt;
&lt;li&gt;model with fast runtime (2 stage detectors are not part of this), anchor based approaches are not as nice as CenterNet but perform better &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Methods/Structure&quot;&gt;Methods/Structure&lt;a class=&quot;anchor-link&quot; href=&quot;#Methods/Structure&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Scaling-Larger-Models&quot;&gt;Scaling Larger Models&lt;a class=&quot;anchor-link&quot; href=&quot;#Scaling-Larger-Models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;When looking at the compute cost of different layer types in relation to input resolution $\alpha$, number of layers $\beta$ and number of channels  $\gamma$ the authors find the following : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/scaling.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Using CSP Connections, compute cost can be reduced to :&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/csp_scale.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Further advantages of CSP are accuracy gains and faster inference, therefore no tradeoff exists.&lt;/p&gt;
&lt;p&gt;The different scaling factors for an object detection model are displayed in the table below :&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/csp_scalefacts.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&quot;The biggest difference between image classification and object detection is that the former only needs to identify the category of the largest component in an image, while the lat- ter needs to predict the position and size of each object in an image.&quot;&lt;/p&gt;
&lt;p&gt;This ability for a single stage object detector (the receptive field) is most accurately described by the stage. Higher stages are more suitable for predicting larger objects (FPN). The effect of the receptive field according to different parameters is shown below :&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/scale_recept.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Scaling-Models-for-Embedded-Devices&quot;&gt;Scaling Models for Embedded Devices&lt;a class=&quot;anchor-link&quot; href=&quot;#Scaling-Models-for-Embedded-Devices&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The authors consider DRAM and inference speed and select a criteria for models that can run on lower end devices in real time. The criterion for this is a runtime that scales with at most $O(w*h*k*b^{2})$&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/scale_small.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Minimize/balance-size-of-feature-map&quot;&gt;Minimize/balance size of feature map&lt;a class=&quot;anchor-link&quot; href=&quot;#Minimize/balance-size-of-feature-map&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Because OSANet has a lower compute cost $O(max(whbg,whkb^{2}))$ than Dense Net $O(whgbk)$&lt;/p&gt;
&lt;p&gt;The authors also emphasize on minimizing the feature maps. Feature maps have to be treated as a whole in terms of a block (Dense Net, ResNet Block). 
The feature that  &quot;Because the computational block of OSANet belongs to the PlainNet architecture, making CSPNet from any layer of a computational block can achieve the effect of gradient truncation&quot; is exploited to efficiently partition the paths from $b+kg$ to $(b+kg)/2$. When hardware latency τ is considered, this results in : $ceil((b + kg)/2τ ) × τ$&lt;/p&gt;
&lt;h4 id=&quot;Maintain-the-same-number-of-channels-after-convolution&quot;&gt;Maintain the same number of channels after convolution&lt;a class=&quot;anchor-link&quot; href=&quot;#Maintain-the-same-number-of-channels-after-convolution&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In terms of power consumption, memory access cost is also a large factor on embedded Hardware. 
It can be calculated as follows :&lt;/p&gt;
&lt;p&gt;$MAC = hw(C_{in} + C_{out}) + KC_{in}C_{out}$&lt;/p&gt;
&lt;p&gt;h,w, C (in and out) and K are height and width of feature map, the channel number of input and output, and the kernel size of the convolutional filter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/conv_eff.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Minimize-Convolutional-Input/Output-(CIO)&quot;&gt;Minimize Convolutional Input/Output (CIO)&lt;a class=&quot;anchor-link&quot; href=&quot;#Minimize-Convolutional-Input/Output-(CIO)&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;CIO can measure the DRAM in and output. When kg &amp;gt; b/2 CSPOSANET minimizes this according to:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/io_eff.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Scaling&quot;&gt;Scaling&lt;a class=&quot;anchor-link&quot; href=&quot;#Scaling&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;h4 id=&quot;Backbone&quot;&gt;Backbone&lt;a class=&quot;anchor-link&quot; href=&quot;#Backbone&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&quot;In the design of CSPDarknet53, the computation of down-sampling convolution for cross-stage process is not included in a residual block.&quot; That's why the CSPDarknet53 has a compute of whb2(9/4+3/4+5k/2). From the runtime analysis in the beginning, CSP Net only has a better runtime than CSPDarknet when k &amp;gt; 1. Therefore in order to keep a decent runtime, the first CSP stage is converted to a Darknet layer :&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/dark_csp.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Neck&quot;&gt;Neck&lt;a class=&quot;anchor-link&quot; href=&quot;#Neck&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;PANet Neck of Yolov4 is made 40% more compute efficient by CSPizing it.&quot; It mainly intagrates the features from different feature pyramids, and then passes through two sets of reversed Darknet residual layer without shortcut connections.&quot;&lt;/p&gt;
&lt;h4 id=&quot;SPP&quot;&gt;SPP&lt;a class=&quot;anchor-link&quot; href=&quot;#SPP&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The SPP module is also kept inside of the CSP module analgous to the original approach.&lt;/p&gt;
&lt;h3 id=&quot;Models&quot;&gt;Models&lt;a class=&quot;anchor-link&quot; href=&quot;#Models&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The scaling approach allows the creation of a tiny model and different large models.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/yolo-tiny.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;For the large model, 3 different large models are created which are fully CSPized. YOLOv4-P5 can reach 60 FPS with the right width scaling factor. It outperforms EfficientDet-D5 slightly while being 1.9 times faster. The tiny version performs at speeds up to 290 FPS (Xavier AGX, FP16 in Tensorrt) and is more than real time capable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/yolo-large.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author></entry><entry><title type="html">End to End Learning for Autonomous Driving</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/12/12/DRIVE.html" rel="alternate" type="text/html" title="End to End Learning for Autonomous Driving" /><published>2020-12-12T00:00:00-06:00</published><updated>2020-12-12T00:00:00-06:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/12/12/DRIVE</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/12/12/DRIVE.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-12-12-DRIVE.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.05909.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;What-is-the-goal-of-the-authors-?&quot;&gt;What is the goal of the authors ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-is-the-goal-of-the-authors-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Addressing current approaches in End-to-End learning and showing it's potentials/challenges and training methods. Currently, the state of the art is based on modular architectures, where single tasks are learned and analyzed. While some of these modules might be learned, the interconnection between modules (for example bounding boxes as Perception stack outputs) is highly abstracted and therefore very limited. This offers several advantages such as easier debugging and better interpretability. Due to the number of taks that have already been solved with end-to-end learning, (chess, GO, DOTA2, ATARI games,...) it seems reasonable to think that end-to-end will be crucial in the near future. Especially since humans are able to control car while they are tired and/or distracted.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Methods&quot;&gt;Methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Methods&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Imitation-Learning-(or-behavior-cloning)&quot;&gt;Imitation Learning (or behavior cloning)&lt;a class=&quot;anchor-link&quot; href=&quot;#Imitation-Learning-(or-behavior-cloning)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Most dominant approach in end-to-end learning, where the goal it to mimic the behavior of an expert that teaches the system. The nature of this approach makes it simple to collect large amounts of data (Tesla Shadow mode for example). Complicated scenarios still remain a challenge $[1]$, as well as the distribution shift (data unseen during training) problem. Potential solutions to this are data augmentation, data diversification and on-policy learning. Futhermore it remains challenging to find cases that can be classified as anomalies and when a disengagement by the driver results in an actual edge case. Tesla described this challenge in some of their presentations, as it is important to evaluate wheter data by a specific driver is desirable or not.&lt;/p&gt;
&lt;h4 id=&quot;Augmentation&quot;&gt;Augmentation&lt;a class=&quot;anchor-link&quot; href=&quot;#Augmentation&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In this scenario, augmentation in the image domain relates to the well known techniques (such as cropping, cutting, blurring, darkening/brightening, ...). Driving commands should be augmented correspondingly (steering angle has to be flipped when image is flipped left-right for example). Additionally, techniques such as binning and discarding data can be used to increase the dataset balance.&lt;/p&gt;
&lt;h4 id=&quot;Diversification&quot;&gt;Diversification&lt;a class=&quot;anchor-link&quot; href=&quot;#Diversification&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;During the data gathering process, diversifying should be one of the goals. Noise can also be added for further diversification of the data. This approach may allow for unseen scenarios to occur, but should be handled with care as it could lead to suboptimal behavior of the model during real world driving. Possible perubtations are shown below. 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/DIVER.png&quot; alt=&quot;&quot; /&gt; 
$[2][3]$&lt;/p&gt;
&lt;h4 id=&quot;On-policy-learning&quot;&gt;On-policy learning&lt;a class=&quot;anchor-link&quot; href=&quot;#On-policy-learning&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Since decisions made by the autonomous agents can lead to the car drifing away from the optimal trajectory, DAgger $[4]$ proposes to switch between model and data during data collection. Essentially the expert trains the model to recover from mistakes that may occur during it's application. This also allows for question answering applications in case of uncertainty and allows for both offline or online annotations (SafeDAgger $[5]$). Researchers show that this sort of annotation can also be done by using a conventional localization stack during development.&lt;/p&gt;
&lt;p&gt;As already mentioned, dataset inbalance is a problem due to both data frequency and data point difficulty. Sampling difficult datapoints more often during training might help the prediction error on these outliers. Nonetheless, this can lead to &quot;easier&quot; samples not being classified correctly anymore. Moreover, the non convex optimization problem of deep learning gives us no guarantee that the model will converge to the same minimum every time.&lt;/p&gt;
&lt;h3 id=&quot;Reinforcement-Learning&quot;&gt;Reinforcement Learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Reinforcement-Learning&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Since RL learns the driving policy through it's own experience, it is more immune to potentially insufficient data samples. This still remains a huge challenge as policies for the reward are difficult to create and increasinlgy more difficult to interpret with higher complexity. As learning in the real world can be challenging, it may be possible to improve IL models with RL experience and/or run an RL model on the car that runs on it while not taking control of the actuators. Outputs can then be compared with an IL model or interpreted for further deployment. The largest challenge of RL in the context of autonomous driving remains sim2real transfer, which would allow for experienced agents who were trained in simulation to be deployed in a real car.&lt;/p&gt;
&lt;h3 id=&quot;Sim2real-Transfer-Learning&quot;&gt;Sim2real Transfer Learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Sim2real-Transfer-Learning&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;In order to reduce the impact of the sim2real gap, models can be retrained or even operate on data that abstracts from the real world (e.g. segmentation maps) $[6]$  this allows for training of the driving policy to be independet of the testing domain. Conidtional GANs can be used to generate real looking images from simulator data $[7]$, this data can then be used for model training.$[8]$ uses 2 autoencoders to map real world and simulation images to a shared lower dimensional latent space. The behavior of driving models and physcics models is not mentioned in the paper. 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/AUTO.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Inputs&quot;&gt;Inputs&lt;a class=&quot;anchor-link&quot; href=&quot;#Inputs&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Apart from sensor inputs by lidar, camera,imus, HD maps and other sensors should be used as inputs. Architectures that use map representations as input for end-to-end driving are also called mid-to-mid. Fused approaches are also possible, these might cooperate different sensors or timesteps. The use of semantic representations can lead to improved robustness of the model $[9]$. Using vehicle states such as speed and acceleration  can lead to the inertia problem during imitation learning as shown in $[10]$. Navigational Inputs are mostly provided as commands or route planners, ChauffeurNet presents the following top-down map for this purpose : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/CN.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Optionaly text commands could be explored as mentioned by the authors. This may already be deployed in a L4 ride hailing service to some extent.&lt;/p&gt;
&lt;h3 id=&quot;Outputs&quot;&gt;Outputs&lt;a class=&quot;anchor-link&quot; href=&quot;#Outputs&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Outputs can be traditional data such as steering and speed or waypoints or advanced representations like cost maps. These cost maps can be used to generate trajectories using optimization techniques like MPC. A representation of this is shown here : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/cost_map.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Frm the paprer : 
&quot;Direct perception $[10]$ approaches aim to fall between
modular pipelines and end-to-end driving and combine the
benefits of both approaches. Instead of parsing all the objects
in the driving scene and performing robust localization (as
modular approach), the system focuses on a small set of crucial
indicators, called affordances.&quot; This means that the pose of the car in lane and position of other objects relative to it are kown and then can be used to detect affordances in the way of the car : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/afford.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Mutlitask learning predicts multiple outputs at the same time, rich information exists within the layers, which can also be used for debugging and interpretation.&lt;/p&gt;
&lt;h2 id=&quot;Evaluation&quot;&gt;Evaluation&lt;a class=&quot;anchor-link&quot; href=&quot;#Evaluation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Since the models can not all be deployed in a real car, an evaluation strategy has to be adopted. In IL, open-loop evaluation can only be used while closed-loop is used for RL based techniques (easier to perform in simulation). 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/loop.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;An example for error functions used :&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/erorrs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The authors propose the following for the evaluation of different algorithms :&lt;/p&gt;
&lt;p&gt;&quot;For allowing to compare different end-to-end models, future&lt;br /&gt;
models should perform closed-loop evaluation:&lt;br /&gt;
• In diverse locations. Locations not used during training, if possible.&lt;br /&gt;
• In diverse weather and light conditions. Conditions not used during training, if possible.&lt;br /&gt;
• In diverse traffic situations, with low, regular or dense traffic.&lt;br /&gt;
If training the model in CARLA simulator, one should reportthe performance in CARLA and NoCrash benchmarks.&quot;&lt;/p&gt;
&lt;h2 id=&quot;Interpretability&quot;&gt;Interpretability&lt;a class=&quot;anchor-link&quot; href=&quot;#Interpretability&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In order to debug the model and interpret it's failures, interpretatbility is an important goal of the model. Methods for this are visual saliency (shown below),  intermediate representatios and auxiliary outputs. Auxiliary outputs can be used to optimize the intermediate representations, which in turn feeds back into the main and task and improves it's workings.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/saliency.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;Example-Architecture-:&quot;&gt;Example Architecture :&lt;a class=&quot;anchor-link&quot; href=&quot;#Example-Architecture-:&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Based on the most promising end-to-end approaches today, the authors propose this following, theoretic architecutre which has not been implemented yet. The key in an actual implementation would be the fusion of the different neural network outputs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/example_end_to_end.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;Challenges-and-Conclusion&quot;&gt;Challenges and Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Challenges-and-Conclusion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In order to drive safely in the real world, the long tail of edge case has to be handled as by the model. It will be important to find these rare scenarios and improve upon them. Data sampling techniques which emphasize on high loss examples (&quot;When you sort your dataset descending by loss you are guaranteed to find something unexpected, strange and helpful.&quot; - Andrej Karpathy) will be the key for data engineering , by &quot;massaging&quot; the data rare data points can be found and handled correctly. While simulation will play an increasingly more important role due it's advantages for behavior reflex development, success in simulation does not guarantee success in real-world testing.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;$[1]$ Felipe Codevilla, Eder Santana, Antonio M López, and Adrien Gaidon. 
Exploring the limitations of behavior cloning for autonomous driving.&lt;br /&gt;
arXiv preprint arXiv:1904.08980, 2019&lt;br /&gt;
$[2]$ Felipe Codevilla, Matthias Miiller, Antonio López, Vladlen Koltun,&lt;br /&gt;
and Alexey Dosovitskiy. End-to-end driving via conditional imitation&lt;br /&gt;
learning. In 2018 IEEE International Conference on Robotics and&lt;br /&gt;
Automation (ICRA), pages 1–9. IEEE, 2018.&lt;br /&gt;
$[3]$ Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauffeurnet:&lt;br /&gt;
Learning to drive by imitating the best and synthesizing the worst.&lt;br /&gt;
arXiv preprint arXiv:1812.03079, 2018.&lt;br /&gt;
$[4]$ Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of&lt;br /&gt;
imitation learning and structured prediction to no-regret online learning.&lt;br /&gt;
In Proceedings of the fourteenth international conference on artificial&lt;br /&gt;
intelligence and statistics, pages 627–635, 2011.&lt;br /&gt;
$[5]$ Sauhaarda Chowdhuri, Tushar Pankaj, and Karl Zipser. Multinet: Multi-&lt;br /&gt;
modal multi-task learning for autonomous driving. In 2019 IEEE&lt;br /&gt;
Winter Conference on Applications of Computer Vision (WACV), pages 
1496–1504. IEEE, 2019.&lt;br /&gt;
$[6]$ Matthias Müller, Alexey Dosovitskiy, Bernard Ghanem, and Vladlen&lt;br /&gt;
Koltun. Driving policy transfer via modularity and abstraction. arXiv&lt;br /&gt;
preprint arXiv:1804.09364, 2018.&lt;br /&gt;
$[7]$ Xinlei Pan, Yurong You, Ziyan Wang, and Cewu Lu. Virtual to&lt;br /&gt;
real reinforcement learning for autonomous driving. arXiv preprint&lt;br /&gt;
arXiv:1704.03952, 2017.&lt;br /&gt;
$[8]$ Alex Bewley, Jessica Rigley, Yuxuan Liu, Jeffrey Hawke, Richard&lt;br /&gt;
Shen, Vinh-Dieu Lam, and Alex Kendall. Learning to drive from&lt;br /&gt;
simulation without real world labels. In 2019 International Conference&lt;br /&gt;
on Robotics and Automation (ICRA), pages 4818–4824. IEEE, 2019.&lt;br /&gt;
$[9]$ Brady Zhou, Philipp Krähenbühl, and Vladlen Koltun. Does computer&lt;br /&gt;
vision matter for action? arXiv preprint arXiv:1905.12887, 2019.&lt;br /&gt;
$[10]$ Felipe Codevilla, Eder Santana, Antonio M López, and Adrien Gaidon.&lt;br /&gt;
Exploring the limitations of behavior cloning for autonomous driving.&lt;br /&gt;
arXiv preprint arXiv:1904.08980, 2019&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/END_TO_END.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/END_TO_END.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Stand-Alone Self-Attention in Vision Models Summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html" rel="alternate" type="text/html" title="Stand-Alone Self-Attention in Vision Models Summary" /><published>2020-08-06T00:00:00-05:00</published><updated>2020-08-06T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/08/06/SATCONV.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-08-06-SATCONV.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.05909.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Combat the deficits of Conolutional Architectures, (local connectivity, failing to reason globally) by introducing Attention to as a stand-alone layer. The authors prove that this can be both more accurate and more efficient at the same time. Architectures that are attention only, and a mixed version of convolutional and attention architectures are introduced and compared to the vanilla convolutional implementations.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Methods&quot;&gt;Methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Methods&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Conv-Block-deficits&quot;&gt;Conv Block deficits&lt;a class=&quot;anchor-link&quot; href=&quot;#Conv-Block-deficits&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Capturing long range interactions is challenging for convolutions as they do not scale well with large receptives. Since attention has been used to tackle long range dependencies in sequence modeling, since architectures like SE Nets model attention on a chanel wise basis successfully. However in these cases attention was only an add-on to a traditional architecture style. In this paper the authors propose to use attention mechanisms as stand alone layers.&lt;/p&gt;
&lt;h3 id=&quot;Self-Attention&quot;&gt;Self-Attention&lt;a class=&quot;anchor-link&quot; href=&quot;#Self-Attention&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Attention was originally introduced in order to allow for summarization from variable length source sentences. Attention focuses on the important parts of the input and thereby can serve as a primary representation learning mechanism and fully replace recurrence. The word self means that it just considers a single context (query,keys and values are extracted from the same image). The breakthrough in this paper is the use of self-attention layers instead of convolutional layers.&lt;/p&gt;
&lt;p&gt;In this work, already existing mechanisms are used which are not optimized for the image domain. Therefore it is permutation equivariant and has limited expression capability for vision tasks.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/sat_conv.png&quot; alt=&quot;&quot; /&gt; 
&lt;br /&gt;&lt;br /&gt;
The process is as follows : 
1) given a pixel $x_{i,j} \in R^{d_{in}}$ in positions $ab \in N_{k}(i,j)$ a local kernel with kernel size $k$ is extracted. $x_{i,j}$ is the middle of the kernel, which is called &lt;em&gt;memory block&lt;/em&gt;. Prior work only performed global attention, which can only be done with a downsized sample as it is very compute expensive. 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;2) Single headed attention is computed : 
&lt;br /&gt;&lt;br /&gt;
$y_{i,j} = \sum_{ab \in N_{k}(i,j)}  softmax_{a,b}(q_{ij}^{T}k_{ab})v_{ab}$
&lt;br /&gt;&lt;br /&gt;
where the ${queries}$ $q_{ij} = W_Q x_{ij}$&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;
${keys}$ $k_{ab} = W_K x_{ab}$
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;and ${values}$ $v_{ab} = W_V x_{ab}$ 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;are linear transformations of the pixel in position and it's neighbors in the kernel. 
&lt;br /&gt;&lt;br /&gt;
$\texttt{softmax}_{a b}$ is a softmax, which is applied to all logits computed in the neighborhood of $ij$. 
$W_Q, W_K, W_V \in \mathbb{R}^{d_{out} \times d_{in}}$ are learned transforms. 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Local self-attention is similar to convolution in the way that it aggregates spatial information in the neighborhoods, multi attention heads are used to learn unique representations of the input. This is done by partitioning pixel features into $N$ groups and then computing single-headed attention on each one seperately with the transforms $W_Q, W_K, W_V \in \mathbb{R}^{d_{out} \times d_{in}}$. The outputs of the heads are then concatenated. 
&lt;br /&gt;&lt;br /&gt;
2D relative pose embeddings,relative attention is used : 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;1) relative attention computes relative distances of the pixel to each one in the neighborhood : row $(a-i)$ and column offset $(b-j)$&lt;br /&gt;
2) row and column offset are associated with an embedding and concatenated into a vector $r_{a-i,b-j}$&lt;/p&gt;
&lt;p&gt;3) Spatial-relative attention is then defined as : 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;$y_{ij} = \sum_{a,b\in N_{k}(i, j)} softmax_{ab}(q_{ij}^{T}k_{ab}+q_{ij}^{T}r_{a-i,b-j})v_{ab}$&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
The logit measuring similarity between the query and an element results from the content of the element and the relative distance of the element from the query. By inlcuding this spatial information, self-attention also has translation equivariance, just like conv layers. Unlike conv layers, self-attentions parameter count is independent of its spatial extent. 
The compute cost also grows slower : 
For example, if $d_{in} = d_{out} = 128$, a convolution layer with $k = 3$ has the same computational cost as an attention layer with $k = 19$.&lt;/p&gt;
&lt;p&gt;Using this as their basis, a fully attentional architecture is created in two steps:&lt;/p&gt;
&lt;h4 id=&quot;Replacing-Spatial-Convolutions&quot;&gt;Replacing Spatial Convolutions&lt;a class=&quot;anchor-link&quot; href=&quot;#Replacing-Spatial-Convolutions&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;A spatial conv is defined to have spatial extent k &amp;gt; 1, which also includes 1x1 convolutions. These can be viewed as fully connected layers. 
Here the authors want to replace conv blocks in a straightforward way, specificially focusing on ResNet. Therefore the 3x3 convolution in Path B is swapped with a self-attention layer as defined above. All the other blocks are not changed, this might be supobtimal but promises potential improvements using architecture search.&lt;/p&gt;
&lt;h4 id=&quot;Replacing-the-Convolutional-Stem-(intial-layers-of-the-CNN)&quot;&gt;Replacing the Convolutional Stem (intial layers of the CNN)&lt;a class=&quot;anchor-link&quot; href=&quot;#Replacing-the-Convolutional-Stem-(intial-layers-of-the-CNN)&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;This part focuses on replacing the inital layers, as they are the most compute expensive due to the large input size of the image. In the OG ResNet the input is a 7x7 kernel with stride 2, followed by 3x3 max pooling with stride 2. At the beginning RGB pixels are individually uninformative and information is heavily spatially correlated through low level features such as edges. Edge detectors are difficult to learn for self-attention due to spatial correlation, convs learn these easily through distance based weight parameterization. The authors inject spatially-varying linear transforms into the pointwise 1x1 softmax convolution.&lt;/p&gt;
&lt;p&gt;$\tilde{v}_{a b} = \left(\sum_m  p(a, b, m) W_V^{m}\right) x_{a b}$&lt;/p&gt;
&lt;p&gt;The results is similar to convolutions, weights are learned based on a local neighborhood basis. So in total the stem consists of spatially aware value features, followed by max-pooling. A more detailed explanation of this can be found in the appendix of the paper (page 14/15).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Results&quot;&gt;Results&lt;a class=&quot;anchor-link&quot; href=&quot;#Results&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Implementation details for both classification and object detection are in the appendix.&lt;/p&gt;
&lt;h3 id=&quot;ImageNet&quot;&gt;ImageNet&lt;a class=&quot;anchor-link&quot; href=&quot;#ImageNet&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The multi head self-attention layer uses a spatial width of k=7 and 8 attention heads. The position-aware attention stem as described above is used.The stem performs self-attention within each 4×4 block of the original image, followed by batch normalization and a 4×4 max pool operation. Results are below : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/sat_imagenet.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Coco-Object-Detection&quot;&gt;Coco Object Detection&lt;a class=&quot;anchor-link&quot; href=&quot;#Coco-Object-Detection&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Here Retina Net is used with a classification backbone, followed by an FPN, the network has 2 detection heads. Results are in the table below : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/sat_coco.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that using attention based backbone we can achieve results on par with a conv backbone, but with 22% less parameters. This can be extended by additionaly making the FPN and the detection heads attention-based and thereby reducing paraneter count by 34% and more importantly FLOPS by 39%.&lt;/p&gt;
&lt;h3 id=&quot;Where-is-stand-alone-attention-most-useful-?&quot;&gt;Where is stand-alone attention most useful ?&lt;a class=&quot;anchor-link&quot; href=&quot;#Where-is-stand-alone-attention-most-useful-?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/sat_usful.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Results are schown in the tables above.&lt;/p&gt;
&lt;h4 id=&quot;Stem&quot;&gt;Stem&lt;a class=&quot;anchor-link&quot; href=&quot;#Stem&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The basic results for the stem is that, convolutions perform very well here, as described above self-attention can not easily learn edges due to the high spatial correlation which is captured very well by conv layers though.&lt;/p&gt;
&lt;h4 id=&quot;Full-Net&quot;&gt;Full Net&lt;a class=&quot;anchor-link&quot; href=&quot;#Full-Net&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The authors basically state what has been described above, conv layers capture low level features very well, while attention is able to model global relations effectively. Therefore an optimal architecture should contain both attention and convolutional layers.&lt;/p&gt;
&lt;h3 id=&quot;Which-attention-features-are-important-?&quot;&gt;Which attention features are important ?&lt;a class=&quot;anchor-link&quot; href=&quot;#Which-attention-features-are-important-?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Effect of spatial extent of self-attention (Table 4) : 
The value of spatial extent k should generally be larger (for example k=11), the exact optimal setting depends on hyperaparameter choices.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Importance of positional information (Table 5 + 6)
3 types of encoding were used : no positional encoding, sinusodial encoing and absolute pixel position. Relativ encoding performs 2% better than absolute one. Removing content-content interaction only descreases accuracy by 0.5%. Therefore the positional encoding seems to be very important and can be a strong focus of their future research.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Importance of spatially-aware attention stem (Table 7) 
Using stand-alone attention in the stem with spatially-aware values, it outperforms vanilla stand-alone attention by 1.4%, while having similar FLOPS. Using a spatial convolution to the values instead of spatially-aware point-wise transformations (see above), leads to more FLOPS and slightly worse results. A future goal of the authors is to unify attention used across the stem and main&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/SATCONV.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/SATCONV.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ADABOUND Summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/31/ADABOUND.html" rel="alternate" type="text/html" title="ADABOUND Summary" /><published>2020-07-31T00:00:00-05:00</published><updated>2020-07-31T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/31/ADABOUND</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/31/ADABOUND.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-31-ADABOUND.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.09843&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In recent years, a lot of different first order methods (ADAGRAD, RMSProp, ADAM, AMSGRAD) have been introduced for training deep neural nets in order to combat the uniform gradient scaling of SGD and it's slower training speed. Experiments and the release of state of the art CV and NLP models - which use traditional SGD (often with momentum) -  prove that these optimizers generalize poorly in comparison to classic SGD. Sometimes they don't even converge, which is largely because of unstable and extreme learning rates.&lt;/p&gt;
&lt;p&gt;The authors therefore provide new, dynamically bounded versions of AMSGRAD and ADAM, called AMSBOUND and ADABOUND. These new techniques can be seen as adaptive methods during early training, which smoothly transform into SGD/SGD with momentum as training time increases. Their method is inspired by gradient clipping.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Methods&quot;&gt;Methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Methods&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Analysis-of-already-existing-adaptive-methods&quot;&gt;Analysis of already existing adaptive methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Analysis-of-already-existing-adaptive-methods&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/frame_adabound.png&quot; alt=&quot;image&quot; /&gt; 
The figure above shows the generic training loop that was used by the authors, where :&lt;/p&gt;
&lt;p&gt;$g_{t}$ represents the gradient at step $t$&lt;br /&gt;
$\alpha_{t}$ represents the learning rate at step $t$, which is gradually being                           reduced for theoretical proof of convergence&lt;br /&gt;
$m_{t}$ represents the momentum term at step $t$, a function of the gradients&lt;br /&gt;
$V_{t}$ represents the scaling term at step $t$, a function of the gradients&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/adabound_algos.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The table above shows different optimizers w.r.t to the generic training loop. $\phi_{t}$ and $\psi_{t}$ are the respective functions used for computing $m_{t}$ and $V_{t}$ respectively. 
&lt;br /&gt;&lt;br /&gt;
The authors mainly focus on ADAM in their research, as it is very general so it also applies to other adaptive methods (RMSProp, ADAGRAD). Note that ADAM is a combination of RMSprop and momentum, with the distinction that momentum is incorporated directly in the first-order moment estimate (with the use of exponential weighting) and also the additional use of weight decay.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;
At the time of writing, AMSGRAD was a recently released adaptive method, which was supposed to perform as well as SGD. However AMSGRAD does not yield evident improvement when compared to ADAM. AMSGRAD uses a smaller learning rate than ADAM and the AMSGRAD researchers consider that large learning rates are probLematic. Due to the smaller learning rate being used in AMSGRAD, the authors suspect that small learning rates might be a problem for ADAM as well.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;To prove their theory, the authors sample learning rates of several weights and biases for ResNet-34 on CIFAR-10 with ADAM. They randomly select 9 3x3 conv kernels from different layers and biases from the last fully connected layer. Because parameters of the same layer very similar, 9 weights sampled from 9 kernels and one bias are considered. These weights and the bias are visualized in a heatmap. They find that tiny learning rates of smaller than 0.01 and larger ones greater than 1000 lead to the model being closer to convergence. Therefore it is empirically proven, that the learning rate can be both too large and too small in extreme cases. AMSGRAD combats the impact of large learning rates, but it does neglect the other side of the spectrum.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;In a second experiment, the authors consider a set of linear functions with different gradients. In their example (see Page 4, upper part of the page), once every $C$ steps a gradient of -1 (moves into the wrong direction) occurs and at the next step a gradient of 2 occurs. Even though 2 is larger than 1 in terms of the abolute value, the larger gradient can not counteract the -1 gradient as the learning rate is scaled down in this later time step leading to x diverging.
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;3 theorems formalize the intuition : 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Theorem-1&quot;&gt;Theorem 1&lt;a class=&quot;anchor-link&quot; href=&quot;#Theorem-1&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;There is an online convex optimization problem where for any initial step size α, ADAM
has non-zero average regret. (ADAM does not converge) 
This problem does not occur with SGD, as for a large amount of possible choices for α, the average regret for SGD converges to 0. This is mostly a problem at the end of training, where a lot of gradients are close to zero and the average of 2n order momentum is very various beacuse of the exponential moving average. In this case correct signals occur rarely, which may lead to the algorithm not being able to converge (refer to the example with the gradients of -1 and 2 earlier). Their results show that for any case where $\beta_{1} &amp;lt; \sqrt{\beta_{2}}$ , ADAM does not converge regardless of the step size. Note the standard values are usually $\beta_{1}=0.9$ and $\beta_{2}=0.999$, which proves that $\beta_{1} &amp;lt; \sqrt{\beta_{2}}$, because $0.9 &amp;lt; 0.9995$.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Theorem-2&quot;&gt;Theorem 2&lt;a class=&quot;anchor-link&quot; href=&quot;#Theorem-2&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
Refers to the earlier point, that there is an online convex optimzation setting, where ADAM does not converge when $\beta_{1} &amp;lt; \sqrt{\beta_{2}}$ for  $\beta_{1}$,$\beta_{2}$ $\in [0,1)$ 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Theorem-3&quot;&gt;Theorem 3&lt;a class=&quot;anchor-link&quot; href=&quot;#Theorem-3&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
Just like theorem 2, but in a stochastic convex optimization setting  : There is a stochatic convex optimzation setting, where ADAM does not converge when $\beta_{1} &amp;lt; \sqrt{\beta_{2}}$ for  $\beta_{1}$,$\beta_{2}$ $\in [0,1)$ 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;By formulating these 3 theorems, it is proven that the impact of extreme learning rates will lead to a lacking generalization ability and will not be able to solve the problem.&lt;/p&gt;
&lt;h3 id=&quot;Adabound&quot;&gt;Adabound&lt;a class=&quot;anchor-link&quot; href=&quot;#Adabound&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;As mentioned in the beginning of the paper, the goal should be to achieve SGD performance with ADAM training speed. 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/ADABOUND_algo.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;The above algorithm is essentially ADAM with clipping in order to constrain the learning rate to be in $[\eta_{l},\eta_{u}]$. For ADAM $\eta_{l} = 0$ and $\eta_{u} = \infty$ and for SGD(optionally with Momentum) with a learning rate $\alpha$, $\eta_{l} = \eta_{u} = \alpha$. The bound are functions of step $t$, with the lower bound $\eta_{l}(t)$ is a non-decreasing function that starts from zero and converges to SGD's $\alpha$  and the upper bound $\eta_{u}(t)$ being a function that starts from $\infty$ and converges to $\alpha$ as well. 
The functions are : 
$\eta_{l}(t) = 0.1 - \frac{0.1}{(1-\beta_{2})t + 1}$&lt;br /&gt;
$\eta_{u}(t) = 0.1 + \frac{0.1}{(1-\beta_{2})t}$&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;We therefore reach the SGD state in the end as bound become increasingly restricted with higher step size $t$. The authors empasize that this method is better than using a mixed ADAM and SGD approach with a &quot;hard switch&quot; (Kesker &amp;amp; Socher 2017) which also introduces a hard to tune hyperparameter. The result is proven for Adabound in theorem 4, which also shows that ADABOUND is upper bounded by $O(\sqrt{T})$ : 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/theorem4.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;
The authors provide extensive and very understandable proofs in their appendix.&lt;/p&gt;
&lt;h3 id=&quot;Implementation&quot;&gt;Implementation&lt;a class=&quot;anchor-link&quot; href=&quot;#Implementation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
An implementation of ADABOUND and ADABOUNDW by the authors can be found &lt;a href=&quot;https://github.com/Luolc/AdaBound/blob/master/adabound/adabound.py&quot;&gt;here&lt;/a&gt;. We can see that the implementation is very similar to the standard ADAM optimizer in the Pytorch source code. The difference is the if/else condition in the end which keeps track of the maximum 2nd moment running average, as well as the bounding at the end of the &lt;code&gt;step&lt;/code&gt; function. ADABOUNDW also adds weight decay at the end.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Results&quot;&gt;Results&lt;a class=&quot;anchor-link&quot; href=&quot;#Results&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Ada_exp.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;The authors compare ADABOUND and AMSBOUND to ADAM, AMSGRAD, ADAGRAD and SGD on the tasks above with the respective architectures. Hyperparameters are tuned for each optimizer seperately using a logarithmic grid which can be extrapolated if an extreme value is determined to perform the best.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/ada_CNN.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;The above diagrams show the performance of ADABOUND/AMSBOUND when compared to traditional methods. With DenseNet-121 on CIFAR-10 ADABOUND is about 2% better than it's base methods during test. With ResNet, they even outperform SGD (with momentum) @ test time by 1%. Much more improvment in deep conv nets is oberved when compared to perceptrons.&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/ada_RNN.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;For RNNs ADAM does have the best initial progress but stagnates very quickly, with AMSBOUND/ADABOUND being smoother than SGD (with momentum). Compared to ADAM it outperforms it by 1.1% in the 1 layer LSTM case and 2.8% in the 3 Layer case. This further proves that increasingly more complex architectures benefit even more from AMSBOUND/ADABOUND. This is due to extreme learning rates being more likely to occur with complex models.&lt;/p&gt;
&lt;p&gt;The authors summarize that while AMSBOUND/ADABOUND perform well with complex architectures, there is still potential for shallower architectures. It is also unclear why SGD performs so well in most cases. Other ways for improvment should also be considered, the authors propose weight decay as a potential often (AMSBOUNDW).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/adabound.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/adabound.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Mish Paper Summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/24/Mish.html" rel="alternate" type="text/html" title="Mish Paper Summary" /><published>2020-07-24T00:00:00-05:00</published><updated>2020-07-24T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/24/Mish</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/24/Mish.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-24-Mish.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1908.08681.pdf&quot;&gt;Paper Link&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Propose a new activation function which replaces upon the known standards such as ReLU and Swish. The function proposed is called Mish activation and is defined by : $f(x) = x * tanh(softplus(x))$
Recall that Sofplus is defined as $f(x) = ln(1+e^{x})$
The authors show that it can be more effective than ReLU and Swish for Computer Vision tasks. 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Mish.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Methods&quot;&gt;Methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Methods&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Mish-Activation&quot;&gt;Mish Activation&lt;a class=&quot;anchor-link&quot; href=&quot;#Mish-Activation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;As explained in the intro, Mish is a novel activation function. It is inspired by ReLU and Swish and has a bounded bottom value of $~ -0.31$&lt;br /&gt;
The derivative is defined as  :
$f^{'}(x) = \frac{e^{x} * w}{\delta^{2}}$&lt;br /&gt;
With $w=4*(x+1) + 4e^{2x} + e^{3x}  +e^{x} * (4x+6)$&lt;br /&gt;
and $\delta = 2e^{2x} + e^{2x} + 2$&lt;br /&gt;
It also has a self gating property, which means that it simply takes a scalar as input and allows it to easily replace ReLU in existing networks. A plot including Mish and Swish derivatives is shown below : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Mish_derivative.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Code&quot;&gt;Code&lt;a class=&quot;anchor-link&quot; href=&quot;#Code&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We can implement Mish in Pytorch the following way :&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MishImplementation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;nd&quot;&gt;@staticmethod&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_for_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softplus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# x * tanh(ln(1 + exp(x)))&lt;/span&gt;
   &lt;span class=&quot;nd&quot;&gt;@staticmethod&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saved_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sofplus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Credits go to the author of the paper and the implementation above that is used in YOLOv3 by Ultralytics.&lt;/p&gt;
&lt;h3 id=&quot;Explanation&quot;&gt;Explanation&lt;a class=&quot;anchor-link&quot; href=&quot;#Explanation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Mish_prop.png&quot; alt=&quot;image&quot; /&gt; 
The authors explain why Mish does improve upon current results in this section and emphasize the advantageous properties of Mish.&lt;/p&gt;
&lt;p&gt;Like Relu and Swish, Mish is unbounded above, which prevents saturation and therefore vanishing gradients. The about -0.31 bound below adds strong regularization  properties. Not killing gradients when x is below 0 improves gradient flow and therefore improves expressivity. Famously ReLU is not differentiable at 0, the smoothness  of Mish makes it continuously differentiable. The smoother function allow for smoother loss functions and therefore better optimization. The authors summarize these properties and the table above.&lt;/p&gt;
&lt;p&gt;The authors generally recommend to use a higher amount of epochs with Mish activation. This obviously introduces some overhead during training.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Results&quot;&gt;Results&lt;a class=&quot;anchor-link&quot; href=&quot;#Results&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Hyperparameter-study&quot;&gt;Hyperparameter study&lt;a class=&quot;anchor-link&quot; href=&quot;#Hyperparameter-study&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The author studies the difference between ReLU, Swish and Mish in Section 3.1 by considering fully connected nets with different layer amounts batch norm, dropout and no residual connections, plots are shown for every category. The most important takeaway is that Mish is better than current SOTA for optimizing larger and wider networks. It has to be criticized that the author is not using residual connections here, as this might increase the advantage of Mish even more than it would in a real setting with a skip connection network.
They also show that larger batch sizes benefit from Mish, it is also more stable for different initalizations and slightly more robust to noise.&lt;/p&gt;
&lt;p&gt;The results are replicated in experiments with a 6-layer CNN. Here Mish outperforms Swish with 75.6% to 71.7% on CIFAR-100. Swish did not seem to learn for the first 10 epochs here due to dead gradients.&lt;/p&gt;
&lt;p&gt;The author also shows that Mish outperforms Swish when using Cosine Annealing and outperforms Swish by about 9% when using Mixup with $\alpha=0.2$ to compare the two methods.&lt;br /&gt;
Statistical comparison shows that Mish has highest mean test accuracy and lowest mean standard deviation when compared to ReLU and Swish.&lt;/p&gt;
&lt;p&gt;It is also mentioned that Mish is slightly less efficient on GPU than the other two mentioned activation functions.&lt;/p&gt;
&lt;h3 id=&quot;Different-Architectures-with-Mish&quot;&gt;Different Architectures with Mish&lt;a class=&quot;anchor-link&quot; href=&quot;#Different-Architectures-with-Mish&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The author compares Mish by training with different networks and soley replacing ReLU/Swish with Mish, while leaving the hyperparameters unchanged. The superior performance of Mish during CIFAR100 can be seen in the table below : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Mish_results.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/Mish.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/Mish.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Efficient Det paper summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/15/EfficientDet.html" rel="alternate" type="text/html" title="Efficient Det paper summary" /><published>2020-07-15T00:00:00-05:00</published><updated>2020-07-15T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/15/EfficientDet</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/15/EfficientDet.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-15-EfficientDet.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Recently, the SOTA in high accuracy in the fields of object detection and semantic segmentation were mostly achieved by scaling up architectures (e.g. AmoebaNet and NAS-FPN). 
These models are not easily deployable, especially in runtime/compute constrained applications such as autonomous driving. While existing worked has achieved faster rutime through one-stage/anchor-free detectors or model compression, they usually sacrifice accuracy for runtime. The goal is therefore to create an architecture that combines the best of both worlds, and achieve both high accuracy and better efficiency. The authors consider a wide range of compute that someone might have at hand during inference (3B to 300B FLOPS).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Methods&quot;&gt;Methods&lt;a class=&quot;anchor-link&quot; href=&quot;#Methods&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Challenges&quot;&gt;Challenges&lt;a class=&quot;anchor-link&quot; href=&quot;#Challenges&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The authors define 2 main challenges :&lt;/p&gt;
&lt;p&gt;1) efficient multi-scale feature fusion&lt;/p&gt;
&lt;p&gt;The authors do consider recent developments like PANet and NAS-FPN (both improvments of the original FPN approach). Most of these works only sum up the features without weighting them, even though the resolutions are different. That's why the authors of the paper propose a weighted bi-directional feature pyramid network (BiFPN), it introduces weights that can learn the importance of a different input features. It does this while also applying top-down and bottom-up bath augmentation as proposed in the PANet paper.&lt;/p&gt;
&lt;p&gt;2) model scaling&lt;/p&gt;
&lt;p&gt;In the past, the main method to improve performance, was using larger and therefore more powerful backbones. In this paper the authors use NAS to jointly scale resolution of the input, depth and width of the net as well as sclaing the feature network and box/class prediction network. It thus follows the ideas of EfficientNet, which also turns out to be their backbone choice. The architectures that are proposed is therefore a combo of EfficientNet, BiPFN and compound scaling. These models are called EfficientDet, in honor of their Backbone.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/FPNs.png&quot; alt=&quot;image&quot; /&gt; 
$Figure$ $1$&lt;/p&gt;
&lt;h3 id=&quot;BiFPN&quot;&gt;BiFPN&lt;a class=&quot;anchor-link&quot; href=&quot;#BiFPN&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;h4 id=&quot;Problem&quot;&gt;Problem&lt;a class=&quot;anchor-link&quot; href=&quot;#Problem&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;BiFPN aims to aggregate features at different resolutions, as FPN-ish methods downscale the feature level with a resolution of $1/2^{i}$ ($i$ being the layer number) , up-/downsampling is used in order to match features of different resolution.&lt;/p&gt;
&lt;h4 id=&quot;Cross-Scale-Connections&quot;&gt;Cross-Scale Connections&lt;a class=&quot;anchor-link&quot; href=&quot;#Cross-Scale-Connections&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In their research the authors compare PANet (introduces bottom-up aggregation) with FPN and NAS-FPN and find that PANet achieves the best accuracy at the cost of some compute overhead. The authors improve upon these cross-scale connections by removing and thereby simplifying nodes with only one input edge, as these have less contribution to the feature net. Secondly they connect an extra edge from input to output node if they are at the same level (fusion with low compute cost), also see $Figure$ $1$. Furthermore, each bidirectional layer (top-down and bottom-up bath) is repeated multiple times to enable higher level fusion. For concrete implementation details (# of layers), refer to section 4.2 in the paper.&lt;/p&gt;
&lt;h4 id=&quot;Weighted-Feature-Fusion&quot;&gt;Weighted Feature Fusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Weighted-Feature-Fusion&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;As explained earlier, the different resolutions that the feature maps have should be considered and therefore weithed during aggregation. In this work the authors use Pyramid Attention Network, which uses global self-attention upsampling to recover the location of the pixels. In order to weigh each input seperately, an additional learnable weight is added for each input. The authors consider three different fusion approaches :&lt;/p&gt;
&lt;p&gt;1) Unbounded Fusion : $0 = \sum\limits_{i} w_{i} * I_{i}$
=&amp;gt; could lead to instability, so weight norm is applied here&lt;/p&gt;
&lt;p&gt;2) Softmax-based function : $O = \sum\limits_{i} \dfrac{e^{w_{i}}}{\sum\limits_{j} e^{w_{j}}} * I_{i}$&lt;br /&gt;
=&amp;gt; here the idea is to normalize the probablities between 0 and 1, weighting the importance of each input that way. However the softmax introduces extra slowdown on the GPU hardware, so 3) is proposed :&lt;/p&gt;
&lt;p&gt;3) Fast normalized fusion : $O = \sum\limits_{i} \dfrac{w_{i}}{\sum\limits_{j} {w_{j}} + \epsilon} * I_{i}$&lt;/p&gt;
&lt;p&gt;This function has the same learning charaterstics as 2), but it runs about 30% faster on GPU  Depthwise sep. convs are used for better runtime.&lt;/p&gt;
&lt;h3 id=&quot;Architecture&quot;&gt;Architecture&lt;a class=&quot;anchor-link&quot; href=&quot;#Architecture&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/eff_arch.png&quot; alt=&quot;image&quot; /&gt;
$Figure$ $2$&lt;/p&gt;
&lt;p&gt;To create the final architecture a new compound scaling method is introduced which scales Backbone,BiFPN, class/box net and resolution jointly. A heuristics used as object detectors have even more possible configs as classification nets.&lt;/p&gt;
&lt;h4 id=&quot;Backbone&quot;&gt;Backbone&lt;a class=&quot;anchor-link&quot; href=&quot;#Backbone&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;In order to use ImageNet pretraining, the checkpoints of EfficientNet-B0 to B6 are used.&lt;/p&gt;
&lt;h4 id=&quot;BiFPN-Net&quot;&gt;BiFPN Net&lt;a class=&quot;anchor-link&quot; href=&quot;#BiFPN-Net&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The BiFPN depth is linearly increased. The width is scaled exponentially, a grid search is done and 1.35 is used as a base :&lt;/p&gt;
&lt;p&gt;$W_{BiFPN} = 64 * (1.35^{\theta})$&lt;br /&gt;
$D_{BiFPN} = 3 + \theta$&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/efficient_nas.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Box/class-prediction-network&quot;&gt;Box/class prediction network&lt;a class=&quot;anchor-link&quot; href=&quot;#Box/class-prediction-network&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Width is fixed to be same as BiFPN, but depth is increased differently : 
$D_{box} = D_{class} = 3 + [\theta/3]$&lt;/p&gt;
&lt;h4 id=&quot;Image-Resolution&quot;&gt;Image Resolution&lt;a class=&quot;anchor-link&quot; href=&quot;#Image-Resolution&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Image res. is increased using equation : $R_{input} = 512 + \theta * 128$&lt;br /&gt;
128 is used as the features are used in level 3-7 and $2^{7} = 128$&lt;/p&gt;
&lt;p&gt;Heuristics based scaling might not be optimal and could be improved.&lt;/p&gt;
&lt;p&gt;The scaling results can be seen below : 
&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/eff_nas.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The advantage of compound scaling is shown in $Figure$ $2$.&lt;/p&gt;
&lt;h4 id=&quot;Some-Implementation-Details&quot;&gt;Some Implementation Details&lt;a class=&quot;anchor-link&quot; href=&quot;#Some-Implementation-Details&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;SGD with momentum 0.9 and weight decay 4e-5   &lt;/li&gt;
&lt;li&gt;Synch BN w/BN decay of 0.99 end epsilon 1e-3   &lt;/li&gt;
&lt;li&gt;swish activation with weight decay of 0.9998 &lt;/li&gt;
&lt;li&gt;focal-loss with α = 0.25 and γ = 1.5, and aspect ratio {1/2, 1, 2}&lt;/li&gt;
&lt;li&gt;RetinaNnet preprocessing with training-time flip-
ping and scaling&lt;/li&gt;
&lt;li&gt;soft NMS is used for D7, standard NMS for the others&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Results&quot;&gt;Results&lt;a class=&quot;anchor-link&quot; href=&quot;#Results&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/eff_det.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;As the graph above shows, EfficientDet-D0 performs about on par with YOLOv3. It does use 28x fewer flops, which does not directly show in the runtime (also due to the optimized TensorRT implementation that YOLOv3 uses). Overall it is about 4-9x smaller and 13-42x less FLOP hungry than other detectors. In total they are about 4.1x faster on GPU and even 10x faster on CPU. This is probably due to the fact that the CPU can not hide the extra compute requierements in FLOPS as efficiently as the latency hiding GPU architecture.&lt;/p&gt;
&lt;p&gt;The authors compete on a Segmentation task and outperform DeepLabV3+ by 1.7% on COCO Segmentation with 9.8x fewer FLOPS.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/eff_det.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/eff_det.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">SE Net Summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/03/SENET.html" rel="alternate" type="text/html" title="SE Net Summary" /><published>2020-07-03T00:00:00-05:00</published><updated>2020-07-03T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/03/SENET</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/03/SENET.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-07-03-SENET.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;SENET-Paper-Summary&quot;&gt;SENET Paper Summary&lt;a class=&quot;anchor-link&quot; href=&quot;#SENET-Paper-Summary&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;(Jie HU, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu)&lt;/p&gt;
&lt;h3 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;So far most research was focused on the feature hierarchy (global receptive fields are obtained through multi-scale operations, interleaving layers, ...) the authors here want to focus on channel relationship instead. They propose a new blok type called the &quot;Squeeze and excitation block&quot;, which models dependencies between channels. They achieve new SOTA results on ImageNet and won the competition in 2017.&lt;/p&gt;
&lt;h3 id=&quot;Methods-Proposed&quot;&gt;Methods Proposed&lt;a class=&quot;anchor-link&quot; href=&quot;#Methods-Proposed&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Squeeze-and-Excitation Block :&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/se_block.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;p&gt;feature maps $U$ are first passed through a $squeeze$ operation, this results in a descriptor. It stacks the maps by it's spatial Dimensions (HxW), so the total size is (HxWxnumber of channels), essentially it produces an Embedding.&lt;/p&gt;
&lt;p&gt;After that an $excitation$ operation is applied, it is a self gating mechanism. It uses the emedding as an input and outputs per channel weights. This operation is then applied to the feature maps $U$. SE Blocks can be stacked like Dense blocks, Residual blocks, etc.&lt;/p&gt;
&lt;p&gt;In the early layers, SE blocks are class agnostic, while they are very class specific in the later ones. Therfore the advantage feature recalibration can be accumulated in the net.&lt;/p&gt;
&lt;h4 id=&quot;Squeeze&quot;&gt;Squeeze&lt;a class=&quot;anchor-link&quot; href=&quot;#Squeeze&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Each filter only has a local receptive field, which is only focusing on that region. The researchers therefore propose the squeeze operation to create one descriptor of global information. It is generated by shrinking the input $U$ (using global avg. pooling) through it's spatial dimensions :&lt;/p&gt;
&lt;p&gt;$z_{c} = F_{sq}(u_{c}) = \dfrac{1}{H*W} * \sum\limits_{i=1}^H \sum\limits_{j=1}^W u_{c}(i,j) $&lt;/p&gt;
&lt;p&gt;The output $z$ can be interpreted as a collection of the local descriptors, together they describe the whole image.&lt;/p&gt;
&lt;h4 id=&quot;Excitation&quot;&gt;Excitation&lt;a class=&quot;anchor-link&quot; href=&quot;#Excitation&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;The excitation block is responsible for capturing channel-wise dependencies. The authors choose a function that is 1) flexibel (learn non-linear interactions between channels) and 2)  has a non-mutually exclusive relationship, because they want to have the ability to emphasize multiple channels and thereby avoid using a one-hot activation. The authors propose a gating mechanism to do this :&lt;/p&gt;
&lt;p&gt;$s = F_{ex}(z,W) = \sigma(g,(z,W)) = \sigma(W_{2}\delta(W_{1}z))$&lt;/p&gt;
&lt;p&gt;Here $\delta$ is the ReLU activation function&lt;br /&gt;
$W_{1}  \epsilon R^{\frac{C}{R} * C} $&lt;br /&gt;
$W_{2}  \epsilon R^{C * \frac{C}{R} } $&lt;/p&gt;
&lt;p&gt;In order to limit the compute complexity, the authors make the gating mechanism consist of a bottleneck with 2 FC layers around the non-linear activation function. It uses a reduction ration r (for good choice of this refer to 6.1 in the paper), a ReLU and then an increasing layer for dimensionality. The final output is then obtained by rescaling $U$ with an activation (scalar multiplies channel-wise). In a way SE blocks produce their own self attention on channels that are not locally confined, by mapping the descriptor produced by squeeze to channel weights.&lt;/p&gt;
&lt;p&gt;SE blocks can be nicely implemented into existing architectures such as Inception, ResNet or ResNeXt blocks. DOW&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/SE_kinds.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Compute-complexity&quot;&gt;Compute complexity&lt;a class=&quot;anchor-link&quot; href=&quot;#Compute-complexity&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Due to the global avg pooling in the squeeze part, only two FC layers and channelwise scaling in the excitation part, only 0.26% inference compute increase compared to a normal ResNet-50 (with image size 224x224) is observed. SE blocks increases ResNet parameter size by about ~10% (2.5 Million parameters).&lt;/p&gt;
&lt;h3 id=&quot;Results&quot;&gt;Results&lt;a class=&quot;anchor-link&quot; href=&quot;#Results&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;the authors prove that Squeeze and Excitation do both improve the performance and can be easily added to most architectures. It yields an improvement independent of architecture, but the exact type of SE block used should be researched depending on the base architecture.&lt;/li&gt;
&lt;li&gt;Later SE layers learn close to the Identity mapping, so &lt;/li&gt;
&lt;li&gt;2-3% mAP improvement compared to ResNet backbone Faster-RCNN&lt;/li&gt;
&lt;li&gt;25% improvement on ImageNet top-5 error&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/se_block.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/se_block.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">PANet Paper Summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/27/PANet.html" rel="alternate" type="text/html" title="PANet Paper Summary" /><published>2020-06-27T00:00:00-05:00</published><updated>2020-06-27T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/27/PANet</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/27/PANet.html">&lt;h2 id=&quot;panet-paper-summary&quot;&gt;PANet Paper Summary&lt;/h2&gt;

&lt;h3 id=&quot;what-did-the-authors-want-to-achieve-&quot;&gt;What did the authors want to achieve ?&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/PAN.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;improve Computer Vision tasks object detection and mainly instance segmentation&lt;/li&gt;
  &lt;li&gt;build on top of Fast/ Faster /Mask RCNN and improve info propagation&lt;/li&gt;
  &lt;li&gt;design an architecture that can deal with blurry and heavy occlusion of the new datasets back then (2018) , like COCO 2017&lt;/li&gt;
  &lt;li&gt;use in-net feature hierarchy : top down path with lateral connections is augmented to emphasize strong sementical features&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;methods-used&quot;&gt;Methods used&lt;/h3&gt;
&lt;h4 id=&quot;findings&quot;&gt;Findings&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Mask RCNN : long path from low-level to topmost features, which makes it difficult to access localization information&lt;/li&gt;
  &lt;li&gt;Mask RCNN : only single view, multi view preferd to gather diverse information&lt;/li&gt;
  &lt;li&gt;Mask RCNN : predictions based on pooled feature grides that are assigned heuristically, can be updated since lower level info can be important for final prediction&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;contributions&quot;&gt;Contributions&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;PANet is proposed for instance segementation&lt;/li&gt;
  &lt;li&gt;1) bottom-up path augmentation to shorten information path and improve feature pyramid with accurate low-level information 
=&amp;gt; new : propagate low-level features to enhance the feature hierarchy for instance recogniton&lt;/li&gt;
  &lt;li&gt;2) Adaptive feature pooling is introduced to recover broken information between each proposal and all feature levels&lt;/li&gt;
  &lt;li&gt;3) for multi view : augmentation of mask prediction with small fc layers : more diverse info, masks have better quality&lt;/li&gt;
  &lt;li&gt;1) &amp;amp; 2) are both used for detection and segmentation and lead to improvements of both tasks&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;framework&quot;&gt;Framework&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/bottom_up.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bottom-up Path Augmentation&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Intuition : bottom up is augmented to easily propagate lower layers&lt;/li&gt;
  &lt;li&gt;We know that high layers respond to whole objects, lower layers to fine features&lt;/li&gt;
  &lt;li&gt;Localization can be enhanced with top-down paths (FPN)&lt;/li&gt;
  &lt;li&gt;Here a path from low levels to higher ones is built, based on higher layers respose to edges and instance parts which helps localization error&lt;/li&gt;
  &lt;li&gt;Approach follow FPN, also using ResNet : layers with same spatial size are in same feature space (b) in figure 1)&lt;/li&gt;
  &lt;li&gt;As shown in figure 2), each feature map takes a higher resolution feature map $N_{i}$ and a coarser map $P_{i+1}$ and generates a new one using a 3x3 conv with stride 2 for size reduction on each $N_{i}$ map. After that each element of $P_{i+1}$ and the down sampled map are added using lateral connection. The fused map is convoluted using another 3x3 kernel to generate $N_{i+1}$, the whole process is iterated until $P_{5}$ is reached. All convs are followed by a ReLU.&lt;/li&gt;
  &lt;li&gt;up to 0.9 mAP improvement with large input sizes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/mask_pred_pan.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Adaptive Feature Pooling&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Idea : Adapative feature pooling allows each propsal to access info from all levels&lt;/li&gt;
  &lt;li&gt;In FPN, small proposals are assigned to low level features and high proposals to higher level ones . This can be non-optimal, as e.g. 2 examples with 10-pixel difference can be assigned to different levels even though they are rather similar. Also features may not correlate strongly with the layer they belong to.&lt;/li&gt;
  &lt;li&gt;High-level features have a larger receptive field and a more global context, whereas lower ones have fine details and high-localization accuracy. Therefore pooling from all levels and all proposals is fused for prediction. The authors call it adaptive feature pooling. For fusion max operations are used. For each level a ratio of kept features is calculated, surprisingly almost 70% are from other higher levels. The findings show that, features from multi levels together are helpful for accurate prediction. An intuition that is similar to DenseNet.This also supports bottom-up augmentation.&lt;/li&gt;
  &lt;li&gt;The exact process can be seen in figure 1 c), at first each proposal is mapped to different feature levels. Then ROIAlign is used to pool grids from each level. After that fusion of feature grids from different levels is performed using an element-wise max or sum. The focus is on in net feature hierarchy, instead of using different levels from image pyramids. It is comparable to L2 norm, where concat and dimension reduction are used.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Fully Connected Fusion&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Results have shown that MLP is also useful for pixelwise mask prediction. FC layers have different properties compared to FCN, FCN shares parameters and predicts based on local receptifve field. FC layers are localy sensitive since they do not use param sharing. They can adapt to different spatial locations. Using these ideas, it is helpful to differentiate parts in an image and use this information by fusing them.&lt;/li&gt;
  &lt;li&gt;Mask branch operates on pooled features and mainly consists of a FCN net with 4 conv (each 3x3 with 256 filters) and 1 deconv layer (upsample by 2). A shortcut from layer conv3 to fc is also implemented. The fc layer predicts a class agnostic foreground/background mask. It’s efficient and allows for better generality.&lt;/li&gt;
  &lt;li&gt;up to 0.7 mAP improvement for all scales&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Others&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MultiGPU Sync Batch Norm&lt;/li&gt;
  &lt;li&gt;Heavier head : effective for box AP&lt;/li&gt;
  &lt;li&gt;multi scale training&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Total improvement in AP is 4.4 over baseline, half of it is due to Synch BN and multi scale training&lt;/p&gt;

&lt;p&gt;Results&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Ablation study was done for architecture design&lt;/li&gt;
  &lt;li&gt;Winner of 2017 COCO Segmentation, SOTA performance in segmentation (Cityscapes) and detection&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">PANet Paper Summary</summary></entry><entry><title type="html">YOLOv4, a summary</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/24/YOLOv4.html" rel="alternate" type="text/html" title="YOLOv4, a summary" /><published>2020-06-24T00:00:00-05:00</published><updated>2020-06-24T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/24/YOLOv4</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/24/YOLOv4.html">&lt;h1 id=&quot;yolov4-optimal-speed-and-accuracy-of-object-detection-paper-summary&quot;&gt;YOLOv4: Optimal Speed and Accuracy of Object Detection Paper Summary&lt;/h1&gt;
&lt;p&gt;(Alexey Bochkovskiy,Chien-Yao Wang, Hong-Yuan Mark Liao)&lt;/p&gt;

&lt;h2 id=&quot;what-did-the-authors-want-to-achieve-&quot;&gt;What did the authors want to achieve ?&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;fast (real time) object detection, that can be trained on a GPU with 8-16 GB of VRAM&lt;/li&gt;
  &lt;li&gt;a model that can be easily trained and used&lt;/li&gt;
  &lt;li&gt;add state of the art methods for object detection, building on YOLOv3&lt;/li&gt;
  &lt;li&gt;find a good model for both GPU and VPU implementation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/yolov4.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;methods-used&quot;&gt;Methods used&lt;/h2&gt;

&lt;h3 id=&quot;bag-of-freebies-methods-that-only-increase-training-runtime-and-not-inference&quot;&gt;Bag of Freebies (methods that only increase training runtime and not inference)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;New Data Augmentation techniques are used, for photometric and geometric variability : 
=&amp;gt; Random Erase, Cutout to leave part of image of certain value
=&amp;gt; Dropout, Dropblock does the same with the net params&lt;/li&gt;
  &lt;li&gt;Mixup : mult image augmentation&lt;/li&gt;
  &lt;li&gt;Style Transfer GAN for texture stability
    &lt;h4 id=&quot;dataset-bias-&quot;&gt;Dataset Bias :&lt;/h4&gt;
    &lt;p&gt;=&amp;gt; focal loss, data imbalance between different classics
=&amp;gt; one-hot hard representation 
=&amp;gt; soft labels 
BBox regression :&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;MSE has x and y independent, also Anchors
=&amp;gt; IoU loss =&amp;gt; coverage and area are considered 
=&amp;gt; scale invariant, not the case with traditional methods
=&amp;gt; DIoU and CIoU loss&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bag-of-specials-methods-that-only-have-a-small-impact-on-inference-speed-but-improve-accuracy-significantly&quot;&gt;Bag of specials (methods that only have a small impact on inference speed, but improve accuracy significantly)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;only a small cost of compute during inference, improve accuracy :
    &lt;ul&gt;
      &lt;li&gt;enlarging receptive field : SPP, ASPP, RFB, SPP originates 		  from Spatial Pyramid Matching 
=&amp;gt; extract bag of words features 
 	- SPP infeasible for FCN nets, as it outputs a 1D feature 	  vector =&amp;gt; YOLOv3 concat of max pooling outputs with 		  kxk kernel size =&amp;gt; larger receptive field of the backbone 		  =&amp;gt; 2.7% higher AP50, 0.5 more compute necessary&lt;/li&gt;
      &lt;li&gt;ASPP diff. to SPP : max pool of 3x3, dilation of k&lt;/li&gt;
      &lt;li&gt;RFB : several dilated kxk convs =&amp;gt; 7% more AP, 5.7% more compute
Attention module :&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;mainly channelwise and pointwise attention, SE and SAM modules, SAM with no extra cost on GPU =&amp;gt; better for us                  &lt;br /&gt;
Feature Integration :&lt;/li&gt;
  &lt;li&gt;skip connections, hyper-column&lt;/li&gt;
  &lt;li&gt;channelwise weighting on multi-scale with FPN methods :
    &lt;ul&gt;
      &lt;li&gt;SFAM, ASFF, BiFPN, …
Activation Functions :
        &lt;ul&gt;
          &lt;li&gt;Mish, Swish, (fully differentiable) PReLu, …
Post-processing :&lt;/li&gt;
          &lt;li&gt;NMS “messy” with occlusions =&amp;gt; DIoU NMS distance : center to BBox screening process&lt;/li&gt;
          &lt;li&gt;NMS method not necessary in anchor free method&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture-selection&quot;&gt;Architecture Selection&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;things to think about : a reference that is good for classification is not always good for object detection, due to the detector needing :
    &lt;ul&gt;
      &lt;li&gt;higher input size (for small objects)&lt;/li&gt;
      &lt;li&gt;more layers (higher receptive field to cover larger input)&lt;/li&gt;
      &lt;li&gt;more parameters (for greater capacity, to detect different sized objects)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;=&amp;gt; a detector needs a backbone with more 3x3 convs and more params&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/yolov4comp.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Due to that, CSPDarknet53 seems to be the best choice in theory&lt;/p&gt;

&lt;p&gt;The following improvements are done :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SPP additional module for larger receptive field (almost no runtime disadvantage)&lt;/li&gt;
  &lt;li&gt;PANet path-aggregation neck as param aggregation method instead of FPN from YOLOv3&lt;/li&gt;
  &lt;li&gt;YOLOv3 anchor based head is used&lt;/li&gt;
  &lt;li&gt;DropBlock regularization method&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For single GPU training :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;synchBN is not considered : goal is to run on single GPU, thanks guys !!&lt;/li&gt;
  &lt;li&gt;new data augmentation mosaic (mixes 4 training images), Self Adversarial Training =&amp;gt; detection of objects outside their context&lt;br /&gt;
SAT : 
Forward Backward Training : 
1) adversarial attack is performed on input
2) neural net is trained to detect and object on this moded image in a normal way&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;optimal hyper-params while applying genetic algos&lt;/li&gt;
  &lt;li&gt;Cross mini Batch Normalization : mini-batch split within batch&lt;/li&gt;
  &lt;li&gt;SAM is modified from spatial-wise to pointwise attention, as can be seen below :&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/Pan_mod.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;architecture-summary&quot;&gt;Architecture Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Backbone : CSPDarknet53&lt;/li&gt;
  &lt;li&gt;Neck : SPP, PAN&lt;/li&gt;
  &lt;li&gt;Head : Yolov3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Techniques :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Bag of freebies for Backbone : 
CutMix and Mosaic data augmentation, DropBlock reg., class label smoothing&lt;/li&gt;
  &lt;li&gt;Bag of freebies for detector : 
CIoU-loss, CmBN, DropBlock reg., Mosaic data augmentation, SAT, Eliminate gird sensitivity, multi anchors for a single ground truth, cosine annealing, hyperparams, random training shapes&lt;/li&gt;
  &lt;li&gt;Bag of Specials for Backbone : 
Mish activation, Cross-stage partial connections (CSP), Multi-
input weighted residual connections (MiWRC)&lt;/li&gt;
  &lt;li&gt;Bag of specials for detector : 
Mish act., SPP-block, SAM-block, PAN path-aggregation block, DIoU-NMS&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;A FPS /mAP (@Iou50) comparison to other detecors can be seen below :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://Cedric-Perauer.github.io/DL_from_Foundations/images/Yolo_comp.png&quot; alt=&quot;images&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">YOLOv4: Optimal Speed and Accuracy of Object Detection Paper Summary (Alexey Bochkovskiy,Chien-Yao Wang, Hong-Yuan Mark Liao)</summary></entry><entry><title type="html">Summary Gradient Descent Optimization Algorithms</title><link href="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/27/Gradient.html" rel="alternate" type="text/html" title="Summary Gradient Descent Optimization Algorithms" /><published>2020-04-27T00:00:00-05:00</published><updated>2020-04-27T00:00:00-05:00</updated><id>https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/27/-Gradient</id><content type="html" xml:base="https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/27/Gradient.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-04-27- Gradient.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;What-did-the-authors-want-to-achieve-?&quot;&gt;What did the authors want to achieve ?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-the-authors-want-to-achieve-?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;give an overview of the different optimizers that exist and the challenges faced&lt;/li&gt;
&lt;li&gt;analyze additional methods that can be used to improve gradient descent&lt;/li&gt;
&lt;li&gt;the paper is more geared towards the mathematical understandinf of these optimizers as it does not make performance comparisons  &lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Key-elements-(Methods-in-this-case)&quot;&gt;Key elements (Methods in this case)&lt;a class=&quot;anchor-link&quot; href=&quot;#Key-elements-(Methods-in-this-case)&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In the following all sorts of SGD variants and optimization algorithms will be adressed. The following parameters are universal in the upcoming formulas:&lt;/p&gt;
&lt;p&gt;θ : parameters&lt;br /&gt;
 η : learning rate&lt;br /&gt;
J(θ) : objective function depening on models parameters&lt;/p&gt;
&lt;h2 id=&quot;SGD-Based&quot;&gt;SGD Based&lt;a class=&quot;anchor-link&quot; href=&quot;#SGD-Based&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;h3 id=&quot;Batch-Gradient-Descent&quot;&gt;Batch Gradient Descent&lt;a class=&quot;anchor-link&quot; href=&quot;#Batch-Gradient-Descent&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;h4 id=&quot;Formula-:&quot;&gt;Formula :&lt;a class=&quot;anchor-link&quot; href=&quot;#Formula-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;$θ = θ − η · ∇θJ(θ)$&lt;/p&gt;
&lt;h4 id=&quot;Code-:&quot;&gt;Code :&lt;a class=&quot;anchor-link&quot; href=&quot;#Code-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is the vanilla version, it updates all parameters in one update. It is redundant and very slow and can be impossible to compute for large datasets due to memory limitations. It also does not allow for online training.&lt;/p&gt;
&lt;h3 id=&quot;Stochastic-Gradient-Descent-(SGD)&quot;&gt;Stochastic Gradient Descent (SGD)&lt;a class=&quot;anchor-link&quot; href=&quot;#Stochastic-Gradient-Descent-(SGD)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;SGD performs a parameter update for each training example ${x^i}$ and ${y^i}$ respectively and therefore allows online training as well as faster training and the ability to train larger datasets.&lt;/p&gt;
&lt;h4 id=&quot;Formula-:&quot;&gt;Formula :&lt;a class=&quot;anchor-link&quot; href=&quot;#Formula-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;$θ = θ − η · ∇θJ(θ; {x^i};{y^i})$&lt;/p&gt;
&lt;h4 id=&quot;Code-:&quot;&gt;Code :&lt;a class=&quot;anchor-link&quot; href=&quot;#Code-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;SGD also has the potential to reach better local minima, convergance is compilcated however but can be combated with weight decay,almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively. As is common for most batch gradient descent techniques the data is also shuffled in the beginning, the advantages of shuffling are discussed in the additional strategies section.&lt;/p&gt;
&lt;h3 id=&quot;Mini-Batch-Gradient-Descent-(most-commonly-refered-to-as-the-actual-SGD)&quot;&gt;Mini-Batch Gradient Descent (most commonly refered to as the actual SGD)&lt;a class=&quot;anchor-link&quot; href=&quot;#Mini-Batch-Gradient-Descent-(most-commonly-refered-to-as-the-actual-SGD)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Mini-Batch Gradient Descent  takes the best of both worlds and performs updates on smaller batches (common sizes range between 16 and 1024).&lt;/p&gt;
&lt;h4 id=&quot;Formula-:&quot;&gt;Formula :&lt;a class=&quot;anchor-link&quot; href=&quot;#Formula-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;$θ = θ − η · ∇θJ(θ; {x^{i:i+n}};{y^{i:i+n}})$&lt;/p&gt;
&lt;h4 id=&quot;Code-:&quot;&gt;Code :&lt;a class=&quot;anchor-link&quot; href=&quot;#Code-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_batches&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It is usually refered to as SGD. This technique reduces the variance, which leads to better convergence. It can also make use of highly computationally optimized matrix operations to make computing the gradient of a mini-batch very fast.&lt;/p&gt;
&lt;h4 id=&quot;Challenges-of-SGD&quot;&gt;Challenges of SGD&lt;a class=&quot;anchor-link&quot; href=&quot;#Challenges-of-SGD&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;Vanilla SGD does not guarantee good convergence, so the following challenges have to be adressed :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;choosing the learning rate is very difficult, small values can lead to very slow convergence while large ones can lead to divergence&lt;/li&gt;
&lt;li&gt;techniques that combat this like annealing and scheduling of the $\eta$ are used to combat this, however they do not adjust to the dataset used and have to be specified in advance&lt;/li&gt;
&lt;li&gt;the learning rate is fixed for all parameters, however we would like to updates each parameter depending on the occurence of features (e.g. higher lr for rarer features) &lt;/li&gt;
&lt;li&gt;minimizing non-convex functions, where the convex criterion :       &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$f(\lambda*x_{1}+(1-\lambda)*x_{2}) {\leq } \lambda*f(x_{1}) + (1-\lambda) * f(x_{2})$&lt;/p&gt;
&lt;p&gt;is not fullfilled, often leads to convergence in local minima. Often this is due to saddle points as Dauphin et al. found out.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Gradient-descent-optimization-algorithms&quot;&gt;Gradient descent optimization algorithms&lt;a class=&quot;anchor-link&quot; href=&quot;#Gradient-descent-optimization-algorithms&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In the following widely used optimization algorithms will be adressed. The following parameters are universal in the upcoming formulas:&lt;/p&gt;
&lt;p&gt;θ : parameters&lt;br /&gt;
 η : learning rate&lt;br /&gt;
J(θ) : objective function depening on models parameters&lt;br /&gt;
 $γ$ : fraction that was introduced with Momentum (usually set to 0.9)&lt;br /&gt;
 $t$ (index) : time step&lt;br /&gt;
 $\epsilon$ : smoothing term to avoid division by zero, introduced by Adagrad (usuall set to 1e-8)&lt;/p&gt;
&lt;h3 id=&quot;Momentum&quot;&gt;Momentum&lt;a class=&quot;anchor-link&quot; href=&quot;#Momentum&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Momentum.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;SGD has trouble navigating ravines (area where surface curves much more steeply in one dimension than in another), SGD often oscillates in these cases while only making small progress. These are common around local optima. 
Momentum helps accelerate SGD in the relevant direction and dampens these ocsillations by introducing a new $γ$ term that is used to add a fraction of the update vector of the past time step to the current update vector.&lt;br /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Formula-:&quot;&gt;Formula :&lt;a class=&quot;anchor-link&quot; href=&quot;#Formula-:&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;$vt = γv_{t−1} + η∇θJ(θ)$&lt;br /&gt;
$θ = θ − v_{t}$
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;An analogy to this is a ball rolling dowhill, it becomes faster on the way (until it reaches its maximum speed due to air resistance, i.e. $γ&amp;lt;1$)&lt;/p&gt;
&lt;h3 id=&quot;Nesterov-accelerated-gradient-(NGA)&quot;&gt;Nesterov accelerated gradient (NGA)&lt;a class=&quot;anchor-link&quot; href=&quot;#Nesterov-accelerated-gradient-(NGA)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The idea of NGA is that we do not want to blindly go downhill, but rather know when the uphill section starts so we can slow down again. NGA does take this into account by using our momentum term $γv_{t−1}$ to approximate the next time step using $θ − γv_{t}$. (only the gradient is missing for the full update). This allows us to compute the gradient w.r.t the approximate future position of our parameters rather than the current one. 
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Formula&quot;&gt;Formula&lt;a class=&quot;anchor-link&quot; href=&quot;#Formula&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;$vt = γv_{t−1} + η∇θJ(θ-γv_{t−1})$&lt;br /&gt;
$θ = θ − v_{t}$
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/DL_from_Foundations/images/copied_from_nb/images/Hinton_mom.png&quot; alt=&quot;&quot; /&gt;
NAG first makes a jump in the direction of calculated gradient (brown vector) and then makes a correction (green vector) 
This anticipatory update prevents us from going too fast and results in increased
responsiveness, which has significantly increased the performance of RNNs on a number of tasks. Now we are able to update e&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Results-and-Conclusion&quot;&gt;Results and Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Results-and-Conclusion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The authors were able to give a good overview of different SGD techniques and it's optimizers like Momentum, Nesterov accelerated gradient, Adagrad, Adadelta,
RMSprop, Adam, AdaMax, Nadam, as well as different algorithms to optimize asynchronous SGD. Additionally strategies that can improve upon this like curriculum learning, batch norm and early stopping were deployed.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://cedric-perauer.github.io/DL_from_Foundations/images/resnet_var.png" /><media:content medium="image" url="https://cedric-perauer.github.io/DL_from_Foundations/images/resnet_var.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>