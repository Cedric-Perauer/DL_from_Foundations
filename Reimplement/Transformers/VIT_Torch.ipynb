{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn  \n",
    "import matplotlib.pyplot as plt \n",
    "import torchvision\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 100\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32,32,3)\n",
    "import torchvision.transforms as transforms\n",
    "#dataset download \n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), \n",
    "                transforms.Resize(image_size)])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data.cifar100', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR100(root='./data.cifar100', train=False,\n",
    "                                    download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(trainset,batch_size=64)\n",
    "test_dataloader = torch.utils.data.DataLoader(testset,batch_size=64)\n",
    "\n",
    "len_train, len_test = len(train_dataloader), len(test_dataloader)\n",
    "\n",
    "data_iter = iter(train_dataloader)\n",
    "img, label = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):  \n",
    "    def __init__(self,dim,depth,heads,head_dim,mlp_dim,drop_prob=0.0):   \n",
    "        super(Attention,self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm_Residual(dim,nn.MultiheadAttention(dim,heads,drop_prob)), \n",
    "                PreNorm_Residual(dim,FeedForward(dim,mlp_dim,dropout=drop_prob))\n",
    "            ]))\n",
    "    \n",
    "    def forward(self,x):  \n",
    "        for attention, \n",
    "\n",
    "class PreNorm_Residual(nn.Module):\n",
    "    def __init__(self,layer,dim):\n",
    "        super(PreNorm,self).__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layer = layer\n",
    "\n",
    "    def forward(self,x):  \n",
    "        return self.layer(self.norm(x)) + x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,dim,hidden_dim,drop_prob=0.0):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.fc1 = nn.Linear(dim,hidden_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim,dim)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self,x): \n",
    "        x = self.drop(self.gelu(self.fc1(x))) \n",
    "        x = self.drop(self.gelu(self.fc2(x))) \n",
    "        return x \n",
    "\n",
    "\n",
    "class VIT(nn.Module):\n",
    "   def __init__(self,transformer,image_size=image_size,patch_size=patch_size,num_classes=num_classes,projection_dim=projection_dim,num_patches=num_patches):\n",
    "      super(VIT,self).__init__()\n",
    "      assert image_size % patch_size == 0, \"image size must be dividible by patch size\" \n",
    "      self.num_patches =  num_patches\n",
    "      self.patch_dim = 3 * patch_size**2\n",
    "   \n",
    "      self.flatten_patches = Rearrange('b c (h px1) (w px2) -> b (h w) (px1 px2 c)', p1 = patch_size, p2 = patch_size)\n",
    "      self.patch_emedding = nn.Linear(self.patch_dim,projection_dim)\n",
    "\n",
    "      self.position_embedding = nn.Parameter(torch.randn(1, num_patches + 1, projection_dim))   #pose embedding \n",
    "      self.class_token = nn.Parameter(torch.randn(1, 1, projection_dim))   #class embedding \n",
    "      #output\n",
    "      self.mlp_head = nn.Sequential(\n",
    "               nn.LayerNorm(projection_dim),\n",
    "               nn.Linear(projection_dim,num_classes)\n",
    "      )\n",
    "\n",
    "      self.transformer = transformer\n",
    "      self.to_latent = nn.Identity()  \n",
    "   \n",
    "   def forward(self,img):\n",
    "      #\"pre process\"\n",
    "      x = self.flatten_patches(x)\n",
    "      x = self.patch_emedding(x)\n",
    "\n",
    "      b, n, _ = x.shape \n",
    "      class_token = repeat(self.class_token,'() n d -> b n d', b = b)\n",
    "      x = torch.cat((class_token,x),dim=1)\n",
    "      x += self.position_embedding[:,:(n+1)] \n",
    "      x = self.transformer(x)\n",
    "      x = x[:,0]\n",
    "      x = self.to_latent(x)\n",
    "      x = self.mlp_head(x) \n",
    "      return x \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPS(nn.Module): \n",
    "    def __init__(self,hidden_units,drop_prob=0.0,hidden_size1=64):\n",
    "        super(MLPS,self).__init__()\n",
    "        self.layers = []\n",
    "        hidden_units.insert(0,hidden_size1)\n",
    "        for i in range(0,len(hidden_units)-1): \n",
    "            self.layers.append(nn.Linear(hidden_units[i],hidden_units[i+1]))\n",
    "            self.layers.append(nn.Dropout(drop_prob))\n",
    "        \n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "    \n",
    "    def forward(self,x): \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,hidden_units,num_heads,projection_dim,dropout=0.0):\n",
    "        super(TransformerBlock,self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.attention = nn.MultiheadAttention(projection_dim,num_heads,dropout=dropout)\n",
    "        self.norm = nn.LayerNorm(projection_dim,eps=1e-6)\n",
    "        self.mlp = MLPS(hidden_units)\n",
    "\n",
    "    def forward(self,encoded_patches):\n",
    "        x1 = encoded_patches\n",
    "        #add norm layer here\n",
    "        attention_out, out_weights = self.attention(x1,x1,x1)\n",
    "        x2 = x1 + x1 #residual concatenation \n",
    "        #add norm layer here\n",
    "        x3 = self.mlp(x2)\n",
    "        encoded = x3 + x2\n",
    "        return encoded\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "   def __init__(self,transformer=None,image_size=image_size,patch_size=patch_size,num_classes=num_classes,projection_dim=projection_dim,num_patches=num_patches,\n",
    "   hidden_units=transformer_units, num_heads = num_heads,mlp_head_units=mlp_head_units, drop_prob=0.0):\n",
    "      super(VIT,self).__init__()\n",
    "      assert image_size % patch_size == 0, \"image size must be dividible by patch size\" \n",
    "      self.num_patches =  num_patches\n",
    "      self.patch_dim = 3 * patch_size**2\n",
    "   \n",
    "      self.flatten_patches = Rearrange('b c (h px1) (w px2) -> b (h w) (px1 px2 c)', px1 = patch_size, px2 = patch_size)\n",
    "      self.patch_emedding = nn.Linear(self.patch_dim,projection_dim)\n",
    "\n",
    "      self.position_embedding = nn.Parameter(torch.randn(1, num_patches + 1, projection_dim))   #pose embedding \n",
    "      self.class_token = nn.Parameter(torch.randn(1, 1, projection_dim))   #class embedding \n",
    "     \n",
    "      self.transformer = TransformerBlock(hidden_units,num_heads,projection_dim,drop_prob)\n",
    "      self.pose_embedding = nn.Embedding(num_patches+1,projection_dim)\n",
    "      self.projection = nn.Linear(108,projection_dim)\n",
    "      #output\n",
    "      self.mlp_head = MLPS(mlp_head_units,drop_prob=0.5,hidden_size1=9216)\n",
    "\n",
    "      self.to_latent = nn.Linear(mlp_head_units[-1],num_classes)\n",
    "      self.flatten = nn.Flatten()\n",
    "      self.drop = nn.Dropout(drop_prob)\n",
    "\n",
    "   \n",
    "   def forward(self,img):\n",
    "      #\"pre process\"\n",
    "      \"\"\"\n",
    "      x = self.flatten_patches(x)\n",
    "      x = self.patch_emedding(x)\n",
    "      \"\"\"\n",
    "      patch = self.flatten_patches(img)\n",
    "      #patch encoding \n",
    "      positions = torch.arange(0,self.num_patches)\n",
    "      encoded1 = self.pose_embedding(positions)\n",
    "      encoded2 = self.projection(patch)\n",
    "      encoded = encoded1 + encoded2\n",
    "      block = self.transformer(encoded)\n",
    "      block_norm = block #add norm here\n",
    "      representation = self.drop(self.flatten(block_norm))\n",
    "      features = self.mlp_head(representation)\n",
    "      logits = self.to_latent(features)\n",
    "      return logits \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 64\n",
      "layers Sequential(\n",
      "  (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (1): Dropout(p=0.0, inplace=False)\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "layers Sequential(\n",
      "  (0): Linear(in_features=9216, out_features=9216, bias=True)\n",
      "  (1): Dropout(p=0.5, inplace=False)\n",
      "  (2): Linear(in_features=9216, out_features=2048, bias=True)\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "img torch.Size([1, 3, 72, 72])\n"
     ]
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "mlp_head_units = [2048,1024] \n",
    "model = VIT(hidden_units=transformer_units)\n",
    "img, label = next(data_iter)\n",
    "img = img[0]\n",
    "img = img.unsqueeze(0)\n",
    "model(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}