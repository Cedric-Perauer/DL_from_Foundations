{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn  \n",
    "import matplotlib.pyplot as plt \n",
    "import torchvision\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 100\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 100\n",
    "input_shape = (32,32,3)\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 5\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.Resize(image_size)\n",
    "    ])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPS(nn.Module): \n",
    "    def __init__(self,hidden_units,drop_prob=0.0,hidden_size1=64):\n",
    "        super(MLPS,self).__init__()\n",
    "        self.layers = []\n",
    "        hidden_units.insert(0,hidden_size1)\n",
    "        for i in range(0,len(hidden_units)-1): \n",
    "            self.layers.append(nn.Linear(hidden_units[i],hidden_units[i+1]))\n",
    "            self.layers.append(nn.Dropout(drop_prob))\n",
    "        \n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "    \n",
    "    def forward(self,x): \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,hidden_units,num_heads,projection_dim,dropout=0.0):\n",
    "        super(TransformerBlock,self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.attention = nn.MultiheadAttention(projection_dim,num_heads,dropout=dropout)\n",
    "        self.norm = nn.LayerNorm(projection_dim,eps=1e-6)\n",
    "        self.mlp = MLPS(hidden_units)\n",
    "\n",
    "    def forward(self,encoded_patches):\n",
    "        x1 = encoded_patches\n",
    "        #add norm layer here\n",
    "        attention_out, out_weights = self.attention(x1,x1,x1)\n",
    "        x2 = x1 + x1 #residual concatenation \n",
    "        #add norm layer here\n",
    "        x3 = self.mlp(x2)\n",
    "        encoded = x3 + x2\n",
    "        return encoded\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "   def __init__(self,transformer=None,image_size=image_size,patch_size=patch_size,num_classes=num_classes,projection_dim=projection_dim,num_patches=num_patches,\n",
    "   hidden_units=transformer_units, num_heads = num_heads,mlp_head_units=mlp_head_units, drop_prob=0.0):\n",
    "      super(VIT,self).__init__()\n",
    "      assert image_size % patch_size == 0, \"image size must be dividible by patch size\" \n",
    "      self.num_patches =  num_patches\n",
    "      self.patch_dim = 3 * patch_size**2\n",
    "   \n",
    "      self.flatten_patches = Rearrange('b c (h px1) (w px2) -> b (h w) (px1 px2 c)', px1 = patch_size, px2 = patch_size)\n",
    "      self.patch_emedding = nn.Linear(self.patch_dim,projection_dim)\n",
    "\n",
    "      self.position_embedding = nn.Parameter(torch.randn(1, num_patches + 1, projection_dim))   #pose embedding \n",
    "      self.class_token = nn.Parameter(torch.randn(1, 1, projection_dim))   #class embedding \n",
    "     \n",
    "      self.transformer = TransformerBlock(hidden_units,num_heads,projection_dim,drop_prob)\n",
    "      self.pose_embedding = nn.Embedding(num_patches+1,projection_dim)\n",
    "      self.projection = nn.Linear(108,projection_dim)\n",
    "      #output\n",
    "      self.mlp_head = MLPS(mlp_head_units,drop_prob=0.5,hidden_size1=9216)\n",
    "\n",
    "      self.to_latent = nn.Linear(mlp_head_units[-1],num_classes)\n",
    "      self.flatten = nn.Flatten()\n",
    "      self.drop = nn.Dropout(drop_prob)\n",
    "      self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "   \n",
    "   def forward(self,img):\n",
    "      #\"pre process\"\n",
    "      \"\"\"\n",
    "      x = self.flatten_patches(x)\n",
    "      x = self.patch_emedding(x)\n",
    "      \"\"\"\n",
    "      patch = self.flatten_patches(img)\n",
    "      #patch encoding \n",
    "      positions = torch.arange(0,self.num_patches)\n",
    "      encoded1 = self.pose_embedding(positions)\n",
    "      encoded2 = self.projection(patch)\n",
    "      encoded = encoded1 + encoded2\n",
    "      block = self.transformer(encoded)\n",
    "      block_norm = block #add norm here\n",
    "      representation = self.drop(self.flatten(block_norm))\n",
    "      features = self.mlp_head(representation)\n",
    "      logits = self.to_latent(features)\n",
    "      return logits \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.097453594207764\n",
      "378.1101107597351\n",
      "833.0985903739929\n",
      "1527.257068157196\n",
      "2234.0160250663757\n",
      "3477.58642911911\n",
      "4844.256808757782\n",
      "5661.485583782196\n",
      "7157.842547893524\n",
      "8843.365466594696\n",
      "9598.342578411102\n",
      "11015.31965970993\n",
      "12370.721118450165\n",
      "12923.319079875946\n",
      "14260.325152873993\n",
      "14727.783954143524\n",
      "15695.101459026337\n",
      "16672.40446805954\n",
      "18322.389056682587\n",
      "19063.379123210907\n",
      "19501.151111125946\n",
      "19724.660193920135\n",
      "20229.36669111252\n",
      "20588.683936595917\n",
      "21609.88701581955\n",
      "21989.82338666916\n",
      "22639.956290721893\n",
      "23181.625678539276\n",
      "23448.797652721405\n",
      "24064.12155866623\n",
      "24509.9236369133\n",
      "24783.65441083908\n",
      "24966.737945079803\n",
      "26076.55277967453\n",
      "26622.13341474533\n",
      "27141.626212596893\n",
      "27797.334525585175\n",
      "28760.880210399628\n",
      "29136.16329908371\n",
      "29620.96054792404\n",
      "29910.05966901779\n",
      "30261.666839122772\n",
      "30581.55081510544\n",
      "30921.21092557907\n",
      "31588.05144071579\n",
      "32522.729785442352\n",
      "33655.37001371384\n",
      "34073.66751432419\n",
      "34238.18985700607\n",
      "34839.27078962326\n",
      "35069.88425016403\n",
      "35594.79922056198\n",
      "36022.68233060837\n",
      "36164.25070524216\n",
      "36747.819850444794\n",
      "36943.06427717209\n",
      "37172.735698223114\n",
      "37557.91563749313\n",
      "37850.945003032684\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7f8fbe6eb3cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "\n",
    "if device == \"cuda\":\n",
    "    model = model.to(device)\n",
    "\n",
    "val_loss_min = float(\"Inf\")\n",
    "for i in range(0,num_epochs):\n",
    "    train_loss,val_loss = 0.,0. \n",
    "    model.train()\n",
    "\n",
    "    for img, label in train_loader:\n",
    "        if device == \"cuda\":\n",
    "            img, label = img.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img)\n",
    "        loss = criterion(out,label)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        train_loss += loss.item() * batch_size\n",
    "        print(train_loss)\n",
    "    #validation\n",
    "    \n",
    "    model.eval()\n",
    "    for img, label in val_loader:\n",
    "        if device == \"cuda\":\n",
    "            img, label = img.to(device), label.to(device)\n",
    "        \n",
    "        out = model(img)\n",
    "        loss = criterion(out,label)\n",
    "        val_loss += loss.item() * batch_size\n",
    "    \n",
    "    val_loss = val_loss/len(val_loader.dataset)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "\n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, val_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        val_loss_min,\n",
    "        val_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        val_loss_min = val_loss\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([1, 3, 5, 9, 4])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "out = torch.Tensor([[ 0.0336,  0.2922, -0.2848,  1.5458, -0.0692, -0.0332, -0.4918,  1.7504,\n",
    "         -1.0620, -0.6737],\n",
    "        [ 0.0451,  0.0415, -0.0913,  1.0814, -0.6469,  1.1928, -1.3046, -0.3185,\n",
    "         -1.6709, -0.9702],\n",
    "        [-0.6552, -0.5635,  1.4852,  0.8358, -0.5598,  1.9645, -0.7523,  0.5076,\n",
    "          0.6408,  0.7347],\n",
    "        [-0.8069, -0.3164,  0.6914, -0.1573, -1.8538,  0.5686, -0.2744,  1.5524,\n",
    "         -0.4634,  1.1055],\n",
    "        [ 0.2714, -1.4807,  1.1702, -0.3368, -1.1552, -0.0126, -0.3035,  0.1973,\n",
    "         -0.0620,  0.5993]])\n",
    "\n",
    "loss = criterion(out,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0f92eddb19da3b7275830eb0f9466ea88625cd3073f7ac942e8ba9070a4b1cf65"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}