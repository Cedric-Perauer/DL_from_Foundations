{
  
    
        "post0": {
            "title": "Title",
            "content": "DL from the Foundations Batch Norm . Implementing Batch Norm, Layer Norm, Instance Norm, Group Norm and running batch norm (Lesson 3 Part 4) . toc:true - badges: true | comments: true | categories: [jupyter] | image: images/norms.png | . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . #collapse from exp.nb_06 import * . . ConvNet . Let&#39;s get the data and training interface from where we left in the last notebook. . Jump_to lesson 10 video . #collapse x_train,y_train,x_valid,y_valid = get_data() x_train,x_valid = normalize_to(x_train,x_valid) train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) nh,bs = 50,512 c = y_train.max().item()+1 loss_func = F.cross_entropy data = DataBunch(*get_dls(train_ds, valid_ds, bs), c) . . #collapse mnist_view = view_tfm(1,28,28) cbfs = [Recorder, partial(AvgStatsCallback,accuracy), CudaCallback, partial(BatchTransformXCallback, mnist_view)] . . #collapse nfs = [8,16,32,64,64] . . #collapse learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs) . . #collapse %time run.fit(2, learn) . . train: [1.22927203125, tensor(0.5976, device=&#39;cuda:0&#39;)] valid: [0.190905224609375, tensor(0.9418, device=&#39;cuda:0&#39;)] train: [0.17169419921875, tensor(0.9462, device=&#39;cuda:0&#39;)] valid: [0.108160009765625, tensor(0.9669, device=&#39;cuda:0&#39;)] CPU times: user 3.96 s, sys: 714 ms, total: 4.67 s Wall time: 5.38 s . Batchnorm . Custom . Let&#39;s start by building our own BatchNorm layer from scratch. We should be able to improve performance a lot. While training we keep a running exponentially weighted mean and variance average in the update_stats function. During inference we only use running mean and variance that we keep track off. We use register_bufferto create vars and means, this still creates self.vars and self.means, but if the model is moved to the GPU so will all buffers. Also it will be saved along with everything else in the model. . Jump_to lesson 10 video . #collapse_show class BatchNorm(nn.Module): def __init__(self, nf, mom=0.1, eps=1e-5): super().__init__() # NB: pytorch bn mom is opposite of what you&#39;d expect self.mom,self.eps = mom,eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) self.register_buffer(&#39;vars&#39;, torch.ones(1,nf,1,1)) self.register_buffer(&#39;means&#39;, torch.zeros(1,nf,1,1)) def update_stats(self, x): #we average over all batches (0) and over x,y(2,3) coordinates (each filter) #keepdim=True means we can still broadcast nicely as these dimensions will be left empty m = x.mean((0,2,3), keepdim=True) v = x.var ((0,2,3), keepdim=True) self.means.lerp_(m, self.mom) self.vars.lerp_ (v, self.mom) return m,v def forward(self, x): if self.training: with torch.no_grad(): m,v = self.update_stats(x) else: m,v = self.means,self.vars x = (x-m) / (v+self.eps).sqrt() return x*self.mults + self.adds . . Exponential moving average . We use exp. moving average, that way we only need to keep track of one element.The next value is computed with linear interpolation. PyTorch mom=0.1 is actually 0.9 in math terms. (1-mom) . Now we define our batch norm conv_layer, when we use batch norm we can remove the bias layer as batch norm adds are a bias. . #collapse_show def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs): # No bias needed if using bn layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn), GeneralRelu(**kwargs)] if bn: layers.append(BatchNorm(nf)) return nn.Sequential(*layers) . . #collapse_show def init_cnn_(m, f): if isinstance(m, nn.Conv2d): f(m.weight, a=0.1) if getattr(m, &#39;bias&#39;, None) is not None: m.bias.data.zero_() for l in m.children(): init_cnn_(l, f) def init_cnn(m, uniform=False): f = init.kaiming_uniform_ if uniform else init.kaiming_normal_ init_cnn_(m, f) def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs): model = get_cnn_model(data, nfs, layer, **kwargs) init_cnn(model, uniform=uniform) return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func) . . Above the modules are initalized recursively. We can then use it in training and see how it helps keep the activations means to 0 and the std to 1. . #collapse learn,run = get_learn_run(nfs, data, 0.9, conv_layer, cbs=cbfs) . . Train with Hooks : . #collapse_show with Hooks(learn.model, append_stats) as hooks: run.fit(1, learn) fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks[:-1]: ms,ss = h.stats ax0.plot(ms[:10]) ax1.plot(ss[:10]) h.remove() plt.legend(range(6)); fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks[:-1]: ms,ss = h.stats ax0.plot(ms) ax1.plot(ss) . . train: [0.092951201171875, tensor(0.9710, device=&#39;cuda:0&#39;)] valid: [0.1474322998046875, tensor(0.9511, device=&#39;cuda:0&#39;)] . #collapse learn,run = get_learn_run(nfs, data, 1.0, conv_layer, cbs=cbfs) . . #collapse %time run.fit(3, learn) . . train: [0.2753326953125, tensor(0.9117, device=&#39;cuda:0&#39;)] valid: [0.14528587646484376, tensor(0.9554, device=&#39;cuda:0&#39;)] train: [0.08339517578125, tensor(0.9742, device=&#39;cuda:0&#39;)] valid: [0.08496976318359376, tensor(0.9746, device=&#39;cuda:0&#39;)] train: [0.06142646484375, tensor(0.9813, device=&#39;cuda:0&#39;)] valid: [0.11257415771484375, tensor(0.9671, device=&#39;cuda:0&#39;)] CPU times: user 3.36 s, sys: 92.7 ms, total: 3.45 s Wall time: 3.4 s . Builtin batchnorm . Jump_to lesson 10 video . #collapse_show def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs): layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn), GeneralRelu(**kwargs)] if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1)) return nn.Sequential(*layers) . . #collapse learn,run = get_learn_run(nfs, data, 1., conv_layer, cbs=cbfs) . . #collapse %time run.fit(3, learn) . . train: [0.032240205078125, tensor(0.9905, device=&#39;cuda:0&#39;)] valid: [0.0587533935546875, tensor(0.9817, device=&#39;cuda:0&#39;)] train: [0.0237388916015625, tensor(0.9927, device=&#39;cuda:0&#39;)] valid: [0.07805890502929687, tensor(0.9750, device=&#39;cuda:0&#39;)] train: [0.018083040771484375, tensor(0.9945, device=&#39;cuda:0&#39;)] valid: [0.060346484375, tensor(0.9837, device=&#39;cuda:0&#39;)] CPU times: user 2.97 s, sys: 94.8 ms, total: 3.07 s Wall time: 3.02 s . With scheduler . Now let&#39;s add the usual warm-up/annealing. . #collapse sched = combine_scheds([0.3, 0.7], [sched_lin(0.6, 2.), sched_lin(2., 0.1)]) . . #collapse_show learn,run = get_learn_run(nfs, data, 0.9, conv_layer, cbs=cbfs +[partial(ParamScheduler,&#39;lr&#39;, sched)]) . . #collapse run.fit(8, learn) . . train: [0.239993203125, tensor(0.9268, device=&#39;cuda:0&#39;)] valid: [0.1067057373046875, tensor(0.9680, device=&#39;cuda:0&#39;)] train: [0.08213828125, tensor(0.9743, device=&#39;cuda:0&#39;)] valid: [0.1173908203125, tensor(0.9617, device=&#39;cuda:0&#39;)] train: [0.0569355517578125, tensor(0.9820, device=&#39;cuda:0&#39;)] valid: [0.072092724609375, tensor(0.9790, device=&#39;cuda:0&#39;)] train: [0.03151961181640625, tensor(0.9902, device=&#39;cuda:0&#39;)] valid: [0.06193500366210938, tensor(0.9809, device=&#39;cuda:0&#39;)] train: [0.019621298828125, tensor(0.9943, device=&#39;cuda:0&#39;)] valid: [0.04464823913574219, tensor(0.9869, device=&#39;cuda:0&#39;)] train: [0.011318720703125, tensor(0.9972, device=&#39;cuda:0&#39;)] valid: [0.04040174865722656, tensor(0.9886, device=&#39;cuda:0&#39;)] train: [0.007121054077148438, tensor(0.9986, device=&#39;cuda:0&#39;)] valid: [0.040744146728515625, tensor(0.9888, device=&#39;cuda:0&#39;)] train: [0.004829978637695313, tensor(0.9993, device=&#39;cuda:0&#39;)] valid: [0.04020232238769531, tensor(0.9892, device=&#39;cuda:0&#39;)] . More norms . Layer norm . From the paper: &quot;batch normalization cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small&quot;. This is the case for large Nets that only allow for small batch sizes. Also RNNs are a problem, as our for loop can not vary the batch size easily. . General equation for a norm layer with learnable affine: . $$y = frac{x - mathrm{E}[x]}{ sqrt{ mathrm{Var}[x] + epsilon}} * gamma + beta$$ . The difference with BatchNorm is . we don&#39;t keep a moving average | we don&#39;t average over the batches dimension but over the hidden dimension, so it&#39;s independent of the batch size | Jump_to lesson 10 video . #collapse_show class LayerNorm(nn.Module): __constants__ = [&#39;eps&#39;] def __init__(self, eps=1e-5): super().__init__() self.eps = eps self.mult = nn.Parameter(tensor(1.)) self.add = nn.Parameter(tensor(0.)) def forward(self, x): m = x.mean((1,2,3), keepdim=True) v = x.var ((1,2,3), keepdim=True) x = (x-m) / ((v+self.eps).sqrt()) return x*self.mult + self.add . . Keep in mind that compared to BN we use m = x.mean((1,2,3), keepdim=True) instead of m=x.mean((0,2,3), keepdim=True) and we do not use the exp. moving average. The reason is that every image has it&#39;s own mean as we do not use batches anymore. . #collapse def conv_ln(ni, nf, ks=3, stride=2, bn=True, **kwargs): layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True), GeneralRelu(**kwargs)] if bn: layers.append(LayerNorm()) return nn.Sequential(*layers) . . #collapse_show learn,run = get_learn_run(nfs, data, 0.8, conv_ln, cbs=cbfs) . . #collapse_show %time run.fit(3, learn) . . train: [nan, tensor(0.1256, device=&#39;cuda:0&#39;)] valid: [nan, tensor(0.0991, device=&#39;cuda:0&#39;)] train: [nan, tensor(0.0986, device=&#39;cuda:0&#39;)] valid: [nan, tensor(0.0991, device=&#39;cuda:0&#39;)] train: [nan, tensor(0.0986, device=&#39;cuda:0&#39;)] valid: [nan, tensor(0.0991, device=&#39;cuda:0&#39;)] CPU times: user 3.95 s, sys: 73.9 ms, total: 4.03 s Wall time: 3.98 s . Thought experiment: can this distinguish foggy days from sunny days (assuming you&#39;re using it before the first conv)? No we can not, layer norm will lead to the same normalization for both pictures. As we can see LN is not as good as BN, but it can be used for RNNS. . Instance norm . From the paper: . The key difference between contrast and batch normalization is that the latter applies the normalization to a whole batch of images instead for single ones: . begin{equation} label{eq:bnorm} y_{tijk} = frac{x_{tijk} - mu_{i}}{ sqrt{ sigma_i^2 + epsilon}}, quad mu_i = frac{1}{HWT} sum_{t=1}^T sum_{l=1}^W sum_{m=1}^H x_{tilm}, quad sigma_i^2 = frac{1}{HWT} sum_{t=1}^T sum_{l=1}^W sum_{m=1}^H (x_{tilm} - mu_i)^2. end{equation}In order to combine the effects of instance-specific normalization and batch normalization, we propose to replace the latter by the instance normalization (also known as contrast normalization) layer: . begin{equation} label{eq:inorm} y_{tijk} = frac{x_{tijk} - mu_{ti}}{ sqrt{ sigma_{ti}^2 + epsilon}}, quad mu_{ti} = frac{1}{HW} sum_{l=1}^W sum_{m=1}^H x_{tilm}, quad sigma_{ti}^2 = frac{1}{HW} sum_{l=1}^W sum_{m=1}^H (x_{tilm} - mu_{ti})^2. end{equation} Jump_to lesson 10 video . #collapse_show class InstanceNorm(nn.Module): __constants__ = [&#39;eps&#39;] def __init__(self, nf, eps=1e-0): super().__init__() self.eps = eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) def forward(self, x): m = x.mean((2,3), keepdim=True) v = x.var ((2,3), keepdim=True) res = (x-m) / ((v+self.eps).sqrt()) return res*self.mults + self.adds . . Keep in mind that compared to LN we use m = x.mean((2,3), keepdim=True) instead of m=x.mean((1,2,3), keepdim=True). . #collapse def conv_in(ni, nf, ks=3, stride=2, bn=True, **kwargs): layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True), GeneralRelu(**kwargs)] if bn: layers.append(InstanceNorm(nf)) return nn.Sequential(*layers) . . #collapse learn,run = get_learn_run(nfs, data, 0.1, conv_in, cbs=cbfs) . . #collapse %time run.fit(3, learn) . . train: [nan, tensor(0.0986, device=&#39;cuda:0&#39;)] valid: [nan, tensor(0.0991, device=&#39;cuda:0&#39;)] train: [nan, tensor(0.0986, device=&#39;cuda:0&#39;)] valid: [nan, tensor(0.0991, device=&#39;cuda:0&#39;)] train: [nan, tensor(0.0986, device=&#39;cuda:0&#39;)] valid: [nan, tensor(0.0991, device=&#39;cuda:0&#39;)] CPU times: user 4 s, sys: 58.2 ms, total: 4.05 s Wall time: 4.01 s . Question: why can&#39;t this classify anything? We are now using the mean and variance for every image and every channel, throwing away the things that allow classification. It was not designed for Classification, but rather for style transfer where the differences in contrast and overall amount are not important according to the authors. . Lost in all those norms? The authors from the group norm paper have you covered: . . Group norm . Jump_to lesson 10 video . From the PyTorch docs: . GroupNorm(num_groups, num_channels, eps=1e-5, affine=True) . The input channels are separated into num_groups groups, each containing num_channels / num_groups channels. The mean and standard-deviation are calculated separately over the each group. $ gamma$ and $ beta$ are learnable per-channel affine transform parameter vectorss of size num_channels if affine is True. . This layer uses statistics computed from input data in both training and evaluation modes. . Args: . num_groups (int): number of groups to separate the channels into | num_channels (int): number of channels expected in input | eps: a value added to the denominator for numerical stability. Default: 1e-5 | affine: a boolean value that when set to True, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: True. | . Shape: . Input: (N, num_channels, *) | Output: (N, num_channels, *) (same shape as input) | . Examples:: . &gt;&gt;&gt; input = torch.randn(20, 6, 10, 10) &gt;&gt;&gt; # Separate 6 channels into 3 groups &gt;&gt;&gt; m = nn.GroupNorm(3, 6) &gt;&gt;&gt; # Separate 6 channels into 6 groups (equivalent with InstanceNorm) &gt;&gt;&gt; m = nn.GroupNorm(6, 6) &gt;&gt;&gt; # Put all 6 channels into a single group (equivalent with LayerNorm) &gt;&gt;&gt; m = nn.GroupNorm(1, 6) &gt;&gt;&gt; # Activating the module &gt;&gt;&gt; output = m(input) . Fix small batch sizes . What&#39;s the problem? . When we compute the statistics (mean and std) for a BatchNorm Layer on a small batch, it is possible that we get a standard deviation very close to 0. because there aren&#39;t many samples (the variance of one thing is 0. since it&#39;s equal to its mean). . Jump_to lesson 10 video . #collapse data = DataBunch(*get_dls(train_ds, valid_ds, 2), c) . . #collapse_show def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs): layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn), GeneralRelu(**kwargs)] if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1)) return nn.Sequential(*layers) . . #collapse learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs) . . #collapse %time run.fit(1, learn) . . train: [2.3343459375, tensor(0.1976, device=&#39;cuda:0&#39;)] valid: [680970.0864, tensor(0.2176, device=&#39;cuda:0&#39;)] CPU times: user 1min 29s, sys: 759 ms, total: 1min 30s Wall time: 1min 30s . Running Batch Norm . To solve this problem we introduce a Running BatchNorm that uses smoother running mean and variance for the mean and std. Eps is used to avoid divergence, it is used as a hyperparameter. Running Batch Norm is a good solution (best so far according to Jeremy) for the small batch size problem. . 1) It does not divide by the batch standard deviation, but it uses the moving average stats at training time as well, just like during inference. Accuracy increases a lot ! As we should not compute the running average of the variances, especially as they might be different batch sizes as well. We use the formula : $E[X^{2}]-E[X]^{2}$ So we use the squares and the sums with a buffer. . 2) And then we take the exp. moving average of these and interpolate. We also take the exponential moving average of the batch sizes, it tells us what we need to divide by : (total number of elements by the mini batch divided by number of channels) . 3) Debiasing Make sure that at every point, no observation is weighted too much. (early elements have more relevance as they appear more often) . 4) For the first elements : We might be unlucky, so that our first mini batch is very close to zero. So we clamp the first few elements (for example 20) variance to be 0.01. . Jump_to lesson 10 video . #collapse_show class RunningBatchNorm(nn.Module): def __init__(self, nf, mom=0.1, eps=1e-5): super().__init__() self.mom,self.eps = mom,eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) self.register_buffer(&#39;sums&#39;, torch.zeros(1,nf,1,1)) self.register_buffer(&#39;sqrs&#39;, torch.zeros(1,nf,1,1)) self.register_buffer(&#39;batch&#39;, tensor(0.)) self.register_buffer(&#39;count&#39;, tensor(0.)) self.register_buffer(&#39;step&#39;, tensor(0.)) self.register_buffer(&#39;dbias&#39;, tensor(0.)) def update_stats(self, x): bs,nc,*_ = x.shape self.sums.detach_() self.sqrs.detach_() dims = (0,2,3) s = x.sum(dims, keepdim=True) ss = (x*x).sum(dims, keepdim=True) c = self.count.new_tensor(x.numel()/nc) mom1 = 1 - (1-self.mom)/math.sqrt(bs-1) self.mom1 = self.dbias.new_tensor(mom1) self.sums.lerp_(s, self.mom1) self.sqrs.lerp_(ss, self.mom1) self.count.lerp_(c, self.mom1) self.dbias = self.dbias*(1-self.mom1) + self.mom1 self.batch += bs self.step += 1 def forward(self, x): if self.training: self.update_stats(x) sums = self.sums sqrs = self.sqrs c = self.count if self.step&lt;100: sums = sums / self.dbias sqrs = sqrs / self.dbias c = c / self.dbias means = sums/c vars = (sqrs/c).sub_(means*means) if bool(self.batch &lt; 20): vars.clamp_min_(0.01) x = (x-means).div_((vars.add_(self.eps)).sqrt()) return x.mul_(self.mults).add_(self.adds) . . #collapse_show def conv_rbn(ni, nf, ks=3, stride=2, bn=True, **kwargs): layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn), GeneralRelu(**kwargs)] if bn: layers.append(RunningBatchNorm(nf)) return nn.Sequential(*layers) . . #collapse learn,run = get_learn_run(nfs, data, 0.4, conv_rbn, cbs=cbfs) . . #collapse %time run.fit(1, learn) . . train: [0.418019921875, tensor(0.8890, device=&#39;cuda:0&#39;)] valid: [0.1741728515625, tensor(0.9607, device=&#39;cuda:0&#39;)] CPU times: user 3min 56s, sys: 1.22 s, total: 3min 58s Wall time: 3min 58s . This solves the small batch size issue! . What can we do in a single epoch? . Now let&#39;s see with a decent batch size what result we can get. . Jump_to lesson 10 video . #collapse data = DataBunch(*get_dls(train_ds, valid_ds, 32), c) . . #collapse learn,run = get_learn_run(nfs, data, 0.9, conv_rbn, cbs=cbfs +[partial(ParamScheduler,&#39;lr&#39;, sched_lin(1., 0.2))]) . . #collapse %time run.fit(1, learn) . . train: [0.154025556640625, tensor(0.9523, device=&#39;cuda:0&#39;)] valid: [0.2699001953125, tensor(0.9759, device=&#39;cuda:0&#39;)] CPU times: user 15.1 s, sys: 70.4 ms, total: 15.1 s Wall time: 15.1 s . Export . nb_auto_export() .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/2020/04/10/Batchnorm.html",
            "relUrl": "/2020/04/10/Batchnorm.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "DL from the Foundations Callbacks and Learning Rate Finder",
            "content": "DL from the foundations Lesson 3 Part 2 . Exceptions for Callbacks . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . #collapse from exp.nb_05 import * . . Jump_to notebook introduction in lesson 10 video . Early stopping . Better callback cancellation . Jump_to lesson 10 video . #collapse x_train,y_train,x_valid,y_valid = get_data() train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) nh,bs = 50,512 c = y_train.max().item()+1 loss_func = F.cross_entropy . . #collapse data = DataBunch(*get_dls(train_ds, valid_ds, bs), c) . . Improving on Lesson 2 : it makes more sense for the __call__to be inside the callback class. We can now set traces, replace the method,... which gives us extra flexibility. Pass just means that a class has same attributes but we can now use a different name for the class. . #collapse_show class Callback(): _order=0 def set_runner(self, run): self.run=run def __getattr__(self, k): return getattr(self.run, k) @property def name(self): name = re.sub(r&#39;Callback$&#39;, &#39;&#39;, self.__class__.__name__) return camel2snake(name or &#39;callback&#39;) def __call__(self, cb_name): f = getattr(self, cb_name, None) if f and f(): return True return False class TrainEvalCallback(Callback): def begin_fit(self): self.run.n_epochs=0. self.run.n_iter=0 def after_batch(self): if not self.in_train: return self.run.n_epochs += 1./self.iters self.run.n_iter += 1 def begin_epoch(self): self.run.n_epochs=self.epoch self.model.train() self.run.in_train=True def begin_validate(self): self.model.eval() self.run.in_train=False class CancelTrainException(Exception): pass class CancelEpochException(Exception): pass class CancelBatchException(Exception): pass . . We can now use exceptions to handle control flow. So for example we can use them to go to the next batch when an exception is called. . #collapse_show class Runner(): def __init__(self, cbs=None, cb_funcs=None): cbs = listify(cbs) for cbf in listify(cb_funcs): cb = cbf() setattr(self, cb.name, cb) cbs.append(cb) self.stop,self.cbs = False,[TrainEvalCallback()]+cbs @property def opt(self): return self.learn.opt @property def model(self): return self.learn.model @property def loss_func(self): return self.learn.loss_func @property def data(self): return self.learn.data def one_batch(self, xb, yb): try: self.xb,self.yb = xb,yb self(&#39;begin_batch&#39;) self.pred = self.model(self.xb) self(&#39;after_pred&#39;) self.loss = self.loss_func(self.pred, self.yb) self(&#39;after_loss&#39;) if not self.in_train: return self.loss.backward() self(&#39;after_backward&#39;) self.opt.step() self(&#39;after_step&#39;) self.opt.zero_grad() except CancelBatchException: self(&#39;after_cancel_batch&#39;) finally: self(&#39;after_batch&#39;) def all_batches(self, dl): self.iters = len(dl) try: for xb,yb in dl: self.one_batch(xb, yb) except CancelEpochException: self(&#39;after_cancel_epoch&#39;) def fit(self, epochs, learn): self.epochs,self.learn,self.loss = epochs,learn,tensor(0.) try: for cb in self.cbs: cb.set_runner(self) self(&#39;begin_fit&#39;) for epoch in range(epochs): self.epoch = epoch if not self(&#39;begin_epoch&#39;): self.all_batches(self.data.train_dl) with torch.no_grad(): if not self(&#39;begin_validate&#39;): self.all_batches(self.data.valid_dl) self(&#39;after_epoch&#39;) except CancelTrainException: self(&#39;after_cancel_train&#39;) finally: self(&#39;after_fit&#39;) self.learn = None def __call__(self, cb_name): res = False for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) or res return res . . #collapse_show learn = create_learner(get_model, loss_func, data) . . #collapse_show class TestCallback(Callback): _order=1 def after_step(self): print(self.n_iter) if self.n_iter&gt;=10: raise CancelTrainException() . . #collapse run = Runner(cb_funcs=TestCallback) . . #collapse run.fit(3, learn) . . 0 1 2 3 4 5 6 7 8 9 10 . Other callbacks . #collapse_show class AvgStatsCallback(Callback): def __init__(self, metrics): self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False) def begin_epoch(self): self.train_stats.reset() self.valid_stats.reset() def after_loss(self): stats = self.train_stats if self.in_train else self.valid_stats with torch.no_grad(): stats.accumulate(self.run) def after_epoch(self): print(self.train_stats) print(self.valid_stats) class Recorder(Callback): def begin_fit(self): self.lrs = [[] for _ in self.opt.param_groups] self.losses = [] def after_batch(self): if not self.in_train: return for pg,lr in zip(self.opt.param_groups,self.lrs): lr.append(pg[&#39;lr&#39;]) self.losses.append(self.loss.detach().cpu()) def plot_lr (self, pgid=-1): plt.plot(self.lrs[pgid]) def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last]) def plot(self, skip_last=0, pgid=-1): losses = [o.item() for o in self.losses] lrs = self.lrs[pgid] n = len(losses)-skip_last plt.xscale(&#39;log&#39;) plt.plot(lrs[:n], losses[:n]) class ParamScheduler(Callback): _order=1 def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,sched_funcs def begin_fit(self): if not isinstance(self.sched_funcs, (list,tuple)): self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups) def set_param(self): assert len(self.opt.param_groups)==len(self.sched_funcs) for pg,f in zip(self.opt.param_groups,self.sched_funcs): pg[self.pname] = f(self.n_epochs/self.epochs) def begin_batch(self): if self.in_train: self.set_param() . . LR Finder . Exceptions can be used for the lr finder for example, when the right lr is found an exception is called and we can stop ! . NB: You may want to also add something that saves the model before running this, and loads it back after running - otherwise you&#39;ll lose your weights! . Jump_to lesson 10 video . #collapse_show class LR_Find(Callback): _order=1 def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10): self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr self.best_loss = 1e9 def begin_batch(self): if not self.in_train: return pos = self.n_iter/self.max_iter lr = self.min_lr * (self.max_lr/self.min_lr) ** pos for pg in self.opt.param_groups: pg[&#39;lr&#39;] = lr def after_step(self): if self.n_iter&gt;=self.max_iter or self.loss&gt;self.best_loss*10: raise CancelTrainException() if self.loss &lt; self.best_loss: self.best_loss = self.loss . . NB: In fastai we also use exponential smoothing on the loss. For that reason we check for best_loss*3 instead of best_loss*10. . #collapse_show learn = create_learner(get_model, loss_func, data) . . #collapse_show run = Runner(cb_funcs=[LR_Find, Recorder]) . . #collapse_show run.fit(2, learn) . . #collapse_show run.recorder.plot(skip_last=5) . . #collapse_show run.recorder.plot_lr() . .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/09/Lesson-three-part-two.html",
            "relUrl": "/jupyter/2020/04/09/Lesson-three-part-two.html",
            "date": " • Apr 9, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "DL From the Foundations CUDA Hooks . Using Cuda Hooks (Lesson 3 Part 3) . toc:true - badges: true | comments: true | categories: [jupyter] | image: images/cuda.jpg | . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_05b import * torch.set_num_threads(2) . . ConvNet . Jump_to lesson 10 video . #collapse x_train,y_train,x_valid,y_valid = get_data() . . Helper function to quickly normalize with the mean and standard deviation from our training set: . #collapse_show def normalize_to(train, valid): m,s = train.mean(),train.std() return normalize(train, m, s), normalize(valid, m, s) . . #collapse_show x_train,x_valid = normalize_to(x_train,x_valid) train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) . . Let&#39;s check it behaved properly. . #collapse_show x_train.mean(),x_train.std() . . (tensor(3.0614e-05), tensor(1.)) . #collapse_show nh,bs = 50,512 c = y_train.max().item()+1 loss_func = F.cross_entropy data = DataBunch(*get_dls(train_ds, valid_ds, bs), c) . . To refactor layers, it&#39;s useful to have a Lambda layer that can take a basic function and convert it to a layer you can put in nn.Sequential. . NB: if you use a Lambda layer with a lambda function, your model won&#39;t pickle so you won&#39;t be able to save it with PyTorch. So it&#39;s best to give a name to the function you&#39;re using inside your Lambda (like flatten below). . #collapse_show class Lambda(nn.Module): def __init__(self, func): super().__init__() self.func = func def forward(self, x): return self.func(x) def flatten(x): return x.view(x.shape[0], -1) . . This one takes the flat vector of size bs x 784 and puts it back as a batch of images of 28 by 28 pixels: . #collapse_show def mnist_resize(x): return x.view(-1, 1, 28, 28) . . We can now define a simple CNN. With the Lambda class we can now use the resize method in our model directly as well as the flatten method before the fully connected layer. . #collapse_show def get_cnn_model(data): return nn.Sequential( Lambda(mnist_resize), nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), #14 nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 7 nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4 nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2 nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(32,data.c) ) . . #collapse_show model = get_cnn_model(data) . . Basic callbacks from the previous notebook: . #collapse_show cbfs = [Recorder, partial(AvgStatsCallback,accuracy)] . . #collapse_show opt = optim.SGD(model.parameters(), lr=0.4) learn = Learner(model, opt, loss_func, data) run = Runner(cb_funcs=cbfs) . . #collapse_show %time run.fit(1, learn) . . train: [2.06754984375, tensor(0.2895)] valid: [2.14900390625, tensor(0.3016)] CPU times: user 9.03 s, sys: 547 ms, total: 9.58 s Wall time: 3.52 s . CUDA . This took a long time to run, so it&#39;s time to use a GPU. A simple Callback can make sure the model, inputs and targets are all on the same device. . Jump_to lesson 10 video . #collapse_show # Somewhat more flexible way device = torch.device(&#39;cuda&#39;,0) . . #collapse_show class CudaCallback(Callback): def __init__(self,device): self.device=device def begin_fit(self): self.model.to(self.device) def begin_batch(self): self.run.xb,self.run.yb = self.xb.to(self.device),self.yb.to(self.device) . . We can also set the device so we have it as our default : . #collapse_show torch.cuda.set_device(device) . . #collapse_show class CudaCallback(Callback): def begin_fit(self): self.model.cuda() def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda() . . #collapse cbfs.append(CudaCallback) . . #collapse model = get_cnn_model(data) . . #collapse_show opt = optim.SGD(model.parameters(), lr=0.4) learn = Learner(model, opt, loss_func, data) run = Runner(cb_funcs=cbfs) . . #collapse_show %time run.fit(3, learn) . . train: [1.9803146875, tensor(0.2964, device=&#39;cuda:0&#39;)] valid: [0.7705763671875, tensor(0.7300, device=&#39;cuda:0&#39;)] train: [0.4521004296875, tensor(0.8600, device=&#39;cuda:0&#39;)] valid: [0.1957219482421875, tensor(0.9439, device=&#39;cuda:0&#39;)] train: [0.19470791015625, tensor(0.9407, device=&#39;cuda:0&#39;)] valid: [0.1463418701171875, tensor(0.9581, device=&#39;cuda:0&#39;)] CPU times: user 4.12 s, sys: 662 ms, total: 4.78 s Wall time: 5.46 s . Now, that&#39;s definitely faster! . Refactor model . First we can regroup all the conv/relu in a single function. We also set our default values for stride and filter size. . Jump_to lesson 10 video . #collapse_show def conv2d(ni, nf, ks=3, stride=2): return nn.Sequential( nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), nn.ReLU()) . . Another thing is that we can do the mnist resize in a batch transform, that we can do with a Callback. We can pass it a transformation function and store it away. In this case we resize our input and can set it as default with the partial function. . #collapse_show class BatchTransformXCallback(Callback): _order=2 def __init__(self, tfm): self.tfm = tfm def begin_batch(self): self.run.xb = self.tfm(self.xb) def view_tfm(*size): def _inner(x): return x.view(*((-1,)+size)) return _inner . . #collapse_show mnist_view = view_tfm(1,28,28) cbfs.append(partial(BatchTransformXCallback, mnist_view)) . . With the AdaptiveAvgPool, this model can now work on any size input: . #collapse_show nfs = [8,16,32,32] . . We can pass it the above array and configure the network easily that way. . #collapse_show def get_cnn_layers(data, nfs): nfs = [1] + nfs return [ conv2d(nfs[i], nfs[i+1], 5 if i==0 else 3) for i in range(len(nfs)-1) ] + [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)] def get_cnn_model(data, nfs): return nn.Sequential(*get_cnn_layers(data, nfs)) . . And this helper function will quickly give us everything needed to run the training. The kernel size is 5 in the first layer and 3 otherwise. It&#39;s because we want to use a larger filter in order to perform useful computation that reduces the amount of actvations and get useful features that way. . #collapse_show def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy): if opt_func is None: opt_func = optim.SGD opt = opt_func(model.parameters(), lr=lr) learn = Learner(model, opt, loss_func, data) return learn, Runner(cb_funcs=listify(cbs)) . . #collapse model = get_cnn_model(data, nfs) learn,run = get_runner(model, data, lr=0.4, cbs=cbfs) . . #collapse_show model . . Sequential( (0): Sequential( (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)) (1): ReLU() ) (1): Sequential( (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) (2): Sequential( (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) (3): Sequential( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) (4): AdaptiveAvgPool2d(output_size=1) (5): Lambda() (6): Linear(in_features=32, out_features=10, bias=True) ) . #collapse run.fit(3, learn) . . train: [2.3160621875, tensor(0.1493, device=&#39;cuda:0&#39;)] valid: [2.276257421875, tensor(0.1650, device=&#39;cuda:0&#39;)] train: [1.86434125, tensor(0.3412, device=&#39;cuda:0&#39;)] valid: [0.696328564453125, tensor(0.7789, device=&#39;cuda:0&#39;)] train: [0.5320141015625, tensor(0.8298, device=&#39;cuda:0&#39;)] valid: [0.275061376953125, tensor(0.9157, device=&#39;cuda:0&#39;)] . Hooks . Manual insertion . Let&#39;s say we want to do some telemetry, and want the mean and standard deviation of each activations in the model. First we can do it manually like this: . Jump_to lesson 10 video . #collapse_show class SequentialModel(nn.Module): def __init__(self, *layers): super().__init__() self.layers = nn.ModuleList(layers) self.act_means = [[] for _ in layers] self.act_stds = [[] for _ in layers] def __call__(self, x): for i,l in enumerate(self.layers): x = l(x) self.act_means[i].append(x.data.mean()) self.act_stds [i].append(x.data.std ()) return x def __iter__(self): return iter(self.layers) . . #collapse model = SequentialModel(*get_cnn_layers(data, nfs)) learn,run = get_runner(model, data, lr=0.9, cbs=cbfs) . . #collapse run.fit(2, learn) . . train: [1.74529109375, tensor(0.3883, device=&#39;cuda:0&#39;)] valid: [0.45245498046875, tensor(0.8549, device=&#39;cuda:0&#39;)] train: [0.3530631640625, tensor(0.8905, device=&#39;cuda:0&#39;)] valid: [0.138875048828125, tensor(0.9587, device=&#39;cuda:0&#39;)] . Now we can have a look at the means and stds of the activations at the beginning of training. These look awful rn as we do not use any initalization technique so far here. . Means . #collapse_show for l in model.act_means: plt.plot(l) plt.legend(range(6)); . . Std. Deviation . #collapse_show for l in model.act_stds: plt.plot(l) plt.legend(range(6)); . . First 10 means (of first 10 badges) . #collapse_show for l in model.act_means: plt.plot(l[:10]) plt.legend(range(6)); . . First 10 Std. deviations (of first 10 badges) . #collapse_show for l in model.act_stds: plt.plot(l[:10]) plt.legend(range(6)); . . Pytorch hooks . Hooks are PyTorch object you can add to any nn.Module. A hook will be called when a layer, it is registered to, is executed during the forward pass (forward hook) or the backward pass (backward hook). . Hooks don&#39;t require us to rewrite the model. Hooks are essentially PyTorchs Callbacks. . Jump_to lesson 10 video . #collapse model = get_cnn_model(data, nfs) learn,run = get_runner(model, data, lr=0.5, cbs=cbfs) . . #collapse_show act_means = [[] for _ in model] act_stds = [[] for _ in model] . . A hook is attached to a layer, and needs to have a function that takes three arguments: module, input, output. Here we store the mean and std of the output in the correct position of our list. . #collapse_show def append_stats(i, mod, inp, outp): act_means[i].append(outp.data.mean()) act_stds [i].append(outp.data.std()) . . #collapse_show for i,m in enumerate(model): m.register_forward_hook(partial(append_stats, i)) . . #collapse run.fit(1, learn) . . train: [1.57182515625, tensor(0.4757, device=&#39;cuda:0&#39;)] valid: [0.3583569580078125, tensor(0.8997, device=&#39;cuda:0&#39;)] . #collapse_show for o in act_means: plt.plot(o) plt.legend(range(5)); . . Hook class . We can refactor this in a Hook class. It&#39;s very important to remove the hooks when they are deleted, otherwise there will be references kept and the memory won&#39;t be properly released when your model is deleted. . Jump_to lesson 10 video . #collapse_show def children(m): return list(m.children()) class Hook(): def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self)) def remove(self): self.hook.remove() def __del__(self): self.remove() def append_stats(hook, mod, inp, outp): if not hasattr(hook,&#39;stats&#39;): hook.stats = ([],[]) means,stds = hook.stats means.append(outp.data.mean()) stds .append(outp.data.std()) . . NB: In fastai we use a bool param to choose whether to make it a forward or backward hook. In the above version we&#39;re only supporting forward hooks. . #collapse model = get_cnn_model(data, nfs) learn,run = get_runner(model, data, lr=0.5, cbs=cbfs) . . #collapse_show hooks = [Hook(l, append_stats) for l in children(model[:4])] . . #collapse run.fit(1, learn) . . train: [2.12536453125, tensor(0.2445, device=&#39;cuda:0&#39;)] valid: [1.049569140625, tensor(0.6825, device=&#39;cuda:0&#39;)] . #collapse_show for h in hooks: plt.plot(h.stats[0]) h.remove() plt.legend(range(4)); . . A Hooks class . Let&#39;s design our own class that can contain a list of objects. It will behave a bit like a numpy array in the sense that we can index into it via: . a single index | a slice (like 1:5) | a list of indices | a mask of indices ([True,False,False,True,...]) | . The __iter__ method is there to be able to do things like for x in .... . Jump_to lesson 10 video . #collapse_show class ListContainer(): def __init__(self, items): self.items = listify(items) def __getitem__(self, idx): if isinstance(idx, (int,slice)): return self.items[idx] if isinstance(idx[0],bool): assert len(idx)==len(self) # bool mask return [o for m,o in zip(idx,self.items) if m] return [self.items[i] for i in idx] def __len__(self): return len(self.items) def __iter__(self): return iter(self.items) def __setitem__(self, i, o): self.items[i] = o def __delitem__(self, i): del(self.items[i]) def __repr__(self): res = f&#39;{self.__class__.__name__} ({len(self)} items) n{self.items[:10]}&#39; if len(self)&gt;10: res = res[:-1]+ &#39;...]&#39; return res . . #collapse_show ListContainer(range(10)) . . ListContainer (10 items) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . #collapse ListContainer(range(100)) . . ListContainer (100 items) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9...] . #collapse_show t = ListContainer(range(10)) t[[1,2]], t[[False]*8 + [True,False]] . . ([1, 2], [8]) . We can use it to write a Hooks class that contains several hooks. We will also use it in the next notebook as a container for our objects in the data block API. We add &#39;del&#39; medthod so a hook is removed when it is not used anymore to free up memory. . #collapse_show from torch.nn import init class Hooks(ListContainer): def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms]) def __enter__(self, *args): return self def __exit__ (self, *args): self.remove() def __del__(self): self.remove() def __delitem__(self, i): self[i].remove() super().__delitem__(i) def remove(self): for h in self: h.remove() . . #collapse model = get_cnn_model(data, nfs).cuda() learn,run = get_runner(model, data, lr=0.9, cbs=cbfs) . . #collapse hooks = Hooks(model, append_stats) hooks . . Hooks (7 items) [&lt;__main__.Hook object at 0x7f5af1361390&gt;, &lt;__main__.Hook object at 0x7f5af1361780&gt;, &lt;__main__.Hook object at 0x7f5af1361748&gt;, &lt;__main__.Hook object at 0x7f5af1294198&gt;, &lt;__main__.Hook object at 0x7f5af1294e10&gt;, &lt;__main__.Hook object at 0x7f5af1294748&gt;, &lt;__main__.Hook object at 0x7f5af1294b38&gt;] . #collapse hooks.remove() . . #collapse x,y = next(iter(data.train_dl)) x = mnist_resize(x).cuda() . . #collapse x.mean(),x.std() . . (tensor(0.0050, device=&#39;cuda:0&#39;), tensor(1.0050, device=&#39;cuda:0&#39;)) . #collapse p = model[0](x) p.mean(),p.std() . . (tensor(0.1675, device=&#39;cuda:0&#39;, grad_fn=&lt;MeanBackward0&gt;), tensor(0.3281, device=&#39;cuda:0&#39;, grad_fn=&lt;StdBackward0&gt;)) . #collapse for l in model: if isinstance(l, nn.Sequential): init.kaiming_normal_(l[0].weight) l[0].bias.data.zero_() . . #collapse p = model[0](x) p.mean(),p.std() . . (tensor(0.5189, device=&#39;cuda:0&#39;, grad_fn=&lt;MeanBackward0&gt;), tensor(1.1330, device=&#39;cuda:0&#39;, grad_fn=&lt;StdBackward0&gt;)) . Having given an __enter__ and __exit__ method to our Hooks class, we can use it as a context manager. This makes sure that onces we are out of the with block, all the hooks have been removed and aren&#39;t there to pollute our memory. . #collapse_show with Hooks(model, append_stats) as hooks: run.fit(2, learn) fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks: ms,ss = h.stats ax0.plot(ms[:10]) ax1.plot(ss[:10]) plt.legend(range(6)); fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks: ms,ss = h.stats ax0.plot(ms) ax1.plot(ss) plt.legend(range(6)); . . train: [1.9525890625, tensor(0.3193, device=&#39;cuda:0&#39;)] valid: [1.21211796875, tensor(0.6425, device=&#39;cuda:0&#39;)] train: [0.4085144140625, tensor(0.8712, device=&#39;cuda:0&#39;)] valid: [0.17522093505859376, tensor(0.9452, device=&#39;cuda:0&#39;)] . Other statistics . Let&#39;s store more than the means and stds and plot histograms of our activations now. . Jump_to lesson 10 video . #collapse_show def append_stats(hook, mod, inp, outp): if not hasattr(hook,&#39;stats&#39;): hook.stats = ([],[],[]) means,stds,hists = hook.stats means.append(outp.data.mean().cpu()) stds .append(outp.data.std().cpu()) hists.append(outp.data.cpu().histc(40,0,10)) #histc isn&#39;t implemented on the GPU . . #collapse model = get_cnn_model(data, nfs).cuda() learn,run = get_runner(model, data, lr=0.9, cbs=cbfs) . . #collapse_show for l in model: if isinstance(l, nn.Sequential): init.kaiming_normal_(l[0].weight) l[0].bias.data.zero_() . . #collapse_show with Hooks(model, append_stats) as hooks: run.fit(1, learn) . . train: [2.34095421875, tensor(0.1501, device=&#39;cuda:0&#39;)] valid: [2.290508203125, tensor(0.1028, device=&#39;cuda:0&#39;)] . #collapse_show # Thanks to @ste for initial version of histgram plotting code def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p() . . Jump_to lesson 10 video . #collapse_show fig,axes = plt.subplots(2,2, figsize=(15,6)) for ax,h in zip(axes.flatten(), hooks[:4]): ax.imshow(get_hist(h), origin=&#39;lower&#39;) ax.axis(&#39;off&#39;) plt.tight_layout() . . From the histograms, we can easily get more informations like the min or max of the activations . #collapse_show def get_min(h): h1 = torch.stack(h.stats[2]).t().float() return h1[:2].sum(0)/h1.sum(0) . . #collapse_show fig,axes = plt.subplots(2,2, figsize=(15,6)) for ax,h in zip(axes.flatten(), hooks[:4]): ax.plot(get_min(h)) ax.set_ylim(0,1) plt.tight_layout() . . Generalized ReLU . Now let&#39;s use our model with a generalized ReLU that can be shifted and with maximum value. . Jump_to lesson 10 video . #collapse_show def get_cnn_layers(data, nfs, layer, **kwargs): nfs = [1] + nfs return [layer(nfs[i], nfs[i+1], 5 if i==0 else 3, **kwargs) for i in range(len(nfs)-1)] + [ nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)] def conv_layer(ni, nf, ks=3, stride=2, **kwargs): return nn.Sequential( nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), GeneralRelu(**kwargs)) class GeneralRelu(nn.Module): def __init__(self, leak=None, sub=None, maxv=None): super().__init__() self.leak,self.sub,self.maxv = leak,sub,maxv def forward(self, x): x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x) if self.sub is not None: x.sub_(self.sub) if self.maxv is not None: x.clamp_max_(self.maxv) return x def init_cnn(m, uniform=False): f = init.kaiming_uniform_ if uniform else init.kaiming_normal_ for l in m: if isinstance(l, nn.Sequential): f(l[0].weight, a=0.1) l[0].bias.data.zero_() def get_cnn_model(data, nfs, layer, **kwargs): return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs)) . . #collapse_show def append_stats(hook, mod, inp, outp): if not hasattr(hook,&#39;stats&#39;): hook.stats = ([],[],[]) means,stds,hists = hook.stats means.append(outp.data.mean().cpu()) stds .append(outp.data.std().cpu()) hists.append(outp.data.cpu().histc(40,-7,7)) . . #collapse_show model = get_cnn_model(data, nfs, conv_layer, leak=0.1, sub=0.4, maxv=6.) init_cnn(model) learn,run = get_runner(model, data, lr=0.9, cbs=cbfs) . . #collapse_show with Hooks(model, append_stats) as hooks: run.fit(1, learn) fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks: ms,ss,hi = h.stats ax0.plot(ms[:10]) ax1.plot(ss[:10]) h.remove() plt.legend(range(5)); fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks: ms,ss,hi = h.stats ax0.plot(ms) ax1.plot(ss) plt.legend(range(5)); . . train: [0.5544332421875, tensor(0.8234, device=&#39;cuda:0&#39;)] valid: [0.15540943603515625, tensor(0.9536, device=&#39;cuda:0&#39;)] . #collapse_show fig,axes = plt.subplots(2,2, figsize=(15,6)) for ax,h in zip(axes.flatten(), hooks[:4]): ax.imshow(get_hist(h), origin=&#39;lower&#39;) ax.axis(&#39;off&#39;) plt.tight_layout() . . #collapse_show def get_min(h): h1 = torch.stack(h.stats[2]).t().float() return h1[19:22].sum(0)/h1.sum(0) . . #collapse_show fig,axes = plt.subplots(2,2, figsize=(15,6)) for ax,h in zip(axes.flatten(), hooks[:4]): ax.plot(get_min(h)) ax.set_ylim(0,1) plt.tight_layout() . . Jump_to lesson 10 video . #collapse_show def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs): model = get_cnn_model(data, nfs, layer, **kwargs) init_cnn(model, uniform=uniform) return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func) . . #collapse_show sched = combine_scheds([0.5, 0.5], [sched_cos(0.2, 1.), sched_cos(1., 0.1)]) . . #collapse_show learn,run = get_learn_run(nfs, data, 1., conv_layer, cbs=cbfs+[partial(ParamScheduler,&#39;lr&#39;, sched)]) . . #collapse run.fit(8, learn) . . train: [1.31843953125, tensor(0.5775, device=&#39;cuda:0&#39;)] valid: [0.634657666015625, tensor(0.7989, device=&#39;cuda:0&#39;)] train: [0.3491707421875, tensor(0.8969, device=&#39;cuda:0&#39;)] valid: [0.718830908203125, tensor(0.7760, device=&#39;cuda:0&#39;)] train: [0.22474275390625, tensor(0.9335, device=&#39;cuda:0&#39;)] valid: [0.139433447265625, tensor(0.9587, device=&#39;cuda:0&#39;)] train: [0.24052806640625, tensor(0.9311, device=&#39;cuda:0&#39;)] valid: [0.0933504638671875, tensor(0.9738, device=&#39;cuda:0&#39;)] train: [0.087468662109375, tensor(0.9731, device=&#39;cuda:0&#39;)] valid: [0.07542120361328125, tensor(0.9769, device=&#39;cuda:0&#39;)] train: [0.05863072265625, tensor(0.9822, device=&#39;cuda:0&#39;)] valid: [0.06209371337890625, tensor(0.9817, device=&#39;cuda:0&#39;)] train: [0.0425904248046875, tensor(0.9867, device=&#39;cuda:0&#39;)] valid: [0.05970958251953125, tensor(0.9841, device=&#39;cuda:0&#39;)] train: [0.03503335205078125, tensor(0.9896, device=&#39;cuda:0&#39;)] valid: [0.05823819580078125, tensor(0.9834, device=&#39;cuda:0&#39;)] . Uniform init may provide more useful initial weights (normal distribution puts a lot of them at 0). . #collapse learn,run = get_learn_run(nfs, data, 1., conv_layer, uniform=True, cbs=cbfs+[partial(ParamScheduler,&#39;lr&#39;, sched)]) . . #collapse run.fit(8, learn) . . train: [1.212601953125, tensor(0.6339, device=&#39;cuda:0&#39;)] valid: [0.4008196044921875, tensor(0.8711, device=&#39;cuda:0&#39;)] train: [0.4163723046875, tensor(0.8766, device=&#39;cuda:0&#39;)] valid: [0.9500861328125, tensor(0.7101, device=&#39;cuda:0&#39;)] train: [0.31883013671875, tensor(0.9031, device=&#39;cuda:0&#39;)] valid: [0.14645821533203124, tensor(0.9561, device=&#39;cuda:0&#39;)] train: [0.136221357421875, tensor(0.9582, device=&#39;cuda:0&#39;)] valid: [0.09891201171875, tensor(0.9717, device=&#39;cuda:0&#39;)] train: [0.0801023388671875, tensor(0.9752, device=&#39;cuda:0&#39;)] valid: [0.07433181762695312, tensor(0.9799, device=&#39;cuda:0&#39;)] train: [0.0592908984375, tensor(0.9819, device=&#39;cuda:0&#39;)] valid: [0.06670298461914062, tensor(0.9803, device=&#39;cuda:0&#39;)] train: [0.0450291015625, tensor(0.9864, device=&#39;cuda:0&#39;)] valid: [0.0643135009765625, tensor(0.9821, device=&#39;cuda:0&#39;)] train: [0.03759386962890625, tensor(0.9889, device=&#39;cuda:0&#39;)] valid: [0.06419009399414062, tensor(0.9820, device=&#39;cuda:0&#39;)] .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/2020/04/09/Lesson-three-part-three.html",
            "relUrl": "/2020/04/09/Lesson-three-part-three.html",
            "date": " • Apr 9, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "DL from the Foundations Callbacks and __dunder__",
            "content": "Deep Learning from the Foundations Lesson 3 Part1 . . In the beginning we recap some lesson 2 concepts (Callbacks, Variants, under special methods) : . Just our imports : . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . #collapse import torch import matplotlib.pyplot as plt . . Jump_to opening comments and overview of lesson 10 . Callbacks . Callbacks as GUI events . Jump_to lesson 10 video Imports : . #collapse import ipywidgets as widgets . . #collapse_show def f(o): print(&#39;hi&#39;) . . From the ipywidget docs: . the button widget is used to handle mouse clicks. The on_click method of the Button can be used to register function to be called when the button is clicked | . #collapse_show w = widgets.Button(description=&#39;Click me&#39;) . . #collapse w . . Now that we created this button we can pass it a function that will execute when pushing the button. It&#39;s a callback using a function pointer ! . #collapse_show w.on_click(f) . . NB: When callbacks are used in this way they are often called &quot;events&quot;. . Did you know what you can create interactive apps in Jupyter with these widgets? Here&#39;s an example from plotly: . . Creating your own callback . Jump_to lesson 10 video . #collapse from time import sleep . . We create a dummy calculation function to show this concept : . #collapse_show def slow_calculation(cb=None): res = 0 for i in range(5): res += i*i sleep(1) if cb: cb(i) return res . . #collapse_show def show_progress(epoch): print(f&quot;Awesome! We&#39;ve finished epoch {epoch}!&quot;) . . We can now use this show_progress fct to use it as a callback ! . #collapse_show slow_calculation(show_progress) . . Awesome! We&#39;ve finished epoch 0! Awesome! We&#39;ve finished epoch 1! Awesome! We&#39;ve finished epoch 2! Awesome! We&#39;ve finished epoch 3! Awesome! We&#39;ve finished epoch 4! . 30 . Lambdas and partials . Jump_to lesson 10 video . We can also define the function with Lambda and use it right away ! . #collapse_show slow_calculation(lambda o: print(f&quot;Awesome! We&#39;ve finished epoch {o}!&quot;)) . . Awesome! We&#39;ve finished epoch 0! Awesome! We&#39;ve finished epoch 1! Awesome! We&#39;ve finished epoch 2! Awesome! We&#39;ve finished epoch 3! Awesome! We&#39;ve finished epoch 4! . 30 . #collapse_show def show_progress(exclamation, epoch): print(f&quot;{exclamation}! We&#39;ve finished epoch {epoch}!&quot;) . . The above function can not be passed, because it uses 2 arguments. So we use Lambda to fix this : . #collapse_show slow_calculation(lambda o: show_progress(&quot;OK I guess&quot;, o)) . . OK I guess! We&#39;ve finished epoch 0! OK I guess! We&#39;ve finished epoch 1! OK I guess! We&#39;ve finished epoch 2! OK I guess! We&#39;ve finished epoch 3! OK I guess! We&#39;ve finished epoch 4! . 30 . It&#39;s better to do it like this, where we pass the function with the exclamation : . #collapse_show def make_show_progress(exclamation): # Leading &quot;_&quot; is generally understood to be &quot;private&quot; def _inner(epoch): print(f&quot;{exclamation}! We&#39;ve finished epoch {epoch}!&quot;) return _inner . . #collapse_show slow_calculation(make_show_progress(&quot;Nice!&quot;)) . . Nice!! We&#39;ve finished epoch 0! Nice!! We&#39;ve finished epoch 1! Nice!! We&#39;ve finished epoch 2! Nice!! We&#39;ve finished epoch 3! Nice!! We&#39;ve finished epoch 4! . 30 . Obviously we can also do it like this with f2 containing the closure : . #collapse_show f2 = make_show_progress(&quot;Terrific&quot;) . . #collapse slow_calculation(f2) . . Terrific! We&#39;ve finished epoch 0! Terrific! We&#39;ve finished epoch 1! Terrific! We&#39;ve finished epoch 2! Terrific! We&#39;ve finished epoch 3! Terrific! We&#39;ve finished epoch 4! . 30 . #collapse slow_calculation(make_show_progress(&quot;Amazing&quot;)) . . Amazing! We&#39;ve finished epoch 0! Amazing! We&#39;ve finished epoch 1! Amazing! We&#39;ve finished epoch 2! Amazing! We&#39;ve finished epoch 3! Amazing! We&#39;ve finished epoch 4! . 30 . #collapse from functools import partial . . We can also use partial to use the argmuent passed always as given. This means we can only pass the epoch argument now. We can also store it in f2 again like before. . #collapse_show slow_calculation(partial(show_progress, &quot;OK I guess&quot;)) . . OK I guess! We&#39;ve finished epoch 0! OK I guess! We&#39;ve finished epoch 1! OK I guess! We&#39;ve finished epoch 2! OK I guess! We&#39;ve finished epoch 3! OK I guess! We&#39;ve finished epoch 4! . 30 . #collapse f2 = partial(show_progress, &quot;OK I guess&quot;) . . Callbacks as callable classes . Jump_to lesson 10 video . Most of the time we want to use a class, so we store our exclamation and epoch args in class attributes : . #collapse_show class ProgressShowingCallback(): def __init__(self, exclamation=&quot;Awesome&quot;): self.exclamation = exclamation def __call__(self, epoch): print(f&quot;{self.exclamation}! We&#39;ve finished epoch {epoch}!&quot;) . . #collapse_show cb = ProgressShowingCallback(&quot;Just super&quot;) . . call is a magic name, will be called when you take an object and treat it like a function : . #collapse_show cb(&quot;hi&quot;) . . Just super! We&#39;ve finished epoch hi! . #collapse slow_calculation(cb) . . Just super! We&#39;ve finished epoch 0! Just super! We&#39;ve finished epoch 1! Just super! We&#39;ve finished epoch 2! Just super! We&#39;ve finished epoch 3! Just super! We&#39;ve finished epoch 4! . 30 . Multiple callback funcs; *args and **kwargs . Jump_to lesson 10 video . All the things that are positional arguments end up in a tuple (args) and all the keyword arguments (kwargs) are stored as a dict. This is used to wrap other classes/objects, **kwargs can be passed off to the subclasses for example. . #collapse_show def f(*args, **kwargs): print(f&quot;args: {args}; kwargs: {kwargs}&quot;) . . #collapse_show f(3, &#39;a&#39;, thing1=&quot;hello&quot;) . . args: (3, &#39;a&#39;); kwargs: {&#39;thing1&#39;: &#39;hello&#39;} . NB: We&#39;ve been guilty of over-using kwargs in fastai - it&#39;s very convenient for the developer, but is annoying for the end-user unless care is taken to ensure docs show all kwargs too. kwargs can also hide bugs (because it might not tell you about a typo in a param name). In R there&#39;s a very similar issue (R uses ... for the same thing), and matplotlib uses kwargs a lot too. . Let&#39;s go back to our function from the start, adding a callback before and after the calculation. This is a good use for args and kwargs, due to it&#39;s flexibility : . #collapse_show def slow_calculation(cb=None): res = 0 for i in range(5): if cb: cb.before_calc(i) res += i*i sleep(1) if cb: cb.after_calc(i, val=res) return res . . #collapse_show class PrintStepCallback(): def __init__(self): pass #when removing args and kwargs here it won&#39;t work def before_calc(self, *args, **kwargs): print(f&quot;About to start&quot;) def after_calc (self, *args, **kwargs): print(f&quot;Done step&quot;) . . #collapse_show slow_calculation(PrintStepCallback()) . . About to start Done step About to start Done step About to start Done step About to start Done step About to start Done step . 30 . We can now use this with epoch and val to print our details : . #collapse_show class PrintStatusCallback(): def __init__(self): pass def before_calc(self, epoch, **kwargs): print(f&quot;About to start: {epoch}&quot;) def after_calc (self, epoch, val, **kwargs): print(f&quot;After {epoch}: {val}&quot;) . . #collapse_show slow_calculation(PrintStatusCallback()) . . About to start: 0 After 0: 0 About to start: 1 After 1: 1 About to start: 2 After 2: 5 About to start: 3 After 3: 14 About to start: 4 After 4: 30 . 30 . Modifying behavior . Jump_to lesson 10 video . We can now use this to implement early stopping. . #collapse_show def slow_calculation(cb=None): res = 0 for i in range(5): if cb and hasattr(cb,&#39;before_calc&#39;): cb.before_calc(i) res += i*i sleep(1) if cb and hasattr(cb,&#39;after_calc&#39;): if cb.after_calc(i, res): print(&quot;stopping early&quot;) break return res . . #collapse_show class PrintAfterCallback(): def after_calc (self, epoch, val): print(f&quot;After {epoch}: {val}&quot;) if val&gt;10: return True . . #collapse_show slow_calculation(PrintAfterCallback()) . . After 0: 0 After 1: 1 After 2: 5 After 3: 14 stopping early . 14 . We can now use implement a class, this will allow our callback to modify the values inside the class. . #collapse_show class SlowCalculator(): def __init__(self, cb=None): self.cb,self.res = cb,0 def callback(self, cb_name, *args): if not self.cb: return cb = getattr(self.cb,cb_name, None) if cb: return cb(self, *args) def calc(self): for i in range(5): self.callback(&#39;before_calc&#39;, i) self.res += i*i sleep(1) if self.callback(&#39;after_calc&#39;, i): print(&quot;stopping early&quot;) break . . Using dunder method __call__ we can do it like this : . #collapse_show class SlowCalculator(): def __init__(self, cb=None): self.cb,self.res = cb,0 def __call__(self, cb_name, *args): if not self.cb: return cb = getattr(self.cb,cb_name, None) if cb: return cb(self, *args) def calc(self): for i in range(5): self.callback(&#39;before_calc&#39;, i) self.res += i*i sleep(1) if self(&#39;after_calc&#39;, i): print(&quot;stopping early&quot;) break . . #collapse_show class ModifyingCallback(): def after_calc (self, calc, epoch): print(f&quot;After {epoch}: {calc.res}&quot;) if calc.res&gt;10: return True if calc.res&lt;3: calc.res = calc.res*2 . . #collapse_show calculator = SlowCalculator(ModifyingCallback()) calculator.calc() calculator.res . . After 0: 0 After 1: 1 After 2: 6 After 3: 15 stopping early . 15 . __dunder__ thingies . Anything that looks like __this__ is, in some way, special. Python, or some library, can define some functions that they will call at certain documented times. For instance, when your class is setting up a new object, python will call __init__. These are defined as part of the python data model. . For instance, if python sees +, then it will call the special method __add__. If you try to display an object in Jupyter (or lots of other places in Python) it will call __repr__. . Jump_to lesson 10 video . #collapse_show class SloppyAdder(): def __init__(self,o): self.o=o def __add__(self,b): return SloppyAdder(self.o + b.o + 0.01) def __repr__(self): return str(self.o) . . #collapse_show a = SloppyAdder(1) b = SloppyAdder(2) a+b . . 3.01 . Special methods you should probably know about (see data model link above) are: . __getitem__ | __getattr__ | __setattr__ | __del__ | __init__ | __new__ | __enter__ | __exit__ | __len__ | __repr__ | __str__ | . Variance and stuff . Variance . Variance is the average of how far away each data point is from the mean. E.g.: . Jump_to lesson 10 video . #collapse t = torch.tensor([1.,2.,4.,18]) . . #collapse m = t.mean(); m . . tensor(6.2500) . #collapse (t-m).mean() . . tensor(0.) . Oops. We can&#39;t do that. Because by definition the positives and negatives cancel out. So we can fix that in one of (at least) two ways: . #collapse (t-m).pow(2).mean() . . tensor(47.1875) . #collapse (t-m).abs().mean() . . tensor(5.8750) . But the first of these is now a totally different scale, since we squared. So let&#39;s undo that at the end. . #collapse (t-m).pow(2).mean().sqrt() . . tensor(6.8693) . They&#39;re still different. Why? . Note that we have one outlier (18). In the version where we square everything, it makes that much bigger than everything else. . (t-m).pow(2).mean() is refered to as variance. It&#39;s a measure of how spread out the data is, and is particularly sensitive to outliers. . When we take the sqrt of the variance, we get the standard deviation. Since it&#39;s on the same kind of scale as the original data, it&#39;s generally more interpretable. However, since sqrt(1)==1, it doesn&#39;t much matter which we use when talking about unit variance for initializing neural nets. . (t-m).abs().mean() is referred to as the mean absolute deviation. It isn&#39;t used nearly as much as it deserves to be, because mathematicians don&#39;t like how awkward it is to work with. But that shouldn&#39;t stop us, because we have computers and stuff. . Here&#39;s a useful thing to note about variance: . #collapse_show (t-m).pow(2).mean(), (t*t).mean() - (m*m) . . (tensor(47.1875), tensor(47.1875)) . You can see why these are equal if you want to work thru the algebra. Or not. . But, what&#39;s important here is that the latter is generally much easier to work with. In particular, you only have to track two things: the sum of the data, and the sum of squares of the data. Whereas in the first form you actually have to go thru all the data twice (once to calculate the mean, once to calculate the differences). . Let&#39;s go steal the LaTeX from Wikipedia: . $$ operatorname{E} left[X^2 right] - operatorname{E}[X]^2$$ . Covariance and correlation . Here&#39;s how Wikipedia defines covariance: . $$ operatorname{cov}(X,Y) = operatorname{E}{ big[(X - operatorname{E}[X])(Y - operatorname{E}[Y]) big]}$$ . Jump_to lesson 10 video . #collapse t . . tensor([ 1., 2., 4., 18.]) . Let&#39;s see that in code. So now we need two vectors. . #collapse # `u` is twice `t`, plus a bit of randomness u = t*2 u *= torch.randn_like(t)/10+0.95 plt.scatter(t, u); . . #collapse prod = (t-t.mean())*(u-u.mean()); prod . . tensor([ 55.9552, 41.3348, 9.6900, 290.1150]) . #collapse prod.mean() . . tensor(99.2737) . #collapse v = torch.randn_like(t) plt.scatter(t, v); . . #collapse ((t-t.mean())*(v-v.mean())).mean() . . tensor(1.0830) . It&#39;s generally more conveniently defined like so: . $$ operatorname{E} left[X Y right] - operatorname{E} left[X right] operatorname{E} left[Y right]$$ . #collapse cov = (t*v).mean() - t.mean()*v.mean(); cov . . tensor(1.0830) . From now on, you&#39;re not allowed to look at an equation (or especially type it in LaTeX) without also typing it in Python and actually calculating some values. Ideally, you should also plot some values. . Finally, here is the Pearson correlation coefficient: . $$ rho_{X,Y}= frac{ operatorname{cov}(X,Y)}{ sigma_X sigma_Y}$$ . #collapse_show cov / (t.std() * v.std()) . . tensor(0.1165) . It&#39;s just a scaled version of the same thing. Question: Why is it scaled by standard deviation, and not by variance or mean or something else? . Softmax . Here&#39;s our final logsoftmax definition: . Jump_to lesson 10 video . #collapse_show def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log() . . which is: . $$ hbox{logsoftmax(x)}_{i} = x_{i} - log sum_{j} e^{x_{j}}$$ . And our cross entropy loss is: $$- log(p_{i})$$ . Softmax is only a good idea, when our data (e.g. image) has only one (at least one) example, due to the highest output being far higher in Softmax due to the e function. In these cases binomial : $e^{x}/(1+e^{x})$ . Browsing source code . Jump_to lesson 10 video . Jump to tag/symbol by with (with completions) | Jump to current tag | Jump to library tags | Go back | Search | Outlining / folding | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/07/Callbacks-dunder.html",
            "relUrl": "/jupyter/2020/04/07/Callbacks-dunder.html",
            "date": " • Apr 7, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Delving Deep into Rectifiers a summary",
            "content": "Delving Deep into Rectifiers: . Surpassing Human-Level Performance on ImageNet Classification a summary of the paper . What did the authors want to achieve ? . Achieve above human level performance on vision (ImageNet) | Train deep Networks with rectifier functions (ReLu,LReLu,PReLU) effectively | Improve accuracy by enabling the training of deeper and larger networks | . Key elements . The key elements are a different kind of rectifier activation function called PReLu, which is very similar to LReLu as well as a different initalization technique called Kaiming/He init which improves upon the fact that Xavier initalization does not consider the non-linearities of ReLu kind functions | . PReLU . . As we can see PReLu looks a lot like LReLu, having a negative slope a when x &lt; 0, however this slope is not fixed in the beginning but learned by introducing a few hyperparameters | Due to the number of extra hyperparams being equal to the number of channels, no additional risk of overfitting is introduced | PReLu seems to keep more information in early layers and becomes more discriminative in deeper stages due to being able to model more non-linear functions | . Kaiming Initalization . The problem with Xavier init, is that it does not take into account the non-linearities of rectifier funcions, therefore a new init technique is derived by taking these activation functions into account, for the forward pass the following is derived : . Based on the response of a conv layer, which is computed by : $y_{l} = W_{l}*x_{l}+b_{l}$ ,with $x$ being a $ n = k^{2}*c$ vector ($k*k$ co-located pixels, in $c$ channels) and $W_{l}$ being a $d$ x $n$ matrix, where $d$ is the num of filters . | The elements in $W_{l}$ and $x_{l}$ are assumed to be independent from each other and share the same distribution, $W_{l}$ and $x_{l}$ are also independet from each other it follows : $Var[y_{l}] = n_{l} *Var[w_{l}*x_{l}] $ . | We let $w_{l}$ have zero mean, the variance of the product of independent variables gives us : $Var[y_{l}] = n_{l} *Var[w_{l}]*Var[x_{l}] $ , which leads to $Var[y_{l}] = n_{l} *Var[w_{l}]*E[x_{l}^{2}] $ . | $E[x_{l}^{2}]$ is the expectation of the square of $x_{l}$, we notice that $E[x_{l}^{2}] neq Var[x_{l}]$ unless $x_{l}$ has 0 mean (Random variability) , which is not the case for ReLu : $x_{l} = max(0,y_{l-1})$ . | if $w_{l-1}$ is symmetric around 0 and $b_{l-1}=0$, it follows that $y_{l-1}$ is a symmetric distribution around zero. This means that $E[x_{l}^{2}]=0.5 * Var[y_{l-1}]$ when the activation is ReLu thus : $Var[y_{l}] = 0.5 * n_{l} *Var[w_{l}]*Var[y_{l-1}] $ . | when we have L layers we have : . $Var[y_{l}] = Var[y_{1}] * prod^{L}_{l=2} (0.5 * n_{l} *Var[w_{l}])$ . | the initalization should not magnify the magnitude of the inputs signals, this is achieved by applying a proper scalar : . | . $0.5 * n_{l} *Var[w_{l}] = 1, forall {l}$ (ReLu case) . $0.5 *(1+a^{2}) * n_{l} *Var[w_{l}] = 1, forall {l}$ (PReLu case) . | . =&gt; this distribution is a 0-mean Gaussian with a std of $ sqrt{2/n_{l}}$, which is also adopted in the first layer . For the backward pass the same function applies, with $n_{l}=k_{l}^{2}*d_{l-1} = k_{l}^{2}*c_{l}$ replaced by $ tilde{n}=k_{l}^{2}*d_{l}$ : . $0.5 * tilde{n} *Var[w_{l}] = 1, forall {l}$ (ReLu) . $0.5 *(1+a^{2}) * tilde{n} *Var[w_{l}] = 1, forall {l}$ (PReLu case) . | . &quot;This means that if the initialization properly scales the backward signal, then this is also the case for the forward signal; and vice versa. For all models in this paper, both forms can make them converge.&quot; . Implementation Details . The standard hyperparms are as follows : Weight decay is 0.0005 | Momentum is 0.9. | Dropout (50%) is used in the first two fc layers | Minibatch size is fixed as 128 | The learning rates are 1e-2, 1e-3,and 1e-4, and is switched when the error plateaus | Number of epochs : 80 | simple variant of Krizhevsky’s method is used to run Multi-GPUs, the GPUs are synched before the first fc layer to run backprop/forward pass on one of the GPUs (3.8x speedup using 4 GPUs, and a 6.0x speedup using 8 GPUs) | . | The PReLU hyperparameters (slopes) are trained with Backprop, the authors proposed the following : no weight decay is used | the slopes ai are initialized as 0.25 | the slopes aiare not constrained, even without regularization aiis rarely larger than 1 | . | . Results and Conclusion . . PRelu reduces top-1 error by 1.05% and top-5 error by 0.23% (@scale 384), when the large model A is used | Kaiming init allows training deep rectifier networks and converges, this allows them to reduce the error to below human level 4.94% compared to 5.1%, you should check out how the human benchmark was established by checking out Andrej Karpathy&#39;s blog on this | It has to be noted however that this is largely due to the fine grained details that can be learned by NNs, if a prediction is incorrect humans still mostly guess the right category (for example vehicle) while NNs can be completely off. So superhuman performance is only achieved in detecting fine grained classes. This can be confirmed when training on the Pascal VOC dataset. | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/05/Delving-Deep-into-rectifiers.html",
            "relUrl": "/jupyter/2020/04/05/Delving-Deep-into-rectifiers.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Dense Net",
            "content": "Dense Net paper summary . . What did they try to accomplish ? . improve CNNs : fight the vanishing gradient problem, improve regularization, remove redundancy (redundant layers/neurons) existing in current CNNs (like ResNets), which in turn recudes the number of parameters | . Key elements . Concat and Dense conectivity . concatenating feature maps, instead of using the classic ResNet skip connection function : . . . =&gt; lth layer is used as input to (l+1)th layer =&gt; xl = Hl(xl-1) . Dense Nets concatenate feature maps of the same size, which means it has L*(L+1)/(2) connections instead of L in a normal network, where L is the number of layers. Consequently every Dense Net layer has access to the feature maps of the preceeding layers : | . . The activation function Hl is a composite function with 3x3 convolutions, Batch Norm and ReLu activations. . Pooling /Transition Layers . When the size of feature maps changes, concatenation is not viable. The network is divided into several Dense Blocks, in between those 2x2 average pooling with 1x1 conv filters and batch norm are applied forming a “transition layer”. . Growth rate . The growth rate k is a hyperparameter which regulates how much a layer contributes to the global state. If each composite function Hl produces k feature maps, the lth layer has k0 + k * (l-1) feature-maps, where k0 is the number of channels in the input layer. It has to be noted that DenseNets use narrow layers, with k=12. . Bottleneck layers . To reduce the amount of input channels (for compute efficiency), bottleneck layers are used with 1x1 convs before the 3x3 convs applied. . Compression . Compression is used to reduce the number of feature maps at transition layers, if a dense block contains m feature maps, the transition layer will generate a*m feature maps,where 0 &lt; a &lt;= 1 with a = 0.5 in most cases. . Implementation Details . Kaiming/He init. is used | Zero padding is used @ each Dense block | Global pooling after last Dense block, with Softmax activation | 3 Dense blocks are used with all datasets except for ImageNet | Weight decay of 10e-4 | Nesterov momentum of 0.9 | ImageNet implementation uses 7x7 convs instead of 3x3 | . Results and Conclusion . . Bottleneck impact decreases with depth of the network | not the same regularization issues as with ResNets1 | Dense Net BC with 15.3 Million params outperforms much larger Fractal Net (comparable to ResNet-1001), with DenseNet having 90% fewer parameters | a DenseNet with as much compute complexity (FLOPS) as ResNet-50 performs on par with ResNet-101 | DenseNet with 0.8 Million parameters performs as good as ResNet with 10.2 Millon parameters | Deep Supervision is achieved with a single classifier. This provides easier loss functions and doesn’t need a multi classifier (like Inception). | The intuition behind the good performance of DenseNets : architecture style is similar to a ResNet trained with stochastic depth, that means redundant layers are dropped from the beginning allowing smaller Networks | . References that are interesting to follow . DenseNets Implentation Github | ResNets paper | Fractal Nets paper | Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/04/Dense-Net.html",
            "relUrl": "/markdown/2020/04/04/Dense-Net.html",
            "date": " • Apr 4, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastai DL from the Foundations Softmax, NLL, Training, Module, Dataloader, Sampler, Callbacks",
            "content": "Lesson 2 How to train your model . Loss . For our Classification task, we will be using Cross Entropy Loss (also called Log Loss) ourselves. We define a simple Linear Model for our task. We will be using the Pytorch functions, as we already implemented those in Lesson 1 : . class Model(nn.Module): def __init__(self,in_dim,nh,out_dim): super().__init__() self.layers = [nn.Linear(in_dim,nh), nn.ReLu(),nn.Linear(nh,out_dim)] def __call__(self,x): for i in self.layers: x = l(x) return x . Since we are using Softmax, we need to compute it’s output first : . In practice we need it’s log, the code is simple : . def log_softmax(x) : return (x.exp()/x.exp().sum(-1,keepdim=True))).log() . Using simple log-math . log(a/b) = log(a) - log(b) . Which leads to in pseudo code : . log(x.exp()) - log(x.exp().sum()) . We can simplify this log_softmax function, like so : . def log_softmax(x) : return x - x.exp().sum(-1,keepdim=True).log() . . Using numpy integer array indexing we can compute our negative log likelihood like so by passing our softmax output : . def negative_log_likelihood(input,target): return -input[range(target.shape[0]), target].mean() . However we can compute the log of the sum of exponentials in a more stable way to avoid an overflow of big activations, with the LogSumExp trick : . def logsumexp(exp): a= x.max(-1)[0] #maximum of x(j) return a + (x-a[:,None]).exp().sum(-1).log() . Updating our softmax again, this leads to : . def log_softmax(x): return x - x.logsumexp(-1,keepdim=True) . x.logsumexp() is the Pytorch function in this case. In order to compare our function with Pytorch, we can use . test_near(logsumexp(pred), pred.logsumexp(-1)) . test_near will throw an AssertionError if they are not equal to each other. . Now we succesfully implemented F.cross_entropy(pred,y_train), which is made out of F.log_softmax and F.nll_loss . The accuracy can be calculated with : . def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean() . Training Loop . Basically the training loop repeats over the following steps: . get the output of the model on a batch of inputs | compare the output to the labels we have and compute a loss | calculate the gradients of the loss with respect to every parameter of the model | update said parameters with those gradients to make them a little bit better | . Now we can implement our Training Loop : . for epoch in range(epochs): for i in range((n-1)//bs + 1): # slice dataset in batches start_i = i*bs end_i = start_i+bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] loss = loss_func(model(xb), yb) loss.backward() with torch.no_grad(): for l in model.layers: if hasattr(l, &#39;weight&#39;): l.weight -= l.weight.grad * lr l.bias -= l.bias.grad * lr l.weight.grad.zero_() l.bias .grad.zero_() . This looks kind of messy. Since our parameters can all be stored in a model class, we can loop over them and update them easily. However we need to implement a dummy Module first : . Module and improved training loop . class DummyModule(): def __init__(self, n_in, nh, n_out): self._modules = {} self.l1 = nn.Linear(n_in,nh) self.l2 = nn.Linear(nh,n_out) def __setattr__(self,k,v): #this is called everytime self is assigned if not k.startswith(&quot;_&quot;): self._modules[k] = v # just checks if it doesn&#39;t start with _ to avoid calling python _modules recursively super().__setattr__(k,v) #super class is python object def __repr__(self): return f&#39;{self._modules}&#39; def parameters(self): for l in self._modules.values(): for p in l.parameters(): yield p . for simplicity we can now use the Pytorch Module . class Model(nn.Module): def __init__(self,layers): super().__init__() #initalizes self._modules self.layers = layers for i,l in enumerate(self.layers) : self.add_module(f&#39;layer_{i}&#39;,l) def __call__(): for layer in self.layers: x = l(x) return x . now we can call the training more conveniently : . for epoch in range(epochs): for i in range((n-1)//bs + 1): # slice dataset in batches start_i = i*bs end_i = start_i+bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] loss = loss_func(model(xb), yb) loss.backward() with torch.no_grad() for param in model.parameters(): param -= lr * param.grad . We can make it even easier with nn.ModuleList to recreate nn.Sequential. . class SequentialModel(nn.Module): def __init__(self, layers): super().__init__() self.layers = nn.ModuleList(layers) def __call__(self, x): for l in self.layers : x = l(x) return x . Let’s replace our previous manually coded optimization step: . with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() . and instead use just: . opt.step() opt.zero_grad() . Optimizer . By creating our own Optimizer Function . class Optimizer(): def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr def step(self): with torch.no_grad(): for p in self.params: p -= p.grad * lr def zero_grad(self): for p in self.params: p.grad.data.zero_() . PyTorch already provides optimizers, like optim.SGD and optim.Adam, which also handles more stuff. . Now we can further simplify our Training loop : . for epoch in range(epochs): for i in range((n-1)//bs + 1): start_i = i*bs end_i = start_i+bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() . When implementing stuff yourself, it’s always good to put some tests in. Like checking the accuracy for example. . Dataset and DataLoader . Dataset . We can further simplify this by converting . xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] . with a Dataset Class : . class Dataset(): def __init__(self,x,y): self.x,self.y = x,y def __len__(self): return len(self.x) def __getitem__(self,i): return self.x[i], self.y[i] train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) . Now we can call our loop like this : . for e in range(epochs): for i in range((n-1)//bs+1): x,y = train_ds[i*bs:i*bs+bs] pred = model(x) loss = loss_func(pred,y) loss.backward() opt.step() opt.zero_grad() . DataLoader . We can make this even easier with a Dataloader to iterate over our Dataset automatically. . class Dataloader : def __init__(self, ds, bs): self.ds,self.bs = ds,bs def __iter__(self): for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs] . yield is used, in order to produce a series of values over time. We can use the Dataloader with next and iter : . train_dl = DataLoader(train_ds, bs) valid_dl = DataLoader(valid_ds, bs) xb,yb = next(iter(valid_dl)) . now we can put our train loop in a wonderful function : . def fit(): for epoch in range(epochs): for xb,yb in train_dl: pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() . Sampler . In order to have a random order, we can implement a Sampler class : . class Sampler(): def __init__(self, ds, bs, shuffle=False): self.n,self.bs,self.shuffle = len(ds),bs,shuffle def __iter__(self): self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n) for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs] . Now we can change our DataLoader to include our sampler as an argument. . def collate(b): xs,ys = zip(*b) return torch.stack(xs),torch.stack(ys) class DataLoader(): def __init__(self, ds, sampler, collate_fn=collate): self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn def __iter__(self): for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s]) . collate stacks tensors, we can also add padding, etc … PyTorch does the same thing as well, but also adds num_workers which can be used to start several threads and run more efficiently : . train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate,num_workers=num_workers) valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate,num_workers=num_workers) . With Val Set . Using best practices we should add a val set to store our best model and test during training : . def fit(epochs, model, loss_func, opt, train_dl, valid_dl): for epoch in range(epochs): # Handle batchnorm / dropout model.train() # print(model.training) for xb,yb in train_dl: loss = loss_func(model(xb), yb) loss.backward() opt.step() opt.zero_grad() model.eval() # print(model.training) with torch.no_grad(): tot_loss,tot_acc = 0.,0. for xb,yb in valid_dl: pred = model(xb) tot_loss += loss_func(pred, yb) tot_acc += accuracy (pred,yb) nv = len(valid_dl) print(epoch, tot_loss/nv, tot_acc/nv) return tot_loss/nv, tot_acc/nv . I divided it in to a train and val Function for better readability : . def train(model, loss_func, opt, train_dl): model.train() for xb,yb in train_dl : loss = loss_func(model(xb),yb) loss.backward() opt.step() opt.zero_grad() def val(epoch,model, loss_func, valid_dl): model.eval() with torch.no_grad(): total_loss, total_acc = 0.,0. for xb,yb in valid_dl: total_loss += loss_func(model(xb),yb) total_acc += accuracy(model(xb),yb) iterations = len(valid_dl) print(epoch, total_loss/iterations, total_acc/iterations) return total_loss, total_acc, iterations def fit(epochs,model,loss_func,opt,train_dl,valid_dl): for epoch in range(epochs) : train(model,loss_func,opt,train_dl) loss,acc, nv = val(epoch,model,loss_func,valid_dl) return loss/(nv), acc/(nv) . Powerful Training Loops with Callbacks . In order to customize out training loop in many ways (regularization techniques, visualization, early stopping,…), we want to be able to do so easily without having to write a huge loop function all the time that is hard to read and update. For this fastai uses something called callbacks : . . Databunch . This can be used to store our info and doesn’t have any real logic. . class DataBunch(): def __init__(self, train_dl, valid_dl, c=None): self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c @property def train_ds(self): return self.train_dl.dataset @property def valid_ds(self): return self.valid_dl.dataset data = DataBunch(*get_dls(train_ds, valid_ds, bs), c) #c is max y value #further storage wrappers def get_model(data, lr=0.5, nh=50): m = data.train_ds.x.shape[1] model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c)) return model, optim.SGD(model.parameters(), lr=lr) class Learner(): def __init__(self, model, opt, loss_func, data): self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data . With our previous training loop modified a bit we can add Callbacks now, : . def one_batch(xb, yb, cb): if not cb.begin_batch(xb,yb): return loss = cb.learn.loss_func(cb.learn.model(xb), yb) if not cb.after_loss(loss): return loss.backward() if cb.after_backward(): cb.learn.opt.step() if cb.after_step(): cb.learn.opt.zero_grad() def all_batches(dl, cb): for xb,yb in dl: one_batch(xb, yb, cb) if cb.do_stop(): return def fit(epochs, learn, cb): if not cb.begin_fit(learn): return for epoch in range(epochs): if not cb.begin_epoch(epoch): continue all_batches(learn.data.train_dl, cb) if cb.begin_validate(): with torch.no_grad(): all_batches(learn.data.valid_dl, cb) if cb.do_stop() or not cb.after_epoch(): break cb.after_fit() . add Callbacks . class Callback(): def begin_fit(self, learn): self.learn = learn return True def after_fit(self): return True def begin_epoch(self, epoch): self.epoch=epoch return True def begin_validate(self): return True def after_epoch(self): return True def begin_batch(self, xb, yb): self.xb,self.yb = xb,yb return True def after_loss(self, loss): self.loss = loss return True def after_backward(self): return True def after_step(self): return True . Callback Hander . class CallbackHandler(): def __init__(self,cbs=None): self.cbs = cbs if cbs else [] def begin_fit(self, learn): self.learn,self.in_train = learn,True learn.stop = False res = True for cb in self.cbs: res = res and cb.begin_fit(learn) return res def after_fit(self): res = not self.in_train for cb in self.cbs: res = res and cb.after_fit() return res def begin_epoch(self, epoch): self.learn.model.train() self.in_train=True res = True for cb in self.cbs: res = res and cb.begin_epoch(epoch) return res def begin_validate(self): self.learn.model.eval() self.in_train=False res = True for cb in self.cbs: res = res and cb.begin_validate() return res def after_epoch(self): res = True for cb in self.cbs: res = res and cb.after_epoch() return res def begin_batch(self, xb, yb): res = True for cb in self.cbs: res = res and cb.begin_batch(xb, yb) return res def after_loss(self, loss): res = self.in_train for cb in self.cbs: res = res and cb.after_loss(loss) return res def after_backward(self): res = True for cb in self.cbs: res = res and cb.after_backward() return res def after_step(self): res = True for cb in self.cbs: res = res and cb.after_step() return res def do_stop(self): try: return self.learn.stop finally: self.learn.stop = False . Callback Test . class TestCallback(Callback): def begin_fit(self,learn): super().begin_fit(learn) self.n_iters = 0 return True def after_step(self): self.n_iters += 1 print(self.n_iters) if self.n_iters&gt;=10: self.learn.stop = True return True . These methods are checked for in our one_batch function and are executed in our training loop. . Callback Simplification . We do on not need to implement each function seperately by using a call function : . def __call__(self, cb_name): for cb in sorted(self.cbs, key=lambda x: x._order): f = getattr(cb, cb_name, None) if f and f(): return True return False . This allows is to recreate the Calbacks in a better way : . #export import re _camel_re1 = re.compile(&#39;(.)([A-Z][a-z]+)&#39;) _camel_re2 = re.compile(&#39;([a-z0-9])([A-Z])&#39;) def camel2snake(name): s1 = re.sub(_camel_re1, r&#39; 1_ 2&#39;, name) return re.sub(_camel_re2, r&#39; 1_ 2&#39;, s1).lower() class Callback(): _order=0 def set_runner(self, run): self.run=run def __getattr__(self, k): return getattr(self.run, k) @property def name(self): name = re.sub(r&#39;Callback$&#39;, &#39;&#39;, self.__class__.__name__) return camel2snake(name or &#39;callback&#39;) #export class TrainEvalCallback(Callback): def begin_fit(self): self.run.n_epochs=0. self.run.n_iter=0 def after_batch(self): if not self.in_train: return self.run.n_epochs += 1./self.iters self.run.n_iter += 1 def begin_epoch(self): self.run.n_epochs=self.epoch self.model.train() self.run.in_train=True def begin_validate(self): self.model.eval() self.run.in_train=False class TestCallback(Callback): def after_step(self): if self.train_eval.n_iters&gt;=10: return True #export class Runner(): def __init__(self, cbs=None, cb_funcs=None): cbs = listify(cbs) for cbf in listify(cb_funcs): cb = cbf() setattr(self, cb.name, cb) cbs.append(cb) self.stop,self.cbs = False,[TrainEvalCallback()]+cbs @property def opt(self): return self.learn.opt @property def model(self): return self.learn.model @property def loss_func(self): return self.learn.loss_func @property def data(self): return self.learn.data def one_batch(self, xb, yb): self.xb,self.yb = xb,yb if self(&#39;begin_batch&#39;): return self.pred = self.model(self.xb) if self(&#39;after_pred&#39;): return self.loss = self.loss_func(self.pred, self.yb) if self(&#39;after_loss&#39;) or not self.in_train: return self.loss.backward() if self(&#39;after_backward&#39;): return self.opt.step() if self(&#39;after_step&#39;): return self.opt.zero_grad() def all_batches(self, dl): self.iters = len(dl) for xb,yb in dl: if self.stop: break self.one_batch(xb, yb) self(&#39;after_batch&#39;) self.stop=False def fit(self, epochs, learn): self.epochs,self.learn = epochs,learn try: for cb in self.cbs: cb.set_runner(self) if self(&#39;begin_fit&#39;): return for epoch in range(epochs): self.epoch = epoch if not self(&#39;begin_epoch&#39;): self.all_batches(self.data.train_dl) with torch.no_grad(): if not self(&#39;begin_validate&#39;): self.all_batches(self.data.valid_dl) if self(&#39;after_epoch&#39;): break finally: self(&#39;after_fit&#39;) self.learn = None . In order to track our stats we will add a third Callback to see them, it will make us of a class that computes these. . class AvgStats(): def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train def reset(self): self.tot_loss,self.count = 0.,0 self.tot_mets = [0.] * len(self.metrics) @property def all_stats(self): return [self.tot_loss.item()] + self.tot_mets @property def avg_stats(self): return [o/self.count for o in self.all_stats] def __repr__(self): if not self.count: return &quot;&quot; return f&quot;{&#39;train&#39; if self.in_train else &#39;valid&#39;}: {self.avg_stats}&quot; def accumulate(self, run): bn = run.xb.shape[0] self.tot_loss += run.loss * bn self.count += bn for i,m in enumerate(self.metrics): self.tot_mets[i] += m(run.pred, run.yb) * bn class AvgStatsCallback(Callback): def __init__(self, metrics): self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False) def begin_epoch(self): self.train_stats.reset() self.valid_stats.reset() def after_loss(self): stats = self.train_stats if self.in_train else self.valid_stats with torch.no_grad(): stats.accumulate(self.run) def after_epoch(self): print(self.train_stats) print(self.valid_stats) . As this Callback alread tracks all our Stats, we can easily cerate a new Callback to save the best model based on validation loss at a given epoch and introduce early stopping. This can be done by inheriting from AvgStatsCallback which already has handy begin_epoch and after_loss functions that we can use. . class Early_save(AvgStatsCallback): def __init__(self, metrics,early_stopping_iter,loss_stop): self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False) self.lowest_val_loss = float(&quot;inf&quot;) self.cur_val_loss = float(&quot;inf&quot;) self.early_stopping_array = deque(maxlen=3) self.early_stopping_iter = 3 self.loss_stop = loss_stop def save_model(self): self.cur_val_loss, self.cur_val_acc = self.valid_stats.avg_stats if self.cur_val_loss &lt; self.lowest_val_loss : self.lowest_val_loss = self.cur_val_loss torch.save(learn.model.state_dict(),&quot;best_model.pt&quot;) print(&quot;Saving Model with val loss of :{:.4f}&quot;.format(self.lowest_val_loss)) def early_stopping(self): self.early_stopping_array.append(self.cur_val_loss) if (self.early_stopping_array.maxlen == len(self.early_stopping_array)): diff = 0.0 for i in range(0,len(self.early_stopping_array)-1): #check diff between losses diff += abs(self.early_stopping_array[i]-self.early_stopping_array[i+1]) diff /= (len(self.early_stopping_array)-1) if(diff &lt; self.loss_stop): return &quot;stop&quot; def after_epoch(self): print(self.train_stats) print(self.valid_stats) self.save_model() if self.early_stopping()==&quot;stop&quot;: return True print(self.valid_stats.avg_stats[1]) . It keeps track of the last 3 losses and will stop training if the loss difference is too small. It will also save a model that performs best on validation with the handy torch.save function. . The Class now also keeps track of the lowest validation loss overall and can save the best model based on validation loss. Early stopping was implemented by tracking the last n elements, with n=early_stopping_iter in this case. We are storing it in a deque data structures. The early_stopping function will return a string that will then lead to our after_epoch function returning True which will stop training, as we have : . if self(&#39;after_epoch&#39;): break . in our training loop. . Now we can finally call our methods and start trainig. . learn = Learner(*get_model(data), loss_func, data) stats = AvgStatsCallback([accuracy]) run = Runner(cbs=stats) run.fit(2, learn) loss,acc = stats.valid_stats.avg_stats . Using partial we can create a Function that can create a callback function : . from functools import partial acc_cbf = partial(AvgStatsCallback,accuracy) run = Runner(cb_funcs=acc_cbf) . This way we can create Callback funcs easily, e.g. by using a list. . Annealing . We define two new callbacks: the Recorder to save track of the loss and our scheduled learning rate, and a ParamScheduler that can schedule any hyperparameter as long as it’s registered in the state_dict of the optimizer. . class Recorder(Callback): def begin_fit(self): self.lrs,self.losses = [],[] def after_batch(self): if not self.in_train: return self.lrs.append(self.opt.param_groups[-1][&#39;lr&#39;]) self.losses.append(self.loss.detach().cpu()) def plot_lr (self): plt.plot(self.lrs) def plot_loss(self): plt.plot(self.losses) class ParamScheduler(Callback): _order=1 def __init__(self, pname, sched_func): self.pname,self.sched_func = pname,sched_func def set_param(self): for pg in self.opt.param_groups: pg[self.pname] = self.sched_func(self.n_epochs/self.epochs) def begin_batch(self): if self.in_train: self.set_param() def sched_lin(start, end): def _inner(start, end, pos): return start + pos*(end-start) return partial(_inner, start, end) def annealer(f): def _inner(start, end): return partial(f, start, end) return _inner @annealer def sched_lin(start, end, pos): return start + pos*(end-start) . The Decorator annealer passes sched_lin in annealer and replaces sched_lin() definition with what annealer returns. . other scheduler funcs . @annealer def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2 @annealer def sched_no(start, end, pos): return start @annealer def sched_exp(start, end, pos): return start * (end/start) ** pos def cos_1cycle_anneal(start, high, end): return [sched_cos(start, high), sched_cos(high, end)] #This monkey-patch is there to be able to plot tensors torch.Tensor.ndim = property(lambda x: len(x.shape)) . plot them . annealings = &quot;NO LINEAR COS EXP&quot;.split() a = torch.arange(0, 100) p = torch.linspace(0.01,1,100) fns = [sched_no, sched_lin, sched_cos, sched_exp] for fn, t in zip(fns, annealings): f = fn(2, 1e-2) plt.plot(a, [f(o) for o in p], label=t) plt.legend() . Combine Schedulers with a function : . def combine_scheds(pcts, scheds): assert sum(pcts) == 1. pcts = tensor([0] + listify(pcts)) assert torch.all(pcts &gt;= 0) pcts = torch.cumsum(pcts, 0) def _inner(pos): idx = (pos &gt;= pcts).nonzero().max() actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx]) return scheds[idx](actual_pos) return _inner sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) . Here is an example: use 30% of the budget to go from 0.3 to 0.6 following a cosine, then the last 70% of the budget to go from 0.6 to 0.2, still following a cosine. Train for a long time @ high lr, then switch to lower lr with cosine 1 cycle schedules. . Now we can train with our Callbacks and cosine scheduling. . cbfs = [Recorder, partial(AvgStatsCallback,accuracy), partial(ParamScheduler, &#39;lr&#39;, sched)] learn = create_learner(get_model_func(0.3), loss_func, data) run = Runner(cb_funcs=cbfs) run.fit(3, learn) . These Callbacks can also be used to move compute to our GPU ! .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/02/DL-From-Foundations-Part2.html",
            "relUrl": "/markdown/2020/04/02/DL-From-Foundations-Part2.html",
            "date": " • Apr 2, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastai DL from the Foundations Matmul, Initalization, ReLU, Backprop, MSE",
            "content": "Fastai DL from the Foundations Lesson 1 . The idea of this Repo is to manage and document the Code for the fastai Deep Learning from the foundations course and include Papers and Summaries of them when it is helpful to do so. The course focuses on building large components of the fastai library and Pytorch from scratch to allow deep understanding of the fastai and Pytorch frameworks, which enables the creation of own algorithms and makes debugging easier. . . The Code is based on the code in the fastai course (v3), check out their repo which also includes part 1 which is realy focused on the practical side of things. Thank you to Jeremy, Rachel and the fastai Team for this great course. . In order to understand the material of part 2, you should be familiar with the following concepts, depending on each category : . Fundamentals . Affine functions &amp; non-linearities | Parameters &amp; activations | Random weight initalization and transfer learning | Stochastic gradient descent, Momentum and ADAM (a combination of RMSprop and Momentum) | Regularization techniques, specifically batch norm, dropout, weight decay and data augmentation | Embeddings Vision . | Image classification and Regression Lesson 1 | Image classification and Regression Lesson 2 | Conv Nets | Residual and dense blocks | Segmentation : U-Net | GANs NLP . | Language models &amp; NLP Tabular Data . | Continious &amp; categorical variables | Collaborative filtering | . ## Lesson 1 . As we already know DL is mainly based on Linear Algebra, so let’s implement some simple Matrix Multiplication ! We already know that np.matmul can be used for this, bur let’s do it ourselves. . def matmul(a,b): ar,ac = a.shape # n_rows * n_cols br,bc = b.shape assert ac==br #check for right dimensions =&gt; output dim is ar,bc c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): for k in range(ac): # or br c[i,j] += a[i,k] * b[k,j] return c . This is a very simple and inefficient implementation, which runs in 572 ms on my CPU with matrix dimensions 5x784 multiplied by 784x10. As expected the output array has 5 rows, if we used MNIST (50k) rows onw forward pass would take more than an hour which is unacceptable. . To improve this we can pass the Code down to a lower level language (Pytorch uses ATen a Tensor library for this). This can be done with elementwise multiplication (also works on Tensors with rank &gt; 1) : . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum() #row by column return c . . This is essentially using the above formula and executing it in C code, with a runtime of : 802 µs ± 60.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each). Which is about 714 times faster than the first implementation ! Wooho we are done ! . Broadcasting . Hold on not so fast ! We can still do better by removing the inner loop with Broadcasting. Broadcasting “broadcasts” the smaller array across the larger one, so they have compatible shapes, operations are vecorized so that loops are executed in C without any overhead. You can see the broadcasted version of a vector by calling : . &lt;smaller_array&gt;.expand_as(&lt;larger_array&gt;) . after expansion you can call : . &lt;smaller_array&gt;.storage() . and you will see that no additional memory is needed. With this our matmul looks like this : . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): c[i] = (a[i ].unsqueeze(-1) * b).sum(dim=0) #unsqueeze is used to unsqueeze a to rank 2 return c . This code executes in 172 µs ± 14.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each). Which means we are 3325.81 faster than in the beginning, nice. . Einstein Summation . A compact representation to combine products and sums in a general way. . def matmul(a,b): return torch.einsum(&#39;ik,kj-&gt;ij&#39;, a, b) . This speeds up the code a little more (factor 3575 compared to the start), but more improvements can be made. . It uses this string “mini language” (sort of like regex) to specify the multiply, it is a little bit annoying and languages like Swift will hopefully allow us to get rid of this. . Pytorch Op . Pushes the code to BLAS, Hardware optimized code. We can not specify this with Pytorch, with Swift this could be optimized by the programmer more easily. A classic operation is the @ operator, it can do more than matmul (such as Batch wise, Tensor Reductions,…). . Matmul Summary . Algorithm Runtime on CPU Factor improvement . Naive Loops | 572 ms | 1 | . Loops + elementwise row/column multiply | 802 µs | 714 | . Brodacasting | 172 µs | 3326 | . Einstein Summation | 160 µs | 3575 | . Pytorch’s function (uses HW specific BLAS) | 86 µs | 6651 | . Now let’s use it to init our weights and code RELU . Init . We create a 2layer Net, with a hidden layer of size 50. . m is the 2nd dimension size of our input. . nh = 50 w1 = torch.randn(m,nh)/math.sqrt(m) b1 = torch.zeros(nh) w2 = torch.randn(nh,1)/math.sqrt(nh) b2 = torch.zeros(1) . Randn gives us weights with mean 0 and std of 1. Just using random numbers the mean and std of our output vector will be way off. In order to avoid this we divide by sqrt(m), which will keep our output mean and std in bounds. . Another common initalization method is Xavier Initalization. Check out Andrej Karpathy’s lecture (starting at about 45:00) for a good explanation Even more advanced methods like Fixup initalization can be used. The authors of the paper are able to learn deep nets (up to 10k layers) as stable without normalization when using Fixup. . Problem : if the variance halves each layer, the activation will vanish after some time, leading to dead neurons. . Due to this the 2015 ImageNet ResNet winners, see 2.2 in the paper suggested this : Up to that point init was done with random weights from Gaussian distributions, which used fixed std deviations (for example 0.01). These methods however did not allow deeper models (more than 8 layers) to converge in most cases. Due to this, in the older days models like VGG16 had to train the first 8 layers at first, in order to then initalize the next ones. As we can imagine this takes longer to train, but also may lead to a poorer local optimum. Unfortunately the Xavier init paper does not talk about non-linarities, but should not be used with ReLu like functions, as the ReLu function will half the distribution (values smaller than zero are = 0) at every step. . . Looking at the distributions in the plots, you can see that the rapid decrease of the std. deviation leads to ReLu neurons activating less and less. . The Kaiming init paper investigates the variance at each layer and ends up suggesting the following : . essentially it just adds the 2 in the numerator to avoid the halfing of the variance due at each step. . A direct comparison in the paper on a 22 layer model shows the benefit, even though Xavier converges as well, Kaiming init does so significantly faster. With a deeper 30-layer model the advantage of Kaiming is even more evident. . Kaiming init code : . w1 = torch.randn(m,nh)*math.sqrt(2/m) . ReLu can be implemented easily, it clamps values below 0 and is linear otherwise : . def relu(x): return x.clamp_min(0.) . Leaky ReLu avoids 0-ing the gradient by using a small negative slope below 0 (0.01 usually). . Therfore Kaiming init with ReLU can be implemented like this : . w1 = torch.randn(m,nh)*math.sqrt(2./m ) t1 = relu(lin(x_valid, w1, b1)) t1.mean(),t1.std() . Info . The Pytorch source code in the tutorial for torch.nn.Conv2d uses a kaiming init with : . init.kaiming_uniform_(self.weight, a=math.sqrt(5)) . .sqrt(5) was an original bug from the Lua Torch and was fixed now ! . Loss Function MSE . Now that we have done almost one forward pass,we still need to implement an error function. MSE Error, popular for regression tasks, can be implemented like this : . def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() . .squeeze() is used to get rid of a trainiling (,1) in this case. . Gradient and Backward Pass . Mathematically the Backward Pass uses the chain rule to compute all of the gradients. . In order to Backprop effectively, we need to calc the gradients of all of our components. In our case these are our loss, activation functions (only ReLu, which is easy) and our linear layers. . I suggest CS 231n by Andrej Karpathy for mathematical explanation of Backprop. . Let’s start with MSE : . def mse_grad(inp, targ): # grad of loss with respect to output of previous layer inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0] . Relu : . def relu_grad(inp, out): # grad of relu with respect to input activations inp.g = (inp&gt;0).float() * out.g . Very simple, the gradient is either 0 or 1. In the Leaky Relu Case it’s either -0.01 or 1. . Linear Layers . def lin_grad(inp, out, w, b): # grad of matmul with respect to input inp.g = out.g @ w.t() #matrix prod with the transpose w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0) b.g = out.g.sum(0) . Forward and Backward Pass . def forward_and_backward(inp, targ): # forward pass: l1 = inp @ w1 + b1 l2 = relu(l1) out = l2 @ w2 + b2 # we don&#39;t actually need the loss in backward! loss = mse(out, targ) # backward pass, just reverse order: mse_grad(out, targ) lin_grad(l2, out, w2, b2) relu_grad(l1, l2) lin_grad(inp, l1, w1, b1) . In order to check our results, we can use Pytorch : . We can control our results with Pytorch auto_grad() function . xt2 = x_train.clone().requires_grad_(True) w12 = w1.clone().requires_grad_(True) w22 = w2.clone().requires_grad_(True) b12 = b1.clone().requires_grad_(True) b22 = b2.clone().requires_grad_(True) . .requires_grad(True) turns a tensor in to an autograd so it can keep track of each step . Refactor . It’s always good to refactor our code. This can be done by creating classes and using our functions. One for forward and one for backward pass. . class Relu(): def __call__(self, inp): self.inp = inp self.out = inp.clamp_min(0.)-0.5 return self.out def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g . class Lin(): def __init__(self, w, b): self.w,self.b = w,b def __call__(self, inp): self.inp = inp self.out = inp@self.w + self.b return self.out def backward(self): self.inp.g = self.out.g @ self.w.t() # Creating a giant outer product, just to sum it, is inefficient! self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0) self.b.g = self.out.g.sum(0) . class Mse(): def __call__(self, inp, targ): self.inp = inp self.targ = targ self.out = (inp.squeeze() - targ).pow(2).mean() return self.out def backward(self): self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0] . Lastly we create a model class. . class Model(): def __init__(self, w1, b1, w2, b2): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . The execution times is to slow and we want to avoid the call() declarations so we define a module class . further Refactor . class Module(): def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&#39;not implemented&#39;) def backward(self): self.bwd(self.out, *self.args) . class Relu(Module): def forward(self, inp): return inp.clamp_min(0.)-0.5 def bwd(self, out, inp): inp.g = (inp&gt;0).float() * out.g . class Lin(Module): def __init__(self, w, b): self.w,self.b = w,b def forward(self, inp): return inp@self.w + self.b def bwd(self, out, inp): inp.g = out.g @ self.w.t() self.w.g = torch.einsum(&quot;bi,bj-&gt;ij&quot;, inp, out.g) self.b.g = out.g.sum(0) . class Mse(Module): def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean() def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0] . class Model(): def __init__(self): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . Now we can call the forward and backprop passes for our model easily. . Summary . To summarize we implemented nn.Linear and nn.Module and will be able to write the train loop next lesson ! .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/01/DL-From-Foundations-Part1.html",
            "relUrl": "/markdown/2020/04/01/DL-From-Foundations-Part1.html",
            "date": " • Apr 1, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://cedric-perauer.github.io/DL_from_Foundations/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}