{
  
    
        "post0": {
            "title": "Dl From Foundations Part1",
            "content": "Fastai_DL_from_the_foundations . The idea of this Repo is to manage and document the Code for the fastai Deep Learning from the foundations course and include Papers and Summaries of them when it is helpful to do so. The course focuses on building large components of the fastai library and Pytorch from scratch to allow deep understanding of the fastai and Pytorch frameworks, which enables the creation of own algorithms and makes debugging easier. . . The Code is based on the code in the fastai course (v3), check out their repo which also includes part 1 which is realy focused on the practical side of things. Thank you to Jeremy, Rachel and the fastai Team for this great course. . In order to understand the material of part 2, you should be familiar with the following concepts, depending on each category : . 0. Fundamentals . Affine functions &amp; non-linearities | Parameters &amp; activations | Random weight initalization and transfer learning | Stochastic gradient descent, Momentum and ADAM (a combination of RMSprop and Momentum) | Regularization techniques, specifically batch norm, dropout, weight decay and data augmentation | Embeddings Vision . | Image classification and Regression Lesson 1 | Image classification and Regression Lesson 2 | Conv Nets | Residual and dense blocks | Segmentation : U-Net | GANs NLP . | Language models &amp; NLP Tabular Data . | Continious &amp; categorical variables | Collaborative filtering | . ## 1. Lesson 1 . As we already know DL is mainly based on Linear Algebra, so let’s implement some simple Matrix Multiplication ! We already know that np.matmul can be used for this, bur let’s do it ourselves. . def matmul(a,b): ar,ac = a.shape # n_rows * n_cols br,bc = b.shape assert ac==br #check for right dimensions =&gt; output dim is ar,bc c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): for k in range(ac): # or br c[i,j] += a[i,k] * b[k,j] return c . This is a very simple and inefficient implementation, which runs in 572 ms on my CPU with matrix dimensions 5x784 multiplied by 784x10. As expected the output array has 5 rows, if we used MNIST (50k) rows onw forward pass would take more than an hour which is unacceptable. . To improve this we can pass the Code down to a lower level language (Pytorch uses ATen a Tensor library for this). This can be done with elementwise multiplication (also works on Tensors with rank &gt; 1) : . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum() #row by column return c . . This is essentially using the above formula and executing it in C code, with a runtime of : 802 µs ± 60.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each). Which is about 714 times faster than the first implementation ! Wooho we are done ! . Broadcasting . Hold on not so fast ! We can still do better by removing the inner loop with Broadcasting. Broadcasting “broadcasts” the smaller array across the larger one, so they have compatible shapes, operations are vecorized so that loops are executed in C without any overhead. You can see the broadcasted version of a vector by calling : . afetr expansion you can call : &lt;smaller_array&gt;.storage() and you will see that no additional memory is needed. With this our matmul looks like this : . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): c[i] = (a[i ].unsqueeze(-1) * b).sum(dim=0) #unsqueeze is used to unsqueeze a to rank 2 return c . This code executes in 172 µs ± 14.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each). Which means we are 3325.81 faster than in the beginning, nice. ### Einstein Summation A compact representation to combine products and sums in a general way. . def matmul(a,b): return torch.einsum(‘ik,kj-&gt;ij’, a, b) . This speeds up the code a little more (factor 3575 compared to the start), but more improvements can be made. It uses this string &quot;mini language&quot; (sort of like regex) to specify the multiply, it is a little bit annoying and languages like Swift will hopefully allow us to get rid of this. ### Pytorch Op Pushes the code to BLAS, Hardware optimized code. We can not specify this with Pytorch, with Swift this could be optimized by the programmer more easily. A classic operation is the @ operator, it can do more than matmul (such as Batch wise, Tensor Reductions,...). ### Matmul Summary | Algorithm | Runtime on CPU | Factor improvement | | | ||--|--||| | Naive Loops | 572 ms | 1 | | | | Loops + elementwise row/column multiply | 802 µs | 714 | | | | Brodacasting | 172 µs | 3326 | | | | Einstein Summation | 160 µs | 3575 | | | | Pytorch&#39;s function (uses HW specific BLAS) | 86 µs | 6651 | | | ## Now let&#39;s use it to init our weights and code RELU ### Init We create a 2layer Net, with a hidden layer of size 50. m is the 2nd dimension size of our input. . nh = 50 w1 = torch.randn(m,nh)/math.sqrt(m) b1 = torch.zeros(nh) w2 = torch.randn(nh,1)/math.sqrt(nh) b2 = torch.zeros(1) . Randn gives us weights with mean 0 and std of 1. Just using random numbers the mean and std of our output vector will be way off. In order to avoid this we divide by sqrt(m), which will keep our output mean and std in bounds. Another common initalization method is Xavier Initalization. Check out [Andrej Karpathy&#39;s lecture](https://www.youtube.com/watch?v=mzkOF4tULj8) (starting at about 45:00) for a good explanation Even more advanced methods like [Fixup initalization](https://arxiv.org/abs/1901.09321) can be used. The authors of the paper are able to learn deep nets (up to 10k layers) as stable without normalization when using Fixup. Problem : if the variance halves each layer, the activation will vanish after some time, leading to dead neurons. Due to this the [2015 ImageNet ResNet winners](https://arxiv.org/abs/1502.01852) suggested this method : ![](images/resnet_init.png) essentially it just adds the 2 in the numerator to avoid the halfing of the variance due at each step. #### Kaiming init in code : # kaiming init / he init for relu w1 = torch.randn(m,nh)*math.sqrt(2/m) . ReLu can be implemented easily, it clamps values below 0 and is linear otherwise : . def relu(x): return x.clamp_min(0.) . Leaky ReLu avoids 0-ing the gradient by using a small negative slope below 0 (0.01 usually). . Therfore Kaiming init with ReLU can be implemented like this : . w1 = torch.randn(m,nh)*math.sqrt(2./m ) t1 = relu(lin(x_valid, w1, b1)) t1.mean(),t1.std() . Warning !! . The Pytorch source code for torch.nn.Conv2d uses a kaiming init with : . init.kaiming_uniform_(self.weight, a=math.sqrt(5)) . .sqrt(5) is mysterious and does not seem to be a good idea . Loss Function MSE . Now that we have done almost one forward pass,we still need to implement an error function. MSE Error, popular for regression tasks, can be implemented like this : . def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() . .squeeze() is used to get rid of a trainiling (,1) in this case. . Gradient and Backward Pass . Mathematically the Backward Pass uses the chain rule to compute all of the gradients. . In order to Backprop effectively, we need to calc the gradients of all of our components. In our case these are our loss, activation functions (only ReLu, which is easy) and our linear layers. . I suggest CS 231n by Andrej Karpathy for mathematical explanation of Backprop. . Let’s start with MSE : . def mse_grad(inp, targ): # grad of loss with respect to output of previous layer inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0] . Relu : . def relu_grad(inp, out): # grad of relu with respect to input activations inp.g = (inp&gt;0).float() * out.g . Very simple, the gradient is either 0 or 1. In the Leaky Relu Case it’s either -0.01 or 1. . Linear Layers . def lin_grad(inp, out, w, b): # grad of matmul with respect to input inp.g = out.g @ w.t() #matrix prod with the transpose w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0) b.g = out.g.sum(0) . Forward and Backward Pass . def forward_and_backward(inp, targ): # forward pass: l1 = inp @ w1 + b1 l2 = relu(l1) out = l2 @ w2 + b2 # we don&#39;t actually need the loss in backward! loss = mse(out, targ) # backward pass, just reverse order: mse_grad(out, targ) lin_grad(l2, out, w2, b2) relu_grad(l1, l2) lin_grad(inp, l1, w1, b1) . In order to check our results, we can use Pytorch : . We can control our results with Pytorch auto_grad() function . xt2 = x_train.clone().requires_grad_(True) w12 = w1.clone().requires_grad_(True) w22 = w2.clone().requires_grad_(True) b12 = b1.clone().requires_grad_(True) b22 = b2.clone().requires_grad_(True) . .requires_grad(True) turns a tensor in to an autograd so it can keep track of each step . Refactor . It’s always good to refactor our code. This can be done by creating classes and using our functions. One for forward and one for backward pass. . class Relu(): def __call__(self, inp): self.inp = inp self.out = inp.clamp_min(0.)-0.5 return self.out def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g . class Lin(): def __init__(self, w, b): self.w,self.b = w,b def __call__(self, inp): self.inp = inp self.out = inp@self.w + self.b return self.out def backward(self): self.inp.g = self.out.g @ self.w.t() # Creating a giant outer product, just to sum it, is inefficient! self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0) self.b.g = self.out.g.sum(0) . class Mse(): def __call__(self, inp, targ): self.inp = inp self.targ = targ self.out = (inp.squeeze() - targ).pow(2).mean() return self.out def backward(self): self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0] . Lastly we create a model class. . class Model(): def __init__(self, w1, b1, w2, b2): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . The execution times is to slow and we want to avoid the call() declarations so we define a module class . further Refactor . class Module(): def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&#39;not implemented&#39;) def backward(self): self.bwd(self.out, *self.args) . class Relu(Module): def forward(self, inp): return inp.clamp_min(0.)-0.5 def bwd(self, out, inp): inp.g = (inp&gt;0).float() * out.g . class Lin(Module): def __init__(self, w, b): self.w,self.b = w,b def forward(self, inp): return inp@self.w + self.b def bwd(self, out, inp): inp.g = out.g @ self.w.t() self.w.g = torch.einsum(&quot;bi,bj-&gt;ij&quot;, inp, out.g) self.b.g = out.g.sum(0) . class Mse(Module): def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean() def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0] . class Model(): def __init__(self): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . Now we can call the forward and backprop passes for our model easily. . Summary . To summarize we implemented nn.Linear and nn.Module and will be able to write the train loop next lesson ! .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/2020/04/01/DL-From-Foundations-Part1.html",
            "relUrl": "/2020/04/01/DL-From-Foundations-Part1.html",
            "date": " • Apr 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://cedric-perauer.github.io/DL_from_Foundations/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}