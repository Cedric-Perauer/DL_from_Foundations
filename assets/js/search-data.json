{
  
    
        "post0": {
            "title": "Mish Paper Summary",
            "content": "Paper Link . What did the authors want to achieve ? . Propose a new activation function which replaces upon the known standards such as ReLU and Swish. The function proposed is called Mish activation and is defined by : $f(x) = x * tanh(softplus(x))$ Recall that Sofplus is defined as $f(x) = ln(1+e^{x})$ The authors show that it can be more effective than ReLU and Swish for Computer Vision tasks. . Methods . Mish Activation . As explained in the intro, Mish is a novel activation function. It is inspired by ReLU and Swish and has a bounded bottom value of $~ -0.31$ The derivative is defined as : $f^{&#39;}(x) = frac{e^{x} * w}{ delta^{2}}$ With $w=4*(x+1) + 4e^{2x} + e^{3x} +e^{x} * (4x+6)$ and $ delta = 2e^{2x} + e^{2x} + 2$ It also has a self gating property, which means that it simply takes a scalar as input and allows it to easily replace ReLU in existing networks. A plot including Mish and Swish derivatives is shown below : . Code . We can implement Mish in Pytorch the following way : . class MishImplementation(torch.autograd.Function): @staticmethod def forward(ctx,x): ctx.save_for_backward(x) return x.mul(torch.tanh(F.softplus(x)) # x * tanh(ln(1 + exp(x))) @staticmethod def backward(ctx,grad_output): x = ctx.saved_tensors[0] sx = torch.sigmoid(x) fx = F.sofplus(x).tanh() return grad_output * (fx + x * sx * (1 - fx * fx)) . Credits go to the author of the paper and the implementation above that is used in YOLOv3 by Ultralytics. . Explanation . The authors explain why Mish does improve upon current results in this section and emphasize the advantageous properties of Mish. . Like Relu and Swish, Mish is unbounded above, which prevents saturation and therefore vanishing gradients. The about -0.31 bound below adds strong regularization properties. Not killing gradients when x is below 0 improves gradient flow and therefore improves expressivity. Famously ReLU is not differentiable at 0, the smoothness of Mish makes it continuously differentiable. The smoother function allow for smoother loss functions and therefore better optimization. The authors summarize these properties and the table above. . The authors generally recommend to use a higher amount of epochs with Mish activation. This obviously introduces some overhead during training. . Results . Hyperparameter study . The author studies the difference between ReLU, Swish and Mish in Section 3.1 by considering fully connected nets with different layer amounts batch norm, dropout and no residual connections, plots are shown for every category. The most important takeaway is that Mish is better than current SOTA for optimizing larger and wider networks. It has to be criticized that the author is not using residual connections here, as this might increase the advantage of Mish even more than it would in a real setting with a skip connection network. They also show that larger batch sizes benefit from Mish, it is also more stable for different initalizations and slightly more robust to noise. . The results are replicated in experiments with a 6-layer CNN. Here Mish outperforms Swish with 75.6% to 71.7% on CIFAR-100. Swish did not seem to learn for the first 10 epochs here due to dead gradients. . The author also shows that Mish outperforms Swish when using Cosine Annealing and outperforms Swish by about 9% when using Mixup with $ alpha=0.2$ to compare the two methods. Statistical comparison shows that Mish has highest mean test accuracy and lowest mean standard deviation when compared to ReLU and Swish. . It is also mentioned that Mish is slightly less efficient on GPU than the other two mentioned activation functions. . Different Architectures with Mish . The author compares Mish by training with different networks and soley replacing ReLU/Swish with Mish, while leaving the hyperparameters unchanged. The superior performance of Mish during CIFAR100 can be seen in the table below : .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/24/Mish.html",
            "relUrl": "/jupyter/2020/07/24/Mish.html",
            "date": " • Jul 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Efficient Det paper summary",
            "content": "What did the authors want to achieve ? . Recently, the SOTA in high accuracy in the fields of object detection and semantic segmentation were mostly achieved by scaling up architectures (e.g. AmoebaNet and NAS-FPN). These models are not easily deployable, especially in runtime/compute constrained applications such as autonomous driving. While existing worked has achieved faster rutime through one-stage/anchor-free detectors or model compression, they usually sacrifice accuracy for runtime. The goal is therefore to create an architecture that combines the best of both worlds, and achieve both high accuracy and better efficiency. The authors consider a wide range of compute that someone might have at hand during inference (3B to 300B FLOPS). . Methods . Challenges . The authors define 2 main challenges : . 1) efficient multi-scale feature fusion . The authors do consider recent developments like PANet and NAS-FPN (both improvments of the original FPN approach). Most of these works only sum up the features without weighting them, even though the resolutions are different. That&#39;s why the authors of the paper propose a weighted bi-directional feature pyramid network (BiFPN), it introduces weights that can learn the importance of a different input features. It does this while also applying top-down and bottom-up bath augmentation as proposed in the PANet paper. . 2) model scaling . In the past, the main method to improve performance, was using larger and therefore more powerful backbones. In this paper the authors use NAS to jointly scale resolution of the input, depth and width of the net as well as sclaing the feature network and box/class prediction network. It thus follows the ideas of EfficientNet, which also turns out to be their backbone choice. The architectures that are proposed is therefore a combo of EfficientNet, BiPFN and compound scaling. These models are called EfficientDet, in honor of their Backbone. . $Figure$ $1$ . BiFPN . Problem . BiFPN aims to aggregate features at different resolutions, as FPN-ish methods downscale the feature level with a resolution of $1/2^{i}$ ($i$ being the layer number) , up-/downsampling is used in order to match features of different resolution. . Cross-Scale Connections . In their research the authors compare PANet (introduces bottom-up aggregation) with FPN and NAS-FPN and find that PANet achieves the best accuracy at the cost of some compute overhead. The authors improve upon these cross-scale connections by removing and thereby simplifying nodes with only one input edge, as these have less contribution to the feature net. Secondly they connect an extra edge from input to output node if they are at the same level (fusion with low compute cost), also see $Figure$ $1$. Furthermore, each bidirectional layer (top-down and bottom-up bath) is repeated multiple times to enable higher level fusion. For concrete implementation details (# of layers), refer to section 4.2 in the paper. . Weighted Feature Fusion . As explained earlier, the different resolutions that the feature maps have should be considered and therefore weithed during aggregation. In this work the authors use Pyramid Attention Network, which uses global self-attention upsampling to recover the location of the pixels. In order to weigh each input seperately, an additional learnable weight is added for each input. The authors consider three different fusion approaches : . 1) Unbounded Fusion : $0 = sum limits_{i} w_{i} * I_{i}$ =&gt; could lead to instability, so weight norm is applied here . 2) Softmax-based function : $O = sum limits_{i} dfrac{e^{w_{i}}}{ sum limits_{j} e^{w_{j}}} * I_{i}$ =&gt; here the idea is to normalize the probablities between 0 and 1, weighting the importance of each input that way. However the softmax introduces extra slowdown on the GPU hardware, so 3) is proposed : . 3) Fast normalized fusion : $O = sum limits_{i} dfrac{w_{i}}{ sum limits_{j} {w_{j}} + epsilon} * I_{i}$ . This function has the same learning charaterstics as 2), but it runs about 30% faster on GPU Depthwise sep. convs are used for better runtime. . Architecture . $Figure$ $2$ . To create the final architecture a new compound scaling method is introduced which scales Backbone,BiFPN, class/box net and resolution jointly. A heuristics used as object detectors have even more possible configs as classification nets. . Backbone . In order to use ImageNet pretraining, the checkpoints of EfficientNet-B0 to B6 are used. . BiFPN Net . The BiFPN depth is linearly increased. The width is scaled exponentially, a grid search is done and 1.35 is used as a base : . $W_{BiFPN} = 64 * (1.35^{ theta})$ $D_{BiFPN} = 3 + theta$ . . Box/class prediction network . Width is fixed to be same as BiFPN, but depth is increased differently : $D_{box} = D_{class} = 3 + [ theta/3]$ . Image Resolution . Image res. is increased using equation : $R_{input} = 512 + theta * 128$ 128 is used as the features are used in level 3-7 and $2^{7} = 128$ . Heuristics based scaling might not be optimal and could be improved. . The scaling results can be seen below : . The advantage of compound scaling is shown in $Figure$ $2$. . Some Implementation Details . SGD with momentum 0.9 and weight decay 4e-5 | Synch BN w/BN decay of 0.99 end epsilon 1e-3 | swish activation with weight decay of 0.9998 | focal-loss with α = 0.25 and γ = 1.5, and aspect ratio {1/2, 1, 2} | RetinaNnet preprocessing with training-time flip- ping and scaling | soft NMS is used for D7, standard NMS for the others | . Results . As the graph above shows, EfficientDet-D0 performs about on par with YOLOv3. It does use 28x fewer flops, which does not directly show in the runtime (also due to the optimized TensorRT implementation that YOLOv3 uses). Overall it is about 4-9x smaller and 13-42x less FLOP hungry than other detectors. In total they are about 4.1x faster on GPU and even 10x faster on CPU. This is probably due to the fact that the CPU can not hide the extra compute requierements in FLOPS as efficiently as the latency hiding GPU architecture. . The authors compete on a Segmentation task and outperform DeepLabV3+ by 1.7% on COCO Segmentation with 9.8x fewer FLOPS. .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/15/EfficientDet.html",
            "relUrl": "/jupyter/2020/07/15/EfficientDet.html",
            "date": " • Jul 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "SE Net Summary",
            "content": "SENET Paper Summary . (Jie HU, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu) . What did the authors want to achieve ? . So far most research was focused on the feature hierarchy (global receptive fields are obtained through multi-scale operations, interleaving layers, ...) the authors here want to focus on channel relationship instead. They propose a new blok type called the &quot;Squeeze and excitation block&quot;, which models dependencies between channels. They achieve new SOTA results on ImageNet and won the competition in 2017. . Methods Proposed . Squeeze-and-Excitation Block : . . feature maps $U$ are first passed through a $squeeze$ operation, this results in a descriptor. It stacks the maps by it&#39;s spatial Dimensions (HxW), so the total size is (HxWxnumber of channels), essentially it produces an Embedding. . After that an $excitation$ operation is applied, it is a self gating mechanism. It uses the emedding as an input and outputs per channel weights. This operation is then applied to the feature maps $U$. SE Blocks can be stacked like Dense blocks, Residual blocks, etc. . In the early layers, SE blocks are class agnostic, while they are very class specific in the later ones. Therfore the advantage feature recalibration can be accumulated in the net. . Squeeze . Each filter only has a local receptive field, which is only focusing on that region. The researchers therefore propose the squeeze operation to create one descriptor of global information. It is generated by shrinking the input $U$ (using global avg. pooling) through it&#39;s spatial dimensions : . $z_{c} = F_{sq}(u_{c}) = dfrac{1}{H*W} * sum limits_{i=1}^H sum limits_{j=1}^W u_{c}(i,j) $ . The output $z$ can be interpreted as a collection of the local descriptors, together they describe the whole image. . Excitation . The excitation block is responsible for capturing channel-wise dependencies. The authors choose a function that is 1) flexibel (learn non-linear interactions between channels) and 2) has a non-mutually exclusive relationship, because they want to have the ability to emphasize multiple channels and thereby avoid using a one-hot activation. The authors propose a gating mechanism to do this : . $s = F_{ex}(z,W) = sigma(g,(z,W)) = sigma(W_{2} delta(W_{1}z))$ . Here $ delta$ is the ReLU activation function $W_{1} epsilon R^{ frac{C}{R} * C} $ $W_{2} epsilon R^{C * frac{C}{R} } $ . In order to limit the compute complexity, the authors make the gating mechanism consist of a bottleneck with 2 FC layers around the non-linear activation function. It uses a reduction ration r (for good choice of this refer to 6.1 in the paper), a ReLU and then an increasing layer for dimensionality. The final output is then obtained by rescaling $U$ with an activation (scalar multiplies channel-wise). In a way SE blocks produce their own self attention on channels that are not locally confined, by mapping the descriptor produced by squeeze to channel weights. . SE blocks can be nicely implemented into existing architectures such as Inception, ResNet or ResNeXt blocks. DOW . . Compute complexity . Due to the global avg pooling in the squeeze part, only two FC layers and channelwise scaling in the excitation part, only 0.26% inference compute increase compared to a normal ResNet-50 (with image size 224x224) is observed. SE blocks increases ResNet parameter size by about ~10% (2.5 Million parameters). . Results . the authors prove that Squeeze and Excitation do both improve the performance and can be easily added to most architectures. It yields an improvement independent of architecture, but the exact type of SE block used should be researched depending on the base architecture. | Later SE layers learn close to the Identity mapping, so | 2-3% mAP improvement compared to ResNet backbone Faster-RCNN | 25% improvement on ImageNet top-5 error | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/07/03/SENET.html",
            "relUrl": "/jupyter/2020/07/03/SENET.html",
            "date": " • Jul 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "PANet Paper Summary",
            "content": "PANet Paper Summary . What did the authors want to achieve ? . . improve Computer Vision tasks object detection and mainly instance segmentation | build on top of Fast/ Faster /Mask RCNN and improve info propagation | design an architecture that can deal with blurry and heavy occlusion of the new datasets back then (2018) , like COCO 2017 | use in-net feature hierarchy : top down path with lateral connections is augmented to emphasize strong sementical features | . Methods used . Findings . Mask RCNN : long path from low-level to topmost features, which makes it difficult to access localization information | Mask RCNN : only single view, multi view preferd to gather diverse information | Mask RCNN : predictions based on pooled feature grides that are assigned heuristically, can be updated since lower level info can be important for final prediction | . Contributions . PANet is proposed for instance segementation | 1) bottom-up path augmentation to shorten information path and improve feature pyramid with accurate low-level information =&gt; new : propagate low-level features to enhance the feature hierarchy for instance recogniton | 2) Adaptive feature pooling is introduced to recover broken information between each proposal and all feature levels | 3) for multi view : augmentation of mask prediction with small fc layers : more diverse info, masks have better quality | 1) &amp; 2) are both used for detection and segmentation and lead to improvements of both tasks | . Framework . . Bottom-up Path Augmentation . Intuition : bottom up is augmented to easily propagate lower layers | We know that high layers respond to whole objects, lower layers to fine features | Localization can be enhanced with top-down paths (FPN) | Here a path from low levels to higher ones is built, based on higher layers respose to edges and instance parts which helps localization error | Approach follow FPN, also using ResNet : layers with same spatial size are in same feature space (b) in figure 1) | As shown in figure 2), each feature map takes a higher resolution feature map $N_{i}$ and a coarser map $P_{i+1}$ and generates a new one using a 3x3 conv with stride 2 for size reduction on each $N_{i}$ map. After that each element of $P_{i+1}$ and the down sampled map are added using lateral connection. The fused map is convoluted using another 3x3 kernel to generate $N_{i+1}$, the whole process is iterated until $P_{5}$ is reached. All convs are followed by a ReLU. | up to 0.9 mAP improvement with large input sizes | . . Adaptive Feature Pooling . Idea : Adapative feature pooling allows each propsal to access info from all levels | In FPN, small proposals are assigned to low level features and high proposals to higher level ones . This can be non-optimal, as e.g. 2 examples with 10-pixel difference can be assigned to different levels even though they are rather similar. Also features may not correlate strongly with the layer they belong to. | High-level features have a larger receptive field and a more global context, whereas lower ones have fine details and high-localization accuracy. Therefore pooling from all levels and all proposals is fused for prediction. The authors call it adaptive feature pooling. For fusion max operations are used. For each level a ratio of kept features is calculated, surprisingly almost 70% are from other higher levels. The findings show that, features from multi levels together are helpful for accurate prediction. An intuition that is similar to DenseNet.This also supports bottom-up augmentation. | The exact process can be seen in figure 1 c), at first each proposal is mapped to different feature levels. Then ROIAlign is used to pool grids from each level. After that fusion of feature grids from different levels is performed using an element-wise max or sum. The focus is on in net feature hierarchy, instead of using different levels from image pyramids. It is comparable to L2 norm, where concat and dimension reduction are used. | . Fully Connected Fusion . Results have shown that MLP is also useful for pixelwise mask prediction. FC layers have different properties compared to FCN, FCN shares parameters and predicts based on local receptifve field. FC layers are localy sensitive since they do not use param sharing. They can adapt to different spatial locations. Using these ideas, it is helpful to differentiate parts in an image and use this information by fusing them. | Mask branch operates on pooled features and mainly consists of a FCN net with 4 conv (each 3x3 with 256 filters) and 1 deconv layer (upsample by 2). A shortcut from layer conv3 to fc is also implemented. The fc layer predicts a class agnostic foreground/background mask. It’s efficient and allows for better generality. | up to 0.7 mAP improvement for all scales | . Others . MultiGPU Sync Batch Norm | Heavier head : effective for box AP | multi scale training | . Total improvement in AP is 4.4 over baseline, half of it is due to Synch BN and multi scale training . Results . Ablation study was done for architecture design | Winner of 2017 COCO Segmentation, SOTA performance in segmentation (Cityscapes) and detection | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/27/PANet.html",
            "relUrl": "/markdown/2020/06/27/PANet.html",
            "date": " • Jun 27, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "YOLOv4, a summary",
            "content": "YOLOv4: Optimal Speed and Accuracy of Object Detection Paper Summary . (Alexey Bochkovskiy,Chien-Yao Wang, Hong-Yuan Mark Liao) . What did the authors want to achieve ? . fast (real time) object detection, that can be trained on a GPU with 8-16 GB of VRAM | a model that can be easily trained and used | add state of the art methods for object detection, building on YOLOv3 | find a good model for both GPU and VPU implementation | . . Methods used . Bag of Freebies (methods that only increase training runtime and not inference) . New Data Augmentation techniques are used, for photometric and geometric variability : =&gt; Random Erase, Cutout to leave part of image of certain value =&gt; Dropout, Dropblock does the same with the net params | Mixup : mult image augmentation | Style Transfer GAN for texture stability Dataset Bias : . =&gt; focal loss, data imbalance between different classics =&gt; one-hot hard representation =&gt; soft labels BBox regression : . | MSE has x and y independent, also Anchors =&gt; IoU loss =&gt; coverage and area are considered =&gt; scale invariant, not the case with traditional methods =&gt; DIoU and CIoU loss | . Bag of specials (methods that only have a small impact on inference speed, but improve accuracy significantly) . only a small cost of compute during inference, improve accuracy : enlarging receptive field : SPP, ASPP, RFB, SPP originates from Spatial Pyramid Matching =&gt; extract bag of words features - SPP infeasible for FCN nets, as it outputs a 1D feature vector =&gt; YOLOv3 concat of max pooling outputs with kxk kernel size =&gt; larger receptive field of the backbone =&gt; 2.7% higher AP50, 0.5 more compute necessary | ASPP diff. to SPP : max pool of 3x3, dilation of k | RFB : several dilated kxk convs =&gt; 7% more AP, 5.7% more compute Attention module : | . | mainly channelwise and pointwise attention, SE and SAM modules, SAM with no extra cost on GPU =&gt; better for us Feature Integration : | skip connections, hyper-column | channelwise weighting on multi-scale with FPN methods : SFAM, ASFF, BiFPN, … Activation Functions : Mish, Swish, (fully differentiable) PReLu, … Post-processing : | NMS “messy” with occlusions =&gt; DIoU NMS distance : center to BBox screening process | NMS method not necessary in anchor free method | . | . | . Architecture Selection . things to think about : a reference that is good for classification is not always good for object detection, due to the detector needing : higher input size (for small objects) | more layers (higher receptive field to cover larger input) | more parameters (for greater capacity, to detect different sized objects) | . | . =&gt; a detector needs a backbone with more 3x3 convs and more params . . Due to that, CSPDarknet53 seems to be the best choice in theory . The following improvements are done : . SPP additional module for larger receptive field (almost no runtime disadvantage) | PANet path-aggregation neck as param aggregation method instead of FPN from YOLOv3 | YOLOv3 anchor based head is used | DropBlock regularization method | . For single GPU training : . synchBN is not considered : goal is to run on single GPU, thanks guys !! | new data augmentation mosaic (mixes 4 training images), Self Adversarial Training =&gt; detection of objects outside their context SAT : Forward Backward Training : 1) adversarial attack is performed on input 2) neural net is trained to detect and object on this moded image in a normal way | . Also : . optimal hyper-params while applying genetic algos | Cross mini Batch Normalization : mini-batch split within batch | SAM is modified from spatial-wise to pointwise attention, as can be seen below : | . . Architecture Summary . Backbone : CSPDarknet53 | Neck : SPP, PAN | Head : Yolov3 | . Techniques : . Bag of freebies for Backbone : CutMix and Mosaic data augmentation, DropBlock reg., class label smoothing | Bag of freebies for detector : CIoU-loss, CmBN, DropBlock reg., Mosaic data augmentation, SAT, Eliminate gird sensitivity, multi anchors for a single ground truth, cosine annealing, hyperparams, random training shapes | Bag of Specials for Backbone : Mish activation, Cross-stage partial connections (CSP), Multi- input weighted residual connections (MiWRC) | Bag of specials for detector : Mish act., SPP-block, SAM-block, PAN path-aggregation block, DIoU-NMS | . Results . A FPS /mAP (@Iou50) comparison to other detecors can be seen below : . .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/06/24/YOLOv4.html",
            "relUrl": "/markdown/2020/06/24/YOLOv4.html",
            "date": " • Jun 24, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Summary Gradient Descent Optimization Algorithms",
            "content": "What did the authors want to achieve ? . give an overview of the different optimizers that exist and the challenges faced | analyze additional methods that can be used to improve gradient descent | the paper is more geared towards the mathematical understandinf of these optimizers as it does not make performance comparisons | . Key elements (Methods in this case) . In the following all sorts of SGD variants and optimization algorithms will be adressed. The following parameters are universal in the upcoming formulas: . θ : parameters η : learning rate J(θ) : objective function depening on models parameters . SGD Based . Batch Gradient Descent . Formula : . $θ = θ − η · ∇θJ(θ)$ . Code : . for i in range ( nb_epochs ): params_grad = evaluate_gradient ( loss_function , data , params ) params = params - learning_rate * params_grad . This is the vanilla version, it updates all parameters in one update. It is redundant and very slow and can be impossible to compute for large datasets due to memory limitations. It also does not allow for online training. . Stochastic Gradient Descent (SGD) . SGD performs a parameter update for each training example ${x^i}$ and ${y^i}$ respectively and therefore allows online training as well as faster training and the ability to train larger datasets. . Formula : . $θ = θ − η · ∇θJ(θ; {x^i};{y^i})$ . Code : . for i in range ( nb_epochs ): np . random . shuffle ( data ) for example in data : params_grad = evaluate_gradient ( loss_function , example , params ) params = params - learning_rate * params_grad . SGD also has the potential to reach better local minima, convergance is compilcated however but can be combated with weight decay,almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively. As is common for most batch gradient descent techniques the data is also shuffled in the beginning, the advantages of shuffling are discussed in the additional strategies section. . Mini-Batch Gradient Descent (most commonly refered to as the actual SGD) . Mini-Batch Gradient Descent takes the best of both worlds and performs updates on smaller batches (common sizes range between 16 and 1024). . Formula : . $θ = θ − η · ∇θJ(θ; {x^{i:i+n}};{y^{i:i+n}})$ . Code : . for i in range ( nb_epochs ): np . random . shuffle ( data ) for batch in get_batches ( data , batch_size =50): params_grad = evaluate_gradient ( loss_function , batch , params ) params = params - learning_rate * params_grad . It is usually refered to as SGD. This technique reduces the variance, which leads to better convergence. It can also make use of highly computationally optimized matrix operations to make computing the gradient of a mini-batch very fast. . Challenges of SGD . Vanilla SGD does not guarantee good convergence, so the following challenges have to be adressed : . choosing the learning rate is very difficult, small values can lead to very slow convergence while large ones can lead to divergence | techniques that combat this like annealing and scheduling of the $ eta$ are used to combat this, however they do not adjust to the dataset used and have to be specified in advance | the learning rate is fixed for all parameters, however we would like to updates each parameter depending on the occurence of features (e.g. higher lr for rarer features) | minimizing non-convex functions, where the convex criterion : | . $f( lambda*x_{1}+(1- lambda)*x_{2}) { leq } lambda*f(x_{1}) + (1- lambda) * f(x_{2})$ . is not fullfilled, often leads to convergence in local minima. Often this is due to saddle points as Dauphin et al. found out. . Gradient descent optimization algorithms . In the following widely used optimization algorithms will be adressed. The following parameters are universal in the upcoming formulas: . θ : parameters η : learning rate J(θ) : objective function depening on models parameters $γ$ : fraction that was introduced with Momentum (usually set to 0.9) $t$ (index) : time step $ epsilon$ : smoothing term to avoid division by zero, introduced by Adagrad (usuall set to 1e-8) . Momentum . SGD has trouble navigating ravines (area where surface curves much more steeply in one dimension than in another), SGD often oscillates in these cases while only making small progress. These are common around local optima. Momentum helps accelerate SGD in the relevant direction and dampens these ocsillations by introducing a new $γ$ term that is used to add a fraction of the update vector of the past time step to the current update vector. . Formula : . $vt = γv_{t−1} + η∇θJ(θ)$ $θ = θ − v_{t}$ . An analogy to this is a ball rolling dowhill, it becomes faster on the way (until it reaches its maximum speed due to air resistance, i.e. $γ&lt;1$) . Nesterov accelerated gradient (NGA) . The idea of NGA is that we do not want to blindly go downhill, but rather know when the uphill section starts so we can slow down again. NGA does take this into account by using our momentum term $γv_{t−1}$ to approximate the next time step using $θ − γv_{t}$. (only the gradient is missing for the full update). This allows us to compute the gradient w.r.t the approximate future position of our parameters rather than the current one. . Formula . $vt = γv_{t−1} + η∇θJ(θ-γv_{t−1})$ $θ = θ − v_{t}$ . NAG first makes a jump in the direction of calculated gradient (brown vector) and then makes a correction (green vector) This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks. Now we are able to update e . Results and Conclusion . The authors were able to give a good overview of different SGD techniques and it&#39;s optimizers like Momentum, Nesterov accelerated gradient, Adagrad, Adadelta, RMSprop, Adam, AdaMax, Nadam, as well as different algorithms to optimize asynchronous SGD. Additionally strategies that can improve upon this like curriculum learning, batch norm and early stopping were deployed. .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/27/Gradient.html",
            "relUrl": "/jupyter/2020/04/27/Gradient.html",
            "date": " • Apr 27, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Summary LSUV Paper",
            "content": "What did the authors want to achieve ? . improve training of deep nets | generalize Xavier initalization to activations other than ReLU (Kaiming init.), such as tanh and maxout | . Key elements . LSUV extends orthogonal initalization and consists of two steps : . 1) Fill the weights with Gaussian noise with unit variance 2) Decompose to orthonormal basis with QR or SVD decomposition and replace the weights with one of the components. . LSUV then estimates the variance of each convolution and inner product layer, the variance is scaled to equal one. It is worth mentioning that the batch size is neglactable in wide margins. In total LSUV can be seen as orthonormal initialization with batch norm applied at the first mini-batch. The orthonormal initalization of weights matrices de-correlates layer activations, a batch norm similarity is the unit variance normalization procedure. When compared to traditional batch norm, the results are sufficient and computationally more efficient. (Batch Norm adds about 30% in compute complexity to the system).It is not always possible to normalize the variance with the desired precision due to inconsistencies in data variations. . The pseudocode for LSUV can be seen above, in order to restrict the number of maximum trials (avoid infinite loops) a $T_{max}$ is set. 1-5 iterations are required for the desired variance. . Implementation . An implementation tutorial, powered by fastai can be found here. . Results and Conclusion . CIFAR 10/100 . As we can see the FitNet with LSUV outperforms other techniques, but is virtually the same as orthonormal initalization. SGD was used with a learning rate of 0.01 and weight decay @ epoch 10/150/200 for 230 epochs in total. . Analysis of empircal results . For FitNet-1 the authors did not experience any problems with any of the activation functions that they used (ReLU,maxout,tanh) optimizers (SGD,RMSProp) or initalizaton techniques (Xavier,MSRA,Ortho,LSUV). This is most likely due to the fact that CNNs tolerate a wide range of mediocre inits, only the training time increases. However FitNet-4 was much more difficult to optimize. . Training a FitResNet-4 on CIFAR-10, which tests the initalization with ResNet training &quot;out-of-the-box&quot;, LSUV is proven to be the only initalization technique that leads all nets to converge regardless of the activation function that was used : . LSUV compared to Batch Norm . LSUV can be seen as batch norm of layer output done before the start of training. The authors also prove that putting batch norm after the activation function is proven to work for FitNet-4. . ImageNet training . When training on ImageNet the authors found out that, LSUV reduces the starting flat-loss time from 0.5 epochs to 0.05 for CaffeNet. It also converges faster in the beginning, but is then overtaken by a standard CaffeNet architecture at the 30th epoch and has a 1.3% lower precision in the end. The authors of the paper do not have any explanation for this empirical phenomenon. Especially since in contrast GoogLeNet performed better (0.68 compared to 0.672) . LSUV Timing . The significant part of LSUV is SVD-decomposition of the weight matrices. The compute overhead on top of generating the Gaussian noise (that&#39;s almost instant) is about 3.5 Minutes for CaffeNet, which is very small compared to total training time. . The authors state that the experiments confirm the finding of Romero et al. (2015) that very thin, thus fast and low in parameters, but deep networks obtain comparable or even better performance than wider, but shallower nets. LSUV is fast and the results are almost state-of-the art. .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/26/LSUV.html",
            "relUrl": "/jupyter/2020/04/26/LSUV.html",
            "date": " • Apr 26, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Summary Mixup Augmentation Technique",
            "content": "Problems of existing rechniques : . ERM memorizes, does not generalize -&gt; vulnerable to adversarials | Classic data augmentation is used to define neighbors in a single class, this is mostly hand crafted by humans and does not consider multi class combinations ## What did the authors want to achieve ? | improve on undesirable memorization of corrupted labels and sensitivity to adversarial examples | stabilize training (especially of GANs) | . Key elements . The Expected Risk (ER), assuming that the joint distribution between a random input $X$ and a label $Y$ $P(X,Y)$ is an empirical distribution : $P_{ delta}(x,y) = frac{1}{n} sum_{i=1}^{n} delta(x=x_{i},y=y_{i})$ . We can infer the approximation of the expected risk $R_{ delta}(f)$ since $dP_{ delta}(x,y) = 1$, and the loss $l(f(x),y) = sum_{i=1}^{n}l(f(x_{i}),y_{i})$ : . $R_{ delta}(f) = int l(f(x),y)*dP_{ delta}(x,y)= sum_{i=1}^{n}l(f(x_{i}),y_{i})$ . The Empirical Risk Minimization (ERM) (Vapnik,1998) is known as learning our function $f$ by minimizing the loss. $P_{ delta}$ is a naive estimation as it is one of many possible choices. The paper also mentions Vicinal Risk Minimization (Chapelle et al., 2000) which assumes the distribution P to be a sum of all the inputs and labels over a vicinity distribution $v( tilde{x}, tilde{y}|x_{i},y_{i})$ and measures the probability of finding the virtual feature target pair $( tilde{x}, tilde{y})$ in the vicinity of the training feature-target pair $(x_{i},y_{i})$. The approach considered Gaussian vicinities, which is equal to augmenting the training data with additive Gaussian noise. Considering a Dataset of size m, we can infer the empirical vicinal risk : . $R_{v}(f) = frac{1}{m} sum_{i=1}^{m}l(f( tilde{(x_{i}}), tilde{y_{i}})$ . This paper introduces a different generic vicinal distribution, called mixup : . With $ lambda = [0,1]$ and $( tilde{x_{i}}, tilde{y_{i}})$ &amp; $( tilde{x_{j}}, tilde{y_{j}}) being 2 random target vectors$. The mixup parameter $ alpha$ controls the strength of interpolation between the pair, as $ alpha rightarrow 0$ it increasingly recovers to the ERM principle. . The implementation from the paper is relatively straightforward : . for (x1, y1), (x2, y2) in zip(loader1, loader2): lam = numpy.random.beta(alpha, alpha) x = Variable(lam * x1 + (1. - lam) * x2) y = Variable(lam * y1 + (1. - lam) * y2) optimizer.zero_grad() loss(net(x), y).backward() optimizer.step() . What does it do ? Mixup makes the model behave linearly in-between classes/examples, this reduces the amount of oscillations when facing a new example that is outside of the training examples. This happens linearily and is simple and therefore a good bias from the Occam&#39;s razor point of view ( &quot;Entities should not be multiplied without necessity.&quot;). . Results and Conclusion . The authors prove that mixup is a very effective technique across all domains. . Classification . Experiments were made on both ImageNet and CIFAR-10/100 using different alphas for each dataset. $ alpha[0.1,0.4]$ for ImageNet and $ alpha = 1$ for CIFAR-10/100. Comparisons were made using different DenseNet and ResNet models. You can refer to the paper for the exact hyperparameters used in each case. As we can see in the graph above, miuxp outperforms their non-mixup counterparts. Learning rate decays were used @ epochs 10/60/120/180 with an inital value of 0.1 and training was done for a total of 300 epochs. . Speech Data . The google command dataset was used which contains 30 classes of 65000 examples. LeNet and VGG-11 were used with ADAM and a learning rate of 0.001 and mixup variants with alphas of 0.1 and 0.2 were compared to the ERM counterparts. Mixup was able to outperform ERM with VGG-11 which had the lowest error of all models (3.9 percent) on the validation set. LeNet however performed better without mixup applied. Comparing these outcomes, the authors infer that mixuxp works particularly well with higher capacity models. . . Memorization of corrupted labels (Table 2) . Robustness to corrupted labels is compared to ERM and mixup. An updated version of CIFAR is used where 20%,50% and then 80% are replaced by random noise. Usually Dropout was considered to be the best technique for corrupted label learning, so dropout with p ∈ {0.5, 0.7, 0.8, 0.9} and mixup are compared to each other along with a combo of both where α ∈ {1, 2, 4, 8} and p ∈ {0.3, 0.5, 0.7}. The PreAct ResNet-18 (He et al., 2016) model implemented in (Liu, 2017) was used. The model was trained for 200 epochs. . Robustness to adversarial example (Table 3) . Several methods were proposed, such as penalizing the norm of the Jacobian of the model to control the Lipschitz constand of it. Other approaches perform data augmentation on adversarial examples. All of these methods add a lot of compute overhead to ERM. Here the authors prove that mixup can improve on ERM significantly without adding significant compute overhead by penalizing the norm of the gradient of the loss wrt a given input along the most plausible directions (the directions of other training points). ResNet101 models were trained 2 models were trained on ImageNet with ERM and one was trained with mixup. White box attacks are tested at first . For that, the model itself is used to generate adversarial examples fusing FGSM or iterative FGSM methods, 10 iterations with equal step sizes are used, $ epsilon=4$ was set as maximum perturbation for each pixel. Secondly black box attacks are done, this is achieved by using the first ERM model to produce adversarials with FGSM and I-FGSM. Then the robustness of the second ERM model and the mixp model to these exampels is tested. . As we can see in table 3, mixup outperforms ERM in both cases significantly, being p to 2.7 times better when it comes to Top-1 error in the FGSM white box attack category. . . Tabular Data (Table 4) . A series of experiments was performed on the UCI dataset. A FCN with 2 hidden layers &amp; 128 ReLU units was trained for 10 epochs with Adam and a batch size of 16. Table 4 shows that mixup improves test error significantly. . Stabilize GAN Training (Table 5) . When training GANs, the mathematical goal is to solve the optimization problem : . . the binary cross entropy loss is used in this case. Solving this equation is a very difficult optimization problem (Goodfellow et al., 2016) as the discriminator often provides the generator with vanishing gradients. Using mixup, the optimization problem looks like this : . The models are fully connected, with 3 hidden layers and 512 ReLU units. The generator accepts 2D Gaussian noise vectors, 20000 mini-batches of size 128 are used for training with the discriminator being trained for 5 epochs before every generator iteration. Table 5 shows the improved performance of mixup. . A part of these mostly supervised use cases (except for GANs), the authors believe that other non straightforward use cases such as segmentation or different unsupervised techniques should be promising areas of future research. .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/25/Mixup.html",
            "relUrl": "/jupyter/2020/04/25/Mixup.html",
            "date": " • Apr 25, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Summary Bag of Tricks for Image Classification with Convolutional Neural Networks",
            "content": "What did the authors want to achieve ? . explore refinements (Learning Rate, FP Precision,...) to improve Conv Nets | slight architecture improvements on ResNet to increase accuracy (stride, filter size, pooling) | prove the results on well known datasets (ImageNet) | . Key elements . Training Loop pseudocode : . Training . The training/model builds on a &quot;vanilla&quot; training loop above, the ResNet architecture which is then improved, starting with these steps : . 1) decode random image to FP32 in a range from [0,255] 2) crop random area with between [8%,100%] of total pixels and a of ration $4/3$ or $3/4$, then rescale to 224x224 3) flip horizontally with probability of 0.5 4) scale hue, saturation and brightness by randomly drawing from [0.6,1.4] 5) Add PCA Noise w/coefficient randomly sampled from ~ $N(0,0.1)$ 6) normalize RGB : substract 123.68/116.779/103.939 and divide by 58.393/57.12/57.375 . | . The CNN is initalized using Xavier Init and uniformally initalizing the weights from [-a,a] with $a = sqrt{6 / (d_{in} + d_{out})}$ with the two $d$ values corresponding ti input and output filter size. Biases are initalized to 0 as well the Batch Norm vector $ beta $, $ gamma$ vectors are initalized to 1. Nesterov acclerated Momentum is used with 120 epochs and a total batch size of 256, the learning rate is initalized to 0.1 and cut by 10 at the 30th, 60th and 90th epoch. . Large Batch Size, linear scaling learning rate . In the past using large Batch sizes has been difficult (degradation), as larger batch sizes posses less noise, the variance is smaller than on small batch sizes. 4 tricks improve upon this problem : . Linear Scaling Learning Rate : . Linear scaling learning rate was proposed by Goyal et. al, the paper proposes that linear scaling the learning rate with batch size empirically improves ResNet training. He et. al built on this and chooses 0.1 as initial learning rate for batch size of 256. Assuming our batch size is $b$ we scale like this : $0.1 * b/256$ | . Learning Rate Warmup : . at the beginning all parameters are typically random values, therfore large learning rates can lead to numerical instability.Goyal et. al proposes a gradual learning rate increase to combat this. For that $m$ warmup epochs are selected, if our inital learning rate is $ eta$, our value depending on epoch $i$ is : $i* eta/(m)$ | . Zero $ gamma$: . Residual Blocks can consist of Batch Norm layers at the output. Initalizing the $ gamma$ parameters of the Batch Norm layers to 0, mimics smaller networks which makes the parameters easier to initalize. That is achieved because when using 0 for intialization only the input (shortcut connection that is passed to the end of the block) is learned when both $ gamma$ and $ beta$ are zero at the inital stage. | . No bias decay : . Weight decay is only applied to the weights and not the biases as well as the two Batch Norm hyperparamters, this avoids overfitting. | . Low Precision Training . In order to improve training time (from 13.3 minutes w/ batch size of 256 per epoch to 4.4 minutes per epoch with batch size of 1024 for ResNet-50), FP16 is used to store activations and gradients, however copies are made in FP32 for parameter updates. Optionally multiplying a scalar to the loss can make up for the lower range of FP16. . Over the year several improvements to the classic ResNet where introduced (picture above). The paper also introduces a new improvement. . Architecture Improvements . ResNet-B . Resnet-B researchers found out that &quot;ResNet-A&quot; (the original version) ignores three quarters of the input map, due to a 1x1 conv with a stride of 2 in the beginning. Due to that the researchers switched the stride of 2 between the first two layers in Path A. . ResNet-C . This version only changes the start of the network. The computation of a 7x7 Kernel Convolution is 5.4 times slower as a 3x3 vonvolution, that&#39;s why the 7x7 Kernel with stride 2 was replaced with 3x3 Kernels (the first two are with 32 filters and stride of 2/1, while the last one has 64 filters and stride of 1). . ResNet-D . The approach proposed in the paper focuses on the same approach as the ResNet-B researchers did, but in this case Path B is changed. Path B also ignores 3/4 of input due to a 1x1 convolution used at the start of Path B. The paper implements an average Pooling Layer of size 2x2 with stride of 2, before the convolution (now with stride 1). The compute cost is very small. Experiments show that, ResNet-D only needs 15% more compute with a 3% lower throughput than the vanilla architecture. . Training Refinements . Cosine Learning Rate Decay . Cosine annealing (1) is used to decay the learning rate (compared to divsion by 10 every 30 epochs in He et. al). $T$ is the total number of batches. Compared to the step decay, the cosine decay starts to decay the learning since the beginning, but remains large until step decay reduces the learning rate by 10x, which potentially improves the training progress. . Label Smoothing . During training we are minimzing Cross Entropy Loss : , where $q_{i}$ is the Softmax output . Looking at the loss (refer to the paper), it encourages scores to be dramtically distinctive from others. Therefore label smoothing was introduced with Inception-v2 : . Which can generalize better, due to a finite output that is encouraged from the fully connected layer and can generalize better. In the experiments $ epsilon$ is set to 0.1 following Szegedy et al. . Knowledge Distillation . With this approach a pretrained teacher model, teaches a model that has to be trained still. For example a ResNet-152 teaches a ResNet-50. A distillation loss (negative Cross Entropy loss) is used to compare the two Softmax outputs. The total loss then changes to : , where $p$ is the true probability distribution and $z$ and $r$ are the student and learner outputs. T is set 20 here, for a pretrained ResNet-152-D model with cosine decay and label smoothing applied. . Mixup Training . Mixup is another training refinement, according to Jeremy Howard from fastai it could be better than the other augmentation techniques and is also multidomain. Mixup samples 2 images in this case and interpolates between them (using weighted interpolation) : . During the experiments $ alpha$ is set to 0.2 in the Beta Distribution. The # of epochs is increased from 120 to 200. Using mixup and distillation, the teacher model can be trained with mixup as well. . Results and Conclusions . Results can be seen in the picture above. FP16 further improves training by 0.5% and the ResNet-D approach improves accuracy by 1% over the standard approach. . Object Detection . A VGG-19 Faster-RCNN model is trained with Detectron refinements such as linear warmup and long training schedule. Using destill with mixup the mAP can be improved from 77.54 to 81.33. . . Semantic Segmentation . A FCN model is used pre trained on the ADE20K dataset. Both pixel accuracy and mean intersection over union improved. .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/22/Bagoftricks.html",
            "relUrl": "/jupyter/2020/04/22/Bagoftricks.html",
            "date": " • Apr 22, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Fastai Course DL from the Foundations ULMFIT",
            "content": "Fastai ULMFit . This Post is based on the Notebok by the Fastai Course Part2 | . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_12a import * . . Data . We load the data from 12a, instructions to create that file are there if you don&#39;t have it yet so go ahead and see. . Jump_to lesson 12 video . #collapse path = datasets.untar_data(datasets.URLs.IMDB) . . #collapse ll = pickle.load(open(path/&#39;ll_lm.pkl&#39;, &#39;rb&#39;)) . . #collapse bs,bptt = 128,70 data = lm_databunchify(ll, bs, bptt) . . #collapse vocab = ll.train.proc_x[1].vocab . . Finetuning the LM . Before tackling the classification task, we have to finetune our language model to the IMDB corpus. . We have pretrained a small model on wikitext 103 that you can download by uncommenting the following cell. . # ! wget http://files.fast.ai/models/wt103_tiny.tgz -P {path} # ! tar xf {path}/wt103_tiny.tgz -C {path} . #collapse dps = tensor([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.5 tok_pad = vocab.index(PAD) . . #collapse_show emb_sz, nh, nl = 300, 300, 2 model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, *dps) . . #collapse_show old_wgts = torch.load(path/&#39;pretrained&#39;/&#39;pretrained.pth&#39;) old_vocab = pickle.load(open(path/&#39;pretrained&#39;/&#39;vocab.pkl&#39;, &#39;rb&#39;)) . . In our current vocabulary, it is very unlikely that the ids correspond to what is in the vocabulary used to train the pretrain model. The tokens are sorted by frequency (apart from the special tokens that are all first) so that order is specific to the corpus used. For instance, the word &#39;house&#39; has different ids in the our current vocab and the pretrained one. . #collapse_show idx_house_new, idx_house_old = vocab.index(&#39;house&#39;),old_vocab.index(&#39;house&#39;) . . We somehow need to match our pretrained weights to the new vocabulary. This is done on the embeddings and the decoder (since the weights between embeddings and decoders are tied) by putting the rows of the embedding matrix (or decoder bias) in the right order. . It may also happen that we have words that aren&#39;t in the pretrained vocab, in this case, we put the mean of the pretrained embedding weights/decoder bias. . #collapse_show house_wgt = old_wgts[&#39;0.emb.weight&#39;][idx_house_old] house_bias = old_wgts[&#39;1.decoder.bias&#39;][idx_house_old] . . #collapse_show def match_embeds(old_wgts, old_vocab, new_vocab): wgts = old_wgts[&#39;0.emb.weight&#39;] bias = old_wgts[&#39;1.decoder.bias&#39;] wgts_m,bias_m = wgts.mean(dim=0),bias.mean() new_wgts = wgts.new_zeros(len(new_vocab), wgts.size(1)) new_bias = bias.new_zeros(len(new_vocab)) otoi = {v:k for k,v in enumerate(old_vocab)} for i,w in enumerate(new_vocab): if w in otoi: idx = otoi[w] new_wgts[i],new_bias[i] = wgts[idx],bias[idx] else: new_wgts[i],new_bias[i] = wgts_m,bias_m old_wgts[&#39;0.emb.weight&#39;] = new_wgts old_wgts[&#39;0.emb_dp.emb.weight&#39;] = new_wgts old_wgts[&#39;1.decoder.weight&#39;] = new_wgts old_wgts[&#39;1.decoder.bias&#39;] = new_bias return old_wgts . . #collapse_show wgts = match_embeds(old_wgts, old_vocab, vocab) . . Now let&#39;s check that the word &quot;house&quot; was properly converted. . #collapse_show test_near(wgts[&#39;0.emb.weight&#39;][idx_house_new],house_wgt) test_near(wgts[&#39;1.decoder.bias&#39;][idx_house_new],house_bias) . . We can load the pretrained weights in our model before beginning training. . #collapse_show model.load_state_dict(wgts) . . If we want to apply discriminative learning rates, we need to split our model in different layer groups. Let&#39;s have a look at our model. . #collapse_show model . . SequentialRNN( (0): AWD_LSTM( (emb): Embedding(60003, 300, padding_idx=2) (emb_dp): EmbeddingDropout( (emb): Embedding(60003, 300, padding_idx=2) ) (rnns): ModuleList( (0): WeightDropout( (module): LSTM(300, 300, batch_first=True) ) (1): WeightDropout( (module): LSTM(300, 300, batch_first=True) ) ) (input_dp): RNNDropout() (hidden_dps): ModuleList( (0): RNNDropout() (1): RNNDropout() ) ) (1): LinearDecoder( (output_dp): RNNDropout() (decoder): Linear(in_features=300, out_features=60003, bias=True) ) ) . Then we split by doing two groups for each rnn/corresponding dropout, then one last group that contains the embeddings/decoder. This is the one that needs to be trained the most as we may have new embeddings vectors. . #collapse_show def lm_splitter(m): groups = [] for i in range(len(m[0].rnns)): groups.append(nn.Sequential(m[0].rnns[i], m[0].hidden_dps[i])) groups += [nn.Sequential(m[0].emb, m[0].emb_dp, m[0].input_dp, m[1])] return [list(o.parameters()) for o in groups] . . First we train with the RNNs freezed. . #collapse_show for rnn in model[0].rnns: for p in rnn.parameters(): p.requires_grad_(False) . . #collapse_show cbs = [partial(AvgStatsCallback,accuracy_flat), CudaCallback, Recorder, partial(GradientClipping, clip=0.1), partial(RNNTrainer, α=2., β=1.), ProgressCallback] . . #collapse_show learn = Learner(model, data, cross_entropy_flat, opt_func=adam_opt(), cb_funcs=cbs, splitter=lm_splitter) . . #collapse_show lr = 2e-2 cbsched = sched_1cycle([lr], pct_start=0.5, mom_start=0.8, mom_mid=0.7, mom_end=0.8) . . #collapse_show learn.fit(1, cbs=cbsched) . . Then the whole model with discriminative learning rates. . #collapse_show for rnn in model[0].rnns: for p in rnn.parameters(): p.requires_grad_(True) . . #collapse_show lr = 2e-3 cbsched = sched_1cycle([lr/2., lr/2., lr], pct_start=0.5, mom_start=0.8, mom_mid=0.7, mom_end=0.8) . . #collapse_show learn.fit(10, cbs=cbsched) . . We only need to save the encoder (first part of the model) for the classification, as well as the vocabulary used (we will need to use the same in the classification task). . #collapse_show torch.save(learn.model[0].state_dict(), path/&#39;finetuned_enc.pth&#39;) . . #collapse_show pickle.dump(vocab, open(path/&#39;vocab_lm.pkl&#39;, &#39;wb&#39;)) . . #collapse_show torch.save(learn.model.state_dict(), path/&#39;finetuned.pth&#39;) . . Classifier . We have to process the data again otherwise pickle will complain. We also have to use the same vocab as the language model. . Jump_to lesson 12 video . #collapse_show vocab = pickle.load(open(path/&#39;vocab_lm.pkl&#39;, &#39;rb&#39;)) proc_tok,proc_num,proc_cat = TokenizeProcessor(),NumericalizeProcessor(vocab=vocab),CategoryProcessor() . . #collapse_show il = TextList.from_files(path, include=[&#39;train&#39;, &#39;test&#39;]) sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=&#39;test&#39;)) ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat) . . #collapse_show pickle.dump(ll, open(path/&#39;ll_clas.pkl&#39;, &#39;wb&#39;)) . . #collapse_show ll = pickle.load(open(path/&#39;ll_clas.pkl&#39;, &#39;rb&#39;)) vocab = pickle.load(open(path/&#39;vocab_lm.pkl&#39;, &#39;rb&#39;)) . . #collapse_show bs,bptt = 64,70 data = clas_databunchify(ll, bs) . . Ignore padding . We will those two utility functions from PyTorch to ignore the padding in the inputs. . #collapse from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence . . Let&#39;s see how this works: first we grab a batch of the training set. . #collapse_show x,y = next(iter(data.train_dl)) . . #collapse_show x.size() . . We need to pass to the utility functions the lengths of our sentences because it&#39;s applied after the embedding, so we can&#39;t see the padding anymore. . #collapse_show lengths = x.size(1) - (x == 1).sum(1) lengths[:5] . . #collapse_show tst_emb = nn.Embedding(len(vocab), 300) . . #collapse_show tst_emb(x).shape . . #collapse_show 128*70 . . We create a PackedSequence object that contains all of our unpadded sequences . #collapse_show packed = pack_padded_sequence(tst_emb(x), lengths, batch_first=True) . . #collapse_show packed . . #collapse_show packed.data.shape . . #collapse_show len(packed.batch_sizes) . . #collapse_show 8960//70 . . This object can be passed to any RNN directly while retaining the speed of CuDNN. . #collapse_show tst = nn.LSTM(300, 300, 2) . . #collapse_show y,h = tst(packed) . . Then we can unpad it with the following function for other modules: . #collapse_show unpack = pad_packed_sequence(y, batch_first=True) . . #collapse_show unpack[0].shape . . #collapse_show unpack[1] . . We need to change our model a little bit to use this. . #collapse_show class AWD_LSTM1(nn.Module): &quot;AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.&quot; initrange=0.1 def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5): super().__init__() self.bs,self.emb_sz,self.n_hid,self.n_layers,self.pad_token = 1,emb_sz,n_hid,n_layers,pad_token self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token) self.emb_dp = EmbeddingDropout(self.emb, embed_p) self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1, batch_first=True) for l in range(n_layers)] self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns]) self.emb.weight.data.uniform_(-self.initrange, self.initrange) self.input_dp = RNNDropout(input_p) self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)]) def forward(self, input): bs,sl = input.size() mask = (input == self.pad_token) lengths = sl - mask.long().sum(1) n_empty = (lengths == 0).sum() if n_empty &gt; 0: input = input[:-n_empty] lengths = lengths[:-n_empty] self.hidden = [(h[0][:,:input.size(0)], h[1][:,:input.size(0)]) for h in self.hidden] raw_output = self.input_dp(self.emb_dp(input)) new_hidden,raw_outputs,outputs = [],[],[] for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)): raw_output = pack_padded_sequence(raw_output, lengths, batch_first=True) raw_output, new_h = rnn(raw_output, self.hidden[l]) raw_output = pad_packed_sequence(raw_output, batch_first=True)[0] raw_outputs.append(raw_output) if l != self.n_layers - 1: raw_output = hid_dp(raw_output) outputs.append(raw_output) new_hidden.append(new_h) self.hidden = to_detach(new_hidden) return raw_outputs, outputs, mask def _one_hidden(self, l): &quot;Return one hidden state.&quot; nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz return next(self.parameters()).new(1, self.bs, nh).zero_() def reset(self): &quot;Reset the hidden states.&quot; self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)] . . Concat pooling . We will use three things for the classification head of the model: the last hidden state, the average of all the hidden states and the maximum of all the hidden states. The trick is just to, once again, ignore the padding in the last element/average/maximum. . Jump_to lesson 12 video . #collapse_show class Pooling(nn.Module): def forward(self, input): raw_outputs,outputs,mask = input output = outputs[-1] lengths = output.size(1) - mask.long().sum(dim=1) avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1) avg_pool.div_(lengths.type(avg_pool.dtype)[:,None]) max_pool = output.masked_fill(mask[:,:,None], -float(&#39;inf&#39;)).max(dim=1)[0] x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling. return output,x . . #collapse_show emb_sz, nh, nl = 300, 300, 2 tok_pad = vocab.index(PAD) . . #collapse_show enc = AWD_LSTM1(len(vocab), emb_sz, n_hid=nh, n_layers=nl, pad_token=tok_pad) pool = Pooling() enc.bs = bs enc.reset() . . #collapse_show x,y = next(iter(data.train_dl)) output,c = pool(enc(x)) . . We can check we have padding with 1s at the end of each text (except the first which is the longest). . #collapse_show x . . PyTorch puts 0s everywhere we had padding in the output when unpacking. . test_near((output.sum(dim=2) == 0).float(), (x==tok_pad).float()) . So the last hidden state isn&#39;t the last element of output. Let&#39;s check we got everything right. . #collapse_show for i in range(bs): length = x.size(1) - (x[i]==1).long().sum() out_unpad = output[i,:length] test_near(out_unpad[-1], c[i,:300]) test_near(out_unpad.max(0)[0], c[i,300:600]) test_near(out_unpad.mean(0), c[i,600:]) . . Our pooling layer properly ignored the padding, so now let&#39;s group it with a classifier. . #collapse_show def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None): layers = [nn.BatchNorm1d(n_in)] if bn else [] if p != 0: layers.append(nn.Dropout(p)) layers.append(nn.Linear(n_in, n_out)) if actn is not None: layers.append(actn) return layers . . #collapse_show class PoolingLinearClassifier(nn.Module): &quot;Create a linear classifier with pooling.&quot; def __init__(self, layers, drops): super().__init__() mod_layers = [] activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None] for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], drops, activs): mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn) self.layers = nn.Sequential(*mod_layers) def forward(self, input): raw_outputs,outputs,mask = input output = outputs[-1] lengths = output.size(1) - mask.long().sum(dim=1) avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1) avg_pool.div_(lengths.type(avg_pool.dtype)[:,None]) max_pool = output.masked_fill(mask[:,:,None], -float(&#39;inf&#39;)).max(dim=1)[0] x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling. x = self.layers(x) return x . . Then we just have to feed our texts to those two blocks, (but we can&#39;t give them all at once to the AWD_LSTM or we might get OOM error: we&#39;ll go for chunks of bptt length to regularly detach the history of our hidden states.) . #collapse_show def pad_tensor(t, bs, val=0.): if t.size(0) &lt; bs: return torch.cat([t, val + t.new_zeros(bs-t.size(0), *t.shape[1:])]) return t . . #collapse_show class SentenceEncoder(nn.Module): def __init__(self, module, bptt, pad_idx=1): super().__init__() self.bptt,self.module,self.pad_idx = bptt,module,pad_idx def concat(self, arrs, bs): return [torch.cat([pad_tensor(l[si],bs) for l in arrs], dim=1) for si in range(len(arrs[0]))] def forward(self, input): bs,sl = input.size() self.module.bs = bs self.module.reset() raw_outputs,outputs,masks = [],[],[] for i in range(0, sl, self.bptt): r,o,m = self.module(input[:,i: min(i+self.bptt, sl)]) masks.append(pad_tensor(m, bs, 1)) raw_outputs.append(r) outputs.append(o) return self.concat(raw_outputs, bs),self.concat(outputs, bs),torch.cat(masks,dim=1) . . #collapse_show def get_text_classifier(vocab_sz, emb_sz, n_hid, n_layers, n_out, pad_token, bptt, output_p=0.4, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5, layers=None, drops=None): &quot;To create a full AWD-LSTM&quot; rnn_enc = AWD_LSTM1(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token, hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p) enc = SentenceEncoder(rnn_enc, bptt) if layers is None: layers = [50] if drops is None: drops = [0.1] * len(layers) layers = [3 * emb_sz] + layers + [n_out] drops = [output_p] + drops return SequentialRNN(enc, PoolingLinearClassifier(layers, drops)) . . #collapse_show emb_sz, nh, nl = 300, 300, 2 dps = tensor([0.4, 0.3, 0.4, 0.05, 0.5]) * 0.25 model = get_text_classifier(len(vocab), emb_sz, nh, nl, 2, 1, bptt, *dps) . . Training . We load our pretrained encoder and freeze it. . Jump_to lesson 12 video . #collapse_show def class_splitter(m): enc = m[0].module groups = [nn.Sequential(enc.emb, enc.emb_dp, enc.input_dp)] for i in range(len(enc.rnns)): groups.append(nn.Sequential(enc.rnns[i], enc.hidden_dps[i])) groups.append(m[1]) return [list(o.parameters()) for o in groups] . . #collapse_show for p in model[0].parameters(): p.requires_grad_(False) . . #collapse_show cbs = [partial(AvgStatsCallback,accuracy), CudaCallback, Recorder, partial(GradientClipping, clip=0.1), ProgressCallback] . . #collapse_show model[0].module.load_state_dict(torch.load(path/&#39;finetuned_enc.pth&#39;)) . . #collapse_show learn = Learner(model, data, F.cross_entropy, opt_func=adam_opt(), cb_funcs=cbs, splitter=class_splitter) . . lr = 1e-2 cbsched = sched_1cycle([lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8) . #collapse_show learn.fit(1, cbs=cbsched) . . #collapse_show for p in model[0].module.rnns[-1].parameters(): p.requires_grad_(True) . . #collapse_show lr = 5e-3 cbsched = sched_1cycle([lr/2., lr/2., lr/2., lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8) . . #collapse_show learn.fit(1, cbs=cbsched) . . #collapse_show for p in model[0].parameters(): p.requires_grad_(True) . . #collapse_show lr = 1e-3 cbsched = sched_1cycle([lr/8., lr/4., lr/2., lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8) . . #collapse_show learn.fit(2, cbs=cbsched) . . #collapse_show x,y = next(iter(data.valid_dl)) . . Predicting on the padded batch or on the individual unpadded samples give the same results. . #collapse_show pred_batch = learn.model.eval()(x.cuda()) . . #collapse_show pred_ind = [] for inp in x: length = x.size(1) - (inp == 1).long().sum() inp = inp[:length] pred_ind.append(learn.model.eval()(inp[None].cuda())) . . #collapse_show assert near(pred_batch, torch.cat(pred_ind)) . .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/21/8ulmfit.html",
            "relUrl": "/jupyter/2020/04/21/8ulmfit.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastai Course DL from the Foundations LSTM Training",
            "content": "Fastai Pretraining on Wikitext 103 . This Post is based on the Notebok by the Fastai Course Part2 | . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_12a import * . . Data . One time download . Jump_to lesson 12 video . #path = datasets.Config().data_path() #version = &#39;103&#39; #2 . #! wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-{version}-v1.zip -P {path} #! unzip -q -n {path}/wikitext-{version}-v1.zip -d {path} #! mv {path}/wikitext-{version}/wiki.train.tokens {path}/wikitext-{version}/train.txt #! mv {path}/wikitext-{version}/wiki.valid.tokens {path}/wikitext-{version}/valid.txt #! mv {path}/wikitext-{version}/wiki.test.tokens {path}/wikitext-{version}/test.txt . Split the articles: WT103 is given as one big text file and we need to chunk it in different articles if we want to be able to shuffle them at the beginning of each epoch. . #collapse path = datasets.Config().data_path()/&#39;wikitext-103&#39; . . #collapse_show def istitle(line): return len(re.findall(r&#39;^ = [^=]* = $&#39;, line)) != 0 . . #collapse_show def read_wiki(filename): articles = [] with open(filename, encoding=&#39;utf8&#39;) as f: lines = f.readlines() current_article = &#39;&#39; for i,line in enumerate(lines): current_article += line if i &lt; len(lines)-2 and lines[i+1] == &#39; n&#39; and istitle(lines[i+2]): current_article = current_article.replace(&#39;&lt;unk&gt;&#39;, UNK) articles.append(current_article) current_article = &#39;&#39; current_article = current_article.replace(&#39;&lt;unk&gt;&#39;, UNK) articles.append(current_article) return articles . . #collapse_show train = TextList(read_wiki(path/&#39;train.txt&#39;), path=path) #+read_file(path/&#39;test.txt&#39;) valid = TextList(read_wiki(path/&#39;valid.txt&#39;), path=path) . . #collapse_show len(train), len(valid) . . #collapse_show sd = SplitData(train, valid) . . #collapse_show proc_tok,proc_num = TokenizeProcessor(),NumericalizeProcessor() . . #collapse_show ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num]) . . #collapse_show pickle.dump(ll, open(path/&#39;ld.pkl&#39;, &#39;wb&#39;)) . . #collapse_show ll = pickle.load( open(path/&#39;ld.pkl&#39;, &#39;rb&#39;)) . . #collapse_show bs,bptt = 128,70 data = lm_databunchify(ll, bs, bptt) . . #collapse_show vocab = ll.train.proc_x[-1].vocab len(vocab) . . Model . #collapse_show dps = np.array([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.2 tok_pad = vocab.index(PAD) . . #collapse_show emb_sz, nh, nl = 300, 300, 2 model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, *dps) . . #collapse_show cbs = [partial(AvgStatsCallback,accuracy_flat), CudaCallback, Recorder, partial(GradientClipping, clip=0.1), partial(RNNTrainer, α=2., β=1.), ProgressCallback] . . #collapse_show learn = Learner(model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt()) . . #collapse_show lr = 5e-3 sched_lr = combine_scheds([0.3,0.7], cos_1cycle_anneal(lr/10., lr, lr/1e5)) sched_mom = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.8, 0.7, 0.8)) cbsched = [ParamScheduler(&#39;lr&#39;, sched_lr), ParamScheduler(&#39;mom&#39;, sched_mom)] . . #collapse_show learn.fit(10, cbs=cbsched) . . #collapse_show torch.save(learn.model.state_dict(), path/&#39;pretrained.pth&#39;) pickle.dump(vocab, open(path/&#39;vocab.pkl&#39;, &#39;wb&#39;)) . .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/21/7lm-pretrain.html",
            "relUrl": "/jupyter/2020/04/21/7lm-pretrain.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Fastai Course DL from the Foundations LSTM RNN",
            "content": "Fastai AWD-LSTM . This Post is based on the Notebok by the Fastai Course Part2 | . In order to avoid writing many layers (number of layers = number of docs for example), that is why a for loop is used. . The same weight matrices are used, so for loops are valid. In order to avoid vanishing gradients for large sequences, LSTMs or GRUs are used. . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_12 import * . . Data . Jump_to lesson 12 video . #collapse path = datasets.untar_data(datasets.URLs.IMDB) . . We have to preprocess the data again to pickle it because if we try to load the previous SplitLabeledData with pickle, it will complain some of the functions aren&#39;t in main. . #collapse il = TextList.from_files(path, include=[&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1)) . . #collapse proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor() . . #collapse ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num]) . . #collapse pickle.dump(ll, open(path/&#39;ll_lm.pkl&#39;, &#39;wb&#39;)) pickle.dump(proc_num.vocab, open(path/&#39;vocab_lm.pkl&#39;, &#39;wb&#39;)) . . #collapse ll = pickle.load(open(path/&#39;ll_lm.pkl&#39;, &#39;rb&#39;)) vocab = pickle.load(open(path/&#39;vocab_lm.pkl&#39;, &#39;rb&#39;)) . . #collapse bs,bptt = 64,70 data = lm_databunchify(ll, bs, bptt) . . AWD-LSTM . Before explaining what an AWD LSTM is, we need to start with an LSTM. RNNs were covered in part 1, if you need a refresher, there is a great visualization of them on this website. . Jump_to lesson 12 video . LSTM from scratch . We need to implement those equations (where $ sigma$ stands for sigmoid): . (picture from Understanding LSTMs by Chris Olah.) . If we want to take advantage of our GPU, it&#39;s better to do one big matrix multiplication than four smaller ones. So we compute the values of the four gates all at once. Since there is a matrix multiplication and a bias, we use nn.Linear to do it. . We need two linear layers: one for the input and one for the hidden state. . #collapse_show class LSTMCell(nn.Module): def __init__(self, ni, nh): super().__init__() self.ih = nn.Linear(ni,4*nh) self.hh = nn.Linear(nh,4*nh) def forward(self, input, state): h,c = state #One big multiplication for all the gates is better than 4 smaller ones gates = (self.ih(input) + self.hh(h)).chunk(4, 1) ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3]) cellgate = gates[3].tanh() c = (forgetgate*c) + (ingate*cellgate) h = outgate * c.tanh() return h, (h,c) . . Then an LSTM layer just applies the cell on all the time steps in order. . #collapse_show class LSTMLayer(nn.Module): def __init__(self, cell, *cell_args): super().__init__() self.cell = cell(*cell_args) def forward(self, input, state): inputs = input.unbind(1) outputs = [] for i in range(len(inputs)): out, state = self.cell(inputs[i], state) outputs += [out] return torch.stack(outputs, dim=1), state . . Now let&#39;s try it out and see how fast we are. We only measure the forward pass. . #collapse_show lstm = LSTMLayer(LSTMCell, 300, 300) . . #collapse_show x = torch.randn(64, 70, 300) h = (torch.zeros(64, 300),torch.zeros(64, 300)) . . CPU . #collapse_show %timeit -n 10 y,h1 = lstm(x,h) . . #collapse_show lstm = lstm.cuda() x = x.cuda() h = (h[0].cuda(), h[1].cuda()) . . #collapse_show def time_fn(f): f() torch.cuda.synchronize() . . CUDA . #collapse_show f = partial(lstm,x,h) time_fn(f) . . #collapse_show %timeit -n 10 time_fn(f) . . Builtin version . Let&#39;s compare with PyTorch! . Jump_to lesson 12 video . #collapse_show lstm = nn.LSTM(300, 300, 1, batch_first=True) . . #collapse_show x = torch.randn(64, 70, 300) h = (torch.zeros(1, 64, 300),torch.zeros(1, 64, 300)) . . CPU . #collapse_show %timeit -n 10 y,h1 = lstm(x,h) . . #collapse_show lstm = lstm.cuda() x = x.cuda() h = (h[0].cuda(), h[1].cuda()) . . #collapse_show f = partial(lstm,x,h) time_fn(f) . . GPU . #collapse_show %timeit -n 10 time_fn(f) . . So our version is running at almost the same speed on the CPU. However, on the GPU, PyTorch uses CuDNN behind the scenes that optimizes greatly the for loop. . Jit version . Jump_to lesson 12 video . #collapse import torch.jit as jit from torch import Tensor . . We have to write everything from scratch to be a bit faster, so we don&#39;t use the linear layers here. . #collapse_show class LSTMCell(jit.ScriptModule): def __init__(self, ni, nh): super().__init__() self.ni = ni self.nh = nh self.w_ih = nn.Parameter(torch.randn(4 * nh, ni)) self.w_hh = nn.Parameter(torch.randn(4 * nh, nh)) self.bias_ih = nn.Parameter(torch.randn(4 * nh)) self.bias_hh = nn.Parameter(torch.randn(4 * nh)) @jit.script_method def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])-&gt;Tuple[Tensor, Tuple[Tensor, Tensor]]: hx, cx = state gates = (input @ self.w_ih.t() + self.bias_ih + hx @ self.w_hh.t() + self.bias_hh) ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1) ingate = torch.sigmoid(ingate) forgetgate = torch.sigmoid(forgetgate) cellgate = torch.tanh(cellgate) outgate = torch.sigmoid(outgate) cy = (forgetgate * cx) + (ingate * cellgate) hy = outgate * torch.tanh(cy) return hy, (hy, cy) . . #collapse_show class LSTMLayer(jit.ScriptModule): def __init__(self, cell, *cell_args): super().__init__() self.cell = cell(*cell_args) @jit.script_method def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])-&gt;Tuple[Tensor, Tuple[Tensor, Tensor]]: inputs = input.unbind(1) outputs = [] for i in range(len(inputs)): out, state = self.cell(inputs[i], state) outputs += [out] return torch.stack(outputs, dim=1), state . . #collapse_show lstm = LSTMLayer(LSTMCell, 300, 300) . . #collapse_show x = torch.randn(64, 70, 300) h = (torch.zeros(64, 300),torch.zeros(64, 300)) . . #collapse_show %timeit -n 10 y,h1 = lstm(x,h) . . #collapse_show lstm = lstm.cuda() x = x.cuda() h = (h[0].cuda(), h[1].cuda()) . . #collapse_show f = partial(lstm,x,h) time_fn(f) . . #collapse_show %timeit -n 10 time_fn(f) . . With jit, we almost get to the CuDNN speed! . Dropout . We want to use the AWD-LSTM from Stephen Merity et al.. First, we&#39;ll need all different kinds of dropouts. Dropout consists into replacing some coefficients by 0 with probability p. To ensure that the average of the weights remains constant, we apply a correction to the weights that aren&#39;t nullified of a factor 1/(1-p) (think of what happens to the activations if you want to figure out why!) . We usually apply dropout by drawing a mask that tells us which elements to nullify or not: . Jump_to lesson 12 video . #collapse_show def dropout_mask(x, sz, p): return x.new(*sz).bernoulli_(1-p).div_(1-p) . . #collapse_show x = torch.randn(10,10) mask = dropout_mask(x, (10,10), 0.5); mask . . Once with have a dropout mask mask, applying the dropout to x is simply done by x = x * mask. We create our own dropout mask and don&#39;t rely on pytorch dropout because we do not want to nullify all the coefficients randomly: on the sequence dimension, we will want to have always replace the same positions by zero along the seq_len dimension. . #collapse_show (x*mask).std(),x.std() . . Inside a RNN, a tensor x will have three dimensions: bs, seq_len, vocab_size. Recall that we want to consistently apply the dropout mask across the seq_len dimension, therefore, we create a dropout mask for the first and third dimension and broadcast it to the seq_len dimension. . #collapse_show class RNNDropout(nn.Module): def __init__(self, p=0.5): super().__init__() self.p=p def forward(self, x): if not self.training or self.p == 0.: return x m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p) return x * m . . #collapse_show dp = RNNDropout(0.3) tst_input = torch.randn(3,3,7) tst_input, dp(tst_input) . . WeightDropout is dropout applied to the weights of the inner LSTM hidden to hidden matrix. This is a little hacky if we want to preserve the CuDNN speed and not reimplement the cell from scratch. We add a parameter that will contain the raw weights, and we replace the weight matrix in the LSTM at the beginning of the forward pass. . #collapse_show import warnings WEIGHT_HH = &#39;weight_hh_l0&#39; class WeightDropout(nn.Module): def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]): super().__init__() self.module,self.weight_p,self.layer_names = module,weight_p,layer_names for layer in self.layer_names: #Makes a copy of the weights of the selected layers. w = getattr(self.module, layer) self.register_parameter(f&#39;{layer}_raw&#39;, nn.Parameter(w.data)) self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False) def _setweights(self): for layer in self.layer_names: raw_w = getattr(self, f&#39;{layer}_raw&#39;) self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training) def forward(self, *args): self._setweights() with warnings.catch_warnings(): #To avoid the warning that comes because the weights aren&#39;t flattened. warnings.simplefilter(&quot;ignore&quot;) return self.module.forward(*args) . . Let&#39;s try it! . #collapse_show module = nn.LSTM(5, 2) dp_module = WeightDropout(module, 0.4) getattr(dp_module.module, WEIGHT_HH) . . It&#39;s at the beginning of a forward pass that the dropout is applied to the weights. . #collapse_show tst_input = torch.randn(4,20,5) h = (torch.zeros(1,20,2), torch.zeros(1,20,2)) x,h = dp_module(tst_input,h) getattr(dp_module.module, WEIGHT_HH) . . EmbeddingDropout applies dropout to full rows of the embedding matrix. . #collapse_show class EmbeddingDropout(nn.Module): &quot;Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.&quot; def __init__(self, emb, embed_p): super().__init__() self.emb,self.embed_p = emb,embed_p self.pad_idx = self.emb.padding_idx if self.pad_idx is None: self.pad_idx = -1 def forward(self, words, scale=None): if self.training and self.embed_p != 0: size = (self.emb.weight.size(0),1) mask = dropout_mask(self.emb.weight.data, size, self.embed_p) masked_embed = self.emb.weight * mask else: masked_embed = self.emb.weight if scale: masked_embed.mul_(scale) return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm, self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse) . . #collapse_show enc = nn.Embedding(100, 7, padding_idx=1) enc_dp = EmbeddingDropout(enc, 0.5) tst_input = torch.randint(0,100,(8,)) enc_dp(tst_input) . . Main model . The main model is a regular LSTM with several layers, but using all those kinds of dropouts. . #collapse_show def to_detach(h): &quot;Detaches `h` from its history.&quot; return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h) . . #collapse_show class AWD_LSTM(nn.Module): &quot;AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.&quot; initrange=0.1 def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5): super().__init__() self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token) self.emb_dp = EmbeddingDropout(self.emb, embed_p) self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1, batch_first=True) for l in range(n_layers)] self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns]) self.emb.weight.data.uniform_(-self.initrange, self.initrange) self.input_dp = RNNDropout(input_p) self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)]) def forward(self, input): bs,sl = input.size() if bs!=self.bs: self.bs=bs self.reset() raw_output = self.input_dp(self.emb_dp(input)) new_hidden,raw_outputs,outputs = [],[],[] for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)): raw_output, new_h = rnn(raw_output, self.hidden[l]) new_hidden.append(new_h) raw_outputs.append(raw_output) if l != self.n_layers - 1: raw_output = hid_dp(raw_output) outputs.append(raw_output) self.hidden = to_detach(new_hidden) return raw_outputs, outputs def _one_hidden(self, l): &quot;Return one hidden state.&quot; nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz return next(self.parameters()).new(1, self.bs, nh).zero_() def reset(self): &quot;Reset the hidden states.&quot; self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)] . . On top of this, we will apply a linear decoder. It&#39;s often best to use the same matrix as the one for the embeddings in the weights of the decoder. . #collapse_show class LinearDecoder(nn.Module): def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True): super().__init__() self.output_dp = RNNDropout(output_p) self.decoder = nn.Linear(n_hid, n_out, bias=bias) if bias: self.decoder.bias.data.zero_() if tie_encoder: self.decoder.weight = tie_encoder.weight else: init.kaiming_uniform_(self.decoder.weight) def forward(self, input): raw_outputs, outputs = input output = self.output_dp(outputs[-1]).contiguous() decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2))) return decoded, raw_outputs, outputs . . #collapse_show class SequentialRNN(nn.Sequential): &quot;A sequential module that passes the reset call to its children.&quot; def reset(self): for c in self.children(): if hasattr(c, &#39;reset&#39;): c.reset() . . And now we stack all of this together! . #collapse_show def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True): rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token, hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p) enc = rnn_enc.emb if tie_weights else None return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias)) . . tok_pad = vocab.index(PAD) . Now we can test this all works without throwing a bug. . #collapse_show tst_model = get_language_model(len(vocab), 300, 300, 2, tok_pad) tst_model = tst_model.cuda() . . #collapse_show x,y = next(iter(data.train_dl)) . . #collapse_show z = tst_model(x.cuda()) . . We return three things to help with regularization: the true output (probabilities for each word), but also the activations of the encoder, with or without dropouts. . len(z) . decoded, raw_outputs, outputs = z . The decoded tensor is flattened to bs * seq_len by len(vocab): . decoded.size() . raw_outputs and outputs each contain the results of the intermediary layers: . #collapse_show len(raw_outputs),len(outputs) . . #collapse_show [o.size() for o in raw_outputs], [o.size() for o in outputs] . . Callbacks to train the model . We need to add a few tweaks to train a language model: first we will clip the gradients. This is a classic technique that will allow us to use a higher learning rate by putting a maximum value on the norm of the gradients. . #collapse_show class GradientClipping(Callback): def __init__(self, clip=None): self.clip = clip def after_backward(self): if self.clip: nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip) . . Then we add an RNNTrainer that will do four things: . change the output to make it contain only the decoded tensor (for the loss function) and store the raw_outputs and outputs | apply Activation Regularization (AR): we add to the loss an L2 penalty on the last activations of the AWD LSTM (with dropout applied) | apply Temporal Activation Regularization (TAR): we add to the loss an L2 penalty on the difference between two consecutive (in terms of words) raw outputs | trigger the shuffle of the LMDataset at the beginning of each epoch | . #collapse_show class RNNTrainer(Callback): def __init__(self, α, β): self.α,self.β = α,β def after_pred(self): #Save the extra outputs for later and only returns the true output. self.raw_out,self.out = self.pred[1],self.pred[2] self.run.pred = self.pred[0] def after_loss(self): #AR and TAR if self.α != 0.: self.run.loss += self.α * self.out[-1].float().pow(2).mean() if self.β != 0.: h = self.raw_out[-1] if h.size(1)&gt;1: self.run.loss += self.β * (h[:,1:] - h[:,:-1]).float().pow(2).mean() def begin_epoch(self): #Shuffle the texts at the beginning of the epoch if hasattr(self.dl.dataset, &quot;batchify&quot;): self.dl.dataset.batchify() . . Lastly we write a flattened version of the cross entropy loss and the accuracy metric. . #collapse_show def cross_entropy_flat(input, target): bs,sl = target.size() return F.cross_entropy(input.view(bs * sl, -1), target.view(bs * sl)) def accuracy_flat(input, target): bs,sl = target.size() return accuracy(input.view(bs * sl, -1), target.view(bs * sl)) . . #collapse_show emb_sz, nh, nl = 300, 300, 2 model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, input_p=0.6, output_p=0.4, weight_p=0.5, embed_p=0.1, hidden_p=0.2) . . #collapse_show cbs = [partial(AvgStatsCallback,accuracy_flat), CudaCallback, Recorder, partial(GradientClipping, clip=0.1), partial(RNNTrainer, α=2., β=1.), ProgressCallback] . . #collapse_show learn = Learner(model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt()) . . #collapse_show learn.fit(1) . .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/21/6awd-lstm.html",
            "relUrl": "/jupyter/2020/04/21/6awd-lstm.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Fastai Course DL from the Foundations NLP Intro",
            "content": "Fastai Preprocess text . This Post is based on the Notebok by the Fastai Course Part2 | . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_11a import * . . Data . We will use the IMDB dataset that consists of 50,000 labeled reviews of movies (positive or negative) and 50,000 unlabelled ones. . Jump_to lesson 12 video . #collapse path = datasets.untar_data(datasets.URLs.IMDB) . . Downloading https://s3.amazonaws.com/fast-ai-nlp/imdb . #collapse path.ls() . . [PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/tmp_clas&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/ld.pkl&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/test&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/train&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/README&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/models&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/tmp_lm&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/imdb.vocab&#39;)] . We define a subclass of ItemList that will read the texts in the corresponding filenames. . #collapse_show def read_file(fn): with open(fn, &#39;r&#39;, encoding = &#39;utf8&#39;) as f: return f.read() class TextList(ItemList): @classmethod def from_files(cls, path, extensions=&#39;.txt&#39;, recurse=True, include=None, **kwargs): return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs) def get(self, i): if isinstance(i, Path): return read_file(i) return i . . Just in case there are some text log files, we restrict the ones we take to the training, test, and unsupervised folders. . #collapse_show il = TextList.from_files(path, include=[&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) . . We should expect a total of 100,000 texts. . #collapse_show len(il.items) . . 100000 . Here is the first one as an example. . #collapse_show txt = il[0] txt . . &#39;Comedian Adam Sandler &#39;s last theatrical release &#34;I Now Pronounce You Chuck and Larry&#34; served as a loud and proud plea for tolerance of the gay community. The former &#34;Saturday Night Live&#34; funnyman &#39;s new movie &#34;You Don &#39;t Mess with the Zohan&#34; (*** out of ****) constitutes his plea for tolerance toward Israeli and Palestinian immigrants in America. These unfortunate people are often punished in America for the crimes of their counterparts in the war-ravaged Middle East. Although &#34;Zohan&#34; advocates a lofty cause, Sandler doesn &#39;t let his political agenda overshadow his usual crude, below-the-belt, juvenile shenanigans that rely on obscene bodily functions, promiscuous sex, and far-fetched harebrained idiocy. Indeed, the hysterical horseplay that Sandler and company revel in may distract you from the plight of these uprooted, misplaced misfits that had fled to Uncle Sam &#39;s shores because they believe America is a Utopia. Interestingly, Sandler plays a Jewish counterterrorist agent of the Mossad, Israel &#39;s secret police, with a hopelessly corny accent. Zohan &#39;s exploits appear to foreshadow Will Smith &#39;s upcoming &#34;Hancock.&#34; Zohan is the best Jewish secret agent in the whole wide world. He is literally indestructible. He catches bullets in his nose. He can swim faster than a dolphin, and a razor-toothed piranha fish in his bikini swim trunks amuses him.&lt;br /&gt;&lt;br /&gt;Zohan (Adam Sandler) is cooking fish at the beach when his superiors interrupt his vacation and inform him that the dreaded Arab terrorist, the Phantom (John Turturro of &#34;Transformers&#34;), is up to his old tricks again. Naturally, Zohan is furious! Actually, Zohan captured the Phantom three months ago, but the politicians have exchanged the Phantom for political prisoners. Now, Zohan must nab his nemesis again! The Phantom and Zohan tangle in a spectacular fight in the sea and Zohan doesn &#39;t survive. In reality, Zohan deliberately fakes his death so that he can immigrate to New York City and realize his life-long dream of cutting hair for Paul Mitchell. Zohan gives himself an obsolete Frankie Avalon haircut, trims his beard, and smuggles himself onto a plane bound for America. If what happens before his flight seems outlandish, once he is on the jet, he spends his time in the cargo hold with two fluffy dogs named &#34;Scrappy&#34; and &#34;Coco.&#34; Zohan styles their hair from photos in his Paul Mitchell haircut book.&lt;br /&gt;&lt;br /&gt;At first, Zohan has no luck getting a job with Paul Mitchell, much less cutting hair. Zohan defends Michael (Nick Swardson of &#34;Reno 911, The Movie&#34;) in a street brawl after a motorist blames Michael for his accident with a delivery truck. A grateful Michael invites Zohan to stay with his mother, Gail (Lainie Kazan of &#34;Dayton &#39;s Devils&#34;), and him. Zohan practices cutting Gail &#39;s hair when he isn &#39;t having in lusty sex with her. Eventually, Zohan gets a job sweeping up hair at a salon owned by Dalia (Emmanuelle Chriqui of &#34;Wrong Turn&#34;) who as it turns out is a Palestinian. Indeed, Zohan knows about her heritage but doesn &#39;t let it bother him. One day when one of Dalia &#39;s hair stylists doesn &#39;t show up, Zohan takes advantage of her absence to cut hair. Much to Dalia &#39;s surprise, Zohan wins the allegiance of the over sixty crowd. Older woman line up around the block to have him fashion their hair. After each session, Zohan takes each older lady in the back and assuages their sexual appetites.&lt;br /&gt;&lt;br /&gt;Meanwhile, a millionaire real estate developer Walbridge (Michael Buffer of &#34;Rocky Balboa&#34;) hikes the rent to force Dalia and others like her out of her store to make way for his mall with a roller-coaster. Zohan surprises both Dalia and Walbridge &#39;s people and forks over the money for her to pay the rent. An angry Walbridge contacts a white supremacy group to ignite a neighborhood war between the Israelis and Palestinians. This happens about the same time that Zohan falls in love with Dalia. Perennial Sandler cohort Rob Schneider of &#34;Deuce Bigalow&#34; appears as a cretinous Palestinian named Salim who doesn &#39;t know the difference between nitroglycerin and Neosporin. He tries to blow up Zohan for an old grudge. It seems Zohan beat Salim up and stole his goat.&lt;br /&gt;&lt;br /&gt;&#34;You Don &#39;t Mess with the Zohan&#34; qualifies as a surreal comedy. Scenarists Robert Smigel of &#34;Saturday Night Live,&#34; Judd Apatow of &#34;The 40-Year Old Virgin,&#34; and Sandler himself vigorously ignore the laws of logic in this zany comedy. The movie that most closely resembles &#34;Zohan&#34; is &#34;Little Nicky,&#34; because both characters boast supernatural abilities. &#34;You Don &#39;t Mess with the Zohan&#34; will keep Adam Sandler fans in stitches.&#39; . For text classification, we will split by the grand parent folder as before, but for language modeling, we take all the texts and just put 10% aside. . #collapse_show sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1)) . . #collapse_show sd . . SplitData Train: TextList (89885 items) [PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/30860_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/36250_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/24690_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/21770_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/9740_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/40778_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/44512_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/22672_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/25946_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/40866_0.txt&#39;)...] Path: /home/jupyter/.fastai/data/imdb Valid: TextList (10115 items) [PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/1041_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/38186_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/16367_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/47167_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/58_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/49861_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/306_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/18238_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/34952_0.txt&#39;), PosixPath(&#39;/home/jupyter/.fastai/data/imdb/unsup/24288_0.txt&#39;)...] Path: /home/jupyter/.fastai/data/imdb . Tokenizing . We need to tokenize the dataset first, which is splitting a sentence in individual tokens. Those tokens are the basic words or punctuation signs with a few tweaks: don&#39;t for instance is split between do and n&#39;t. We will use a processor for this, in conjunction with the spacy library. . Jump_to lesson 12 video . #collapse_show import spacy,html . . Before even tokenizeing, we will apply a bit of preprocessing on the texts to clean them up (we saw the one up there had some HTML code). These rules are applied before we split the sentences in tokens. . #collapse_show UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = &quot;xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj&quot;.split() def sub_br(t): &quot;Replaces the &lt;br /&gt; by n&quot; re_br = re.compile(r&#39;&lt; s*br s*/?&gt;&#39;, re.IGNORECASE) return re_br.sub(&quot; n&quot;, t) def spec_add_spaces(t): &quot;Add spaces around / and #&quot; return re.sub(r&#39;([/#])&#39;, r&#39; 1 &#39;, t) def rm_useless_spaces(t): &quot;Remove multiple spaces&quot; return re.sub(&#39; {2,}&#39;, &#39; &#39;, t) def replace_rep(t): &quot;Replace repetitions at the character level: cccc -&gt; TK_REP 4 c&quot; def _replace_rep(m:Collection[str]) -&gt; str: c,cc = m.groups() return f&#39; {TK_REP} {len(cc)+1} {c} &#39; re_rep = re.compile(r&#39;( S)( 1{3,})&#39;) return re_rep.sub(_replace_rep, t) def replace_wrep(t): &quot;Replace word repetitions: word word word -&gt; TK_WREP 3 word&quot; def _replace_wrep(m:Collection[str]) -&gt; str: c,cc = m.groups() return f&#39; {TK_WREP} {len(cc.split())+1} {c} &#39; re_wrep = re.compile(r&#39;( b w+ W+)( 1{3,})&#39;) return re_wrep.sub(_replace_wrep, t) def fixup_text(x): &quot;Various messy things we&#39;ve seen in documents&quot; re1 = re.compile(r&#39; +&#39;) x = x.replace(&#39;#39;&#39;, &quot;&#39;&quot;).replace(&#39;amp;&#39;, &#39;&amp;&#39;).replace(&#39;#146;&#39;, &quot;&#39;&quot;).replace( &#39;nbsp;&#39;, &#39; &#39;).replace(&#39;#36;&#39;, &#39;$&#39;).replace(&#39; n&#39;, &quot; n&quot;).replace(&#39;quot;&#39;, &quot;&#39;&quot;).replace( &#39;&lt;br /&gt;&#39;, &quot; n&quot;).replace(&#39; &quot;&#39;, &#39;&quot;&#39;).replace(&#39;&lt;unk&gt;&#39;,UNK).replace(&#39; @.@ &#39;,&#39;.&#39;).replace( &#39; @-@ &#39;,&#39;-&#39;).replace(&#39; &#39;, &#39; &#39;) return re1.sub(&#39; &#39;, html.unescape(x)) default_pre_rules = [fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br] default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ] . . #collapse_show replace_rep(&#39;cccc&#39;) . . &#39; xxrep 4 c &#39; . #collapse_show replace_wrep(&#39;word word word word word &#39;) . . &#39; xxwrep 5 word &#39; . These rules are applies after the tokenization on the list of tokens. . #collapse_show def replace_all_caps(x): &quot;Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.&quot; res = [] for t in x: if t.isupper() and len(t) &gt; 1: res.append(TK_UP); res.append(t.lower()) else: res.append(t) return res def deal_caps(x): &quot;Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before.&quot; res = [] for t in x: if t == &#39;&#39;: continue if t[0].isupper() and len(t) &gt; 1 and t[1:].islower(): res.append(TK_MAJ) res.append(t.lower()) return res def add_eos_bos(x): return [BOS] + x + [EOS] default_post_rules = [deal_caps, replace_all_caps, add_eos_bos] . . #collapse_show replace_all_caps([&#39;I&#39;, &#39;AM&#39;, &#39;SHOUTING&#39;]) . . [&#39;I&#39;, &#39;xxup&#39;, &#39;am&#39;, &#39;xxup&#39;, &#39;shouting&#39;] . #collapse_show deal_caps([&#39;My&#39;, &#39;name&#39;, &#39;is&#39;, &#39;Jeremy&#39;]) . . [&#39;xxmaj&#39;, &#39;my&#39;, &#39;name&#39;, &#39;is&#39;, &#39;xxmaj&#39;, &#39;jeremy&#39;] . Since tokenizing and applying those rules takes a bit of time, we&#39;ll parallelize it using ProcessPoolExecutor to go faster. . #collapse_show from spacy.symbols import ORTH from concurrent.futures import ProcessPoolExecutor def parallel(func, arr, max_workers=4): if max_workers&lt;2: results = list(progress_bar(map(func, enumerate(arr)), total=len(arr))) else: with ProcessPoolExecutor(max_workers=max_workers) as ex: return list(progress_bar(ex.map(func, enumerate(arr)), total=len(arr))) if any([o is not None for o in results]): return results . . #collapse_show class TokenizeProcessor(Processor): def __init__(self, lang=&quot;en&quot;, chunksize=2000, pre_rules=None, post_rules=None, max_workers=4): self.chunksize,self.max_workers = chunksize,max_workers self.tokenizer = spacy.blank(lang).tokenizer for w in default_spec_tok: self.tokenizer.add_special_case(w, [{ORTH: w}]) self.pre_rules = default_pre_rules if pre_rules is None else pre_rules self.post_rules = default_post_rules if post_rules is None else post_rules def proc_chunk(self, args): i,chunk = args chunk = [compose(t, self.pre_rules) for t in chunk] docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)] docs = [compose(t, self.post_rules) for t in docs] return docs def __call__(self, items): toks = [] if isinstance(items[0], Path): items = [read_file(i) for i in items] chunks = [items[i: i+self.chunksize] for i in (range(0, len(items), self.chunksize))] toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers) return sum(toks, []) def proc1(self, item): return self.proc_chunk([item])[0] def deprocess(self, toks): return [self.deproc1(tok) for tok in toks] def deproc1(self, tok): return &quot; &quot;.join(tok) . . #collapse_show tp = TokenizeProcessor() . . #collapse_show txt[:250] . . &#39;Comedian Adam Sandler &#39;s last theatrical release &#34;I Now Pronounce You Chuck and Larry&#34; served as a loud and proud plea for tolerance of the gay community. The former &#34;Saturday Night Live&#34; funnyman &#39;s new movie &#34;You Don &#39;t Mess with the Zohan&#34; (*** out o&#39; . #collapse_show &#39; • &#39;.join(tp(il[:100])[0])[:400] . . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:00&lt;00:00] &#39;xxbos • xxmaj • comedian • xxmaj • adam • xxmaj • sandler • &#39;s • last • theatrical • release • &#34; • i • xxmaj • now • xxmaj • pronounce • xxmaj • you • xxmaj • chuck • and • xxmaj • larry • &#34; • served • as • a • loud • and • proud • plea • for • tolerance • of • the • gay • community • . • xxmaj • the • former • &#34; • xxmaj • saturday • xxmaj • night • xxmaj • live • &#34; • funnyman • &#39;s • new • movie •&#39; . Numericalizing . Once we have tokenized our texts, we replace each token by an individual number, this is called numericalizing. Again, we do this with a processor (not so different from the CategoryProcessor). . Jump_to lesson 12 video . #collapse_show import collections class NumericalizeProcessor(Processor): def __init__(self, vocab=None, max_vocab=60000, min_freq=2): self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq def __call__(self, items): #The vocab is defined on the first use. if self.vocab is None: freq = Counter(p for o in items for p in o) self.vocab = [o for o,c in freq.most_common(self.max_vocab) if c &gt;= self.min_freq] for o in reversed(default_spec_tok): if o in self.vocab: self.vocab.remove(o) self.vocab.insert(0, o) if getattr(self, &#39;otoi&#39;, None) is None: self.otoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.vocab)}) return [self.proc1(o) for o in items] def proc1(self, item): return [self.otoi[o] for o in item] def deprocess(self, idxs): assert self.vocab is not None return [self.deproc1(idx) for idx in idxs] def deproc1(self, idx): return [self.vocab[i] for i in idx] . . When we do language modeling, we will infer the labels from the text during training, so there&#39;s no need to label. The training loop expects labels however, so we need to add dummy ones. . #collapse_show proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor() . . #collapse_show %time ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num]) . . &lt;progress value=&#39;45&#39; class=&#39;&#39; max=&#39;45&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [45/45 00:51&lt;00:00] &lt;progress value=&#39;6&#39; class=&#39;&#39; max=&#39;6&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [6/6 00:08&lt;00:00] CPU times: user 23.9 s, sys: 5.06 s, total: 29 s Wall time: 3min 13s . Once the items have been processed they will become list of numbers, we can still access the underlying raw data in x_obj (or y_obj for the targets, but we don&#39;t have any here). . #collapse_show ll.train.x_obj(0) . . &#39;xxbos xxmaj comedian xxmaj adam xxmaj sandler &#39;s last theatrical release &#34; i xxmaj now xxmaj pronounce xxmaj you xxmaj chuck and xxmaj larry &#34; served as a loud and proud plea for tolerance of the gay community . xxmaj the former &#34; xxmaj saturday xxmaj night xxmaj live &#34; funnyman &#39;s new movie &#34; xxmaj you xxmaj do n &#39;t xxmaj mess with the xxmaj zohan &#34; ( * * * out of xxrep 4 * ) constitutes his plea for tolerance toward xxmaj israeli and xxmaj palestinian immigrants in xxmaj america . xxmaj these unfortunate people are often punished in xxmaj america for the crimes of their counterparts in the war - ravaged xxmaj middle xxmaj east . xxmaj although &#34; xxmaj zohan &#34; advocates a lofty cause , xxmaj sandler does n &#39;t let his political agenda overshadow his usual crude , below - the - belt , juvenile shenanigans that rely on obscene bodily functions , promiscuous sex , and far - fetched harebrained idiocy . xxmaj indeed , the hysterical horseplay that xxmaj sandler and company revel in may distract you from the plight of these uprooted , misplaced misfits that had fled to xxmaj uncle xxmaj sam &#39;s shores because they believe xxmaj america is a xxmaj utopia . xxmaj interestingly , xxmaj sandler plays a xxmaj jewish xxunk agent of the xxmaj mossad , xxmaj israel &#39;s secret police , with a hopelessly corny accent . xxmaj zohan &#39;s exploits appear to foreshadow xxmaj will xxmaj smith &#39;s upcoming &#34; xxmaj hancock . &#34; xxmaj zohan is the best xxmaj jewish secret agent in the whole wide world . xxmaj he is literally indestructible . xxmaj he catches bullets in his nose . xxmaj he can swim faster than a dolphin , and a razor - toothed piranha fish in his bikini swim trunks amuses him . n n xxmaj zohan ( xxmaj adam xxmaj sandler ) is cooking fish at the beach when his superiors interrupt his vacation and inform him that the dreaded xxmaj arab terrorist , the xxmaj phantom ( xxmaj john xxmaj turturro of &#34; xxmaj transformers &#34; ) , is up to his old tricks again . xxmaj naturally , xxmaj zohan is furious ! xxmaj actually , xxmaj zohan captured the xxmaj phantom three months ago , but the politicians have exchanged the xxmaj phantom for political prisoners . xxmaj now , xxmaj zohan must nab his nemesis again ! xxmaj the xxmaj phantom and xxmaj zohan tangle in a spectacular fight in the sea and xxmaj zohan does n &#39;t survive . xxmaj in reality , xxmaj zohan deliberately fakes his death so that he can immigrate to xxmaj new xxmaj york xxmaj city and realize his life - long dream of cutting hair for xxmaj paul xxmaj mitchell . xxmaj zohan gives himself an obsolete xxmaj frankie xxmaj avalon haircut , trims his beard , and smuggles himself onto a plane bound for xxmaj america . xxmaj if what happens before his flight seems outlandish , once he is on the jet , he spends his time in the cargo hold with two fluffy dogs named &#34; xxmaj scrappy &#34; and &#34; xxmaj coco . &#34; xxmaj zohan styles their hair from photos in his xxmaj paul xxmaj mitchell haircut book . n n xxmaj at first , xxmaj zohan has no luck getting a job with xxmaj paul xxmaj mitchell , much less cutting hair . xxmaj zohan defends xxmaj michael ( xxmaj nick xxmaj swardson of &#34; xxmaj reno 911 , xxmaj the xxmaj movie &#34; ) in a street brawl after a motorist blames xxmaj michael for his accident with a delivery truck . a grateful xxmaj michael invites xxmaj zohan to stay with his mother , xxmaj gail ( xxmaj lainie xxmaj kazan of &#34; xxmaj dayton &#39;s xxmaj devils &#34; ) , and him . xxmaj zohan practices cutting xxmaj gail &#39;s hair when he is n &#39;t having in lusty sex with her . xxmaj eventually , xxmaj zohan gets a job sweeping up hair at a salon owned by xxmaj dalia ( xxmaj emmanuelle xxmaj chriqui of &#34; xxmaj wrong xxmaj turn &#34; ) who as it turns out is a xxmaj palestinian . xxmaj indeed , xxmaj zohan knows about her heritage but does n &#39;t let it bother him . xxmaj one day when one of xxmaj dalia &#39;s hair stylists does n &#39;t show up , xxmaj zohan takes advantage of her absence to cut hair . xxmaj much to xxmaj dalia &#39;s surprise , xxmaj zohan wins the allegiance of the over sixty crowd . xxmaj older woman line up around the block to have him fashion their hair . xxmaj after each session , xxmaj zohan takes each older lady in the back and xxunk their sexual appetites . n n xxmaj meanwhile , a millionaire real estate developer xxmaj walbridge ( xxmaj michael xxmaj buffer of &#34; xxmaj rocky xxmaj balboa &#34; ) hikes the rent to force xxmaj dalia and others like her out of her store to make way for his mall with a roller - coaster . xxmaj zohan surprises both xxmaj dalia and xxmaj walbridge &#39;s people and forks over the money for her to pay the rent . xxmaj an angry xxmaj walbridge contacts a white supremacy group to ignite a neighborhood war between the xxmaj israelis and xxmaj palestinians . xxmaj this happens about the same time that xxmaj zohan falls in love with xxmaj dalia . xxmaj perennial xxmaj sandler cohort xxmaj rob xxmaj schneider of &#34; xxmaj deuce xxmaj bigalow &#34; appears as a cretinous xxmaj palestinian named xxmaj salim who does n &#39;t know the difference between nitroglycerin and xxmaj xxunk . xxmaj he tries to blow up xxmaj zohan for an old grudge . xxmaj it seems xxmaj zohan beat xxmaj salim up and stole his goat . n n &#34; xxmaj you xxmaj do n &#39;t xxmaj mess with the xxmaj zohan &#34; qualifies as a surreal comedy . xxmaj scenarists xxmaj robert xxmaj smigel of &#34; xxmaj saturday xxmaj night xxmaj live , &#34; xxmaj judd xxmaj apatow of &#34; xxmaj the 40-year xxmaj old xxmaj virgin , &#34; and xxmaj sandler himself vigorously ignore the laws of logic in this zany comedy . xxmaj the movie that most closely resembles &#34; xxmaj zohan &#34; is &#34; xxmaj little xxmaj nicky , &#34; because both characters boast supernatural abilities . &#34; xxmaj you xxmaj do n &#39;t xxmaj mess with the xxmaj zohan &#34; will keep xxmaj adam xxmaj sandler fans in stitches . xxeos&#39; . Since the preprocessing takes time, we save the intermediate result using pickle. Don&#39;t use any lambda functions in your processors or they won&#39;t be able to pickle. . #collapse_show pickle.dump(ll, open(path/&#39;ld.pkl&#39;, &#39;wb&#39;)) . . #collapse_show ll = pickle.load(open(path/&#39;ld.pkl&#39;, &#39;rb&#39;)) . . Batching . We have a bit of work to convert our LabelList in a DataBunch as we don&#39;t just want batches of IMDB reviews. We want to stream through all the texts concatenated. We also have to prepare the targets that are the newt words in the text. All of this is done with the next object called LM_PreLoader. At the beginning of each epoch, it&#39;ll shuffle the articles (if shuffle=True) and create a big stream by concatenating all of them. We divide this big stream in bs smaller streams. That we will read in chunks of bptt length. . Jump_to lesson 12 video . #collapse # Just using those for illustration purposes, they&#39;re not used otherwise. from IPython.display import display,HTML import pandas as pd . . Let&#39;s say our stream is: . #collapse_show stream = &quot;&quot;&quot; In this notebook, we will go back over the example of classifying movie reviews we studied in part 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we&#39;ll have another example of the Processor used in the data block API. Then we will study how we build a language model and train it. n &quot;&quot;&quot; tokens = np.array(tp([stream])[0]) . . &lt;progress value=&#39;1&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 100.00% [1/1 00:00&lt;00:00] Then if we split it in 6 batches it would give something like this: . #collapse_show bs,seq_len = 6,15 d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)]) df = pd.DataFrame(d_tokens) display(HTML(df.to_html(index=False,header=None))) . . xxbos | n | xxmaj | in | this | notebook | , | we | will | go | back | over | the | example | of | . classifying | movie | reviews | we | studied | in | part | 1 | and | dig | deeper | under | the | surface | . | . n | xxmaj | first | we | will | look | at | the | processing | steps | necessary | to | convert | text | into | . numbers | and | how | to | customize | it | . | xxmaj | by | doing | this | , | we | &#39;ll | have | . another | example | of | the | xxmaj | processor | used | in | the | data | block | api | . | n | xxmaj | . then | we | will | study | how | we | build | a | language | model | and | train | it | . | n n | . Then if we have a bptt of 5, we would go over those three batches. . #collapse_show bs,bptt = 6,5 for k in range(3): d_tokens = np.array([tokens[i*seq_len + k*bptt:i*seq_len + (k+1)*bptt] for i in range(bs)]) df = pd.DataFrame(d_tokens) display(HTML(df.to_html(index=False,header=None))) . . xxbos | n | xxmaj | in | this | . classifying | movie | reviews | we | studied | . n | xxmaj | first | we | will | . numbers | and | how | to | customize | . another | example | of | the | xxmaj | . then | we | will | study | how | . notebook | , | we | will | go | . in | part | 1 | and | dig | . look | at | the | processing | steps | . it | . | xxmaj | by | doing | . processor | used | in | the | data | . we | build | a | language | model | . back | over | the | example | of | . deeper | under | the | surface | . | . necessary | to | convert | text | into | . this | , | we | &#39;ll | have | . block | api | . | n | xxmaj | . and | train | it | . | n n | . #collapse_show class LM_PreLoader(): def __init__(self, data, bs=64, bptt=70, shuffle=False): self.data,self.bs,self.bptt,self.shuffle = data,bs,bptt,shuffle total_len = sum([len(t) for t in data.x]) self.n_batch = total_len // bs self.batchify() def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs def __getitem__(self, idx): source = self.batched_data[idx % self.bs] seq_idx = (idx // self.bs) * self.bptt return source[seq_idx:seq_idx+self.bptt],source[seq_idx+1:seq_idx+self.bptt+1] def batchify(self): texts = self.data.x if self.shuffle: texts = texts[torch.randperm(len(texts))] stream = torch.cat([tensor(t) for t in texts]) self.batched_data = stream[:self.n_batch * self.bs].view(self.bs, self.n_batch) . . #collapse_show dl = DataLoader(LM_PreLoader(ll.valid, shuffle=True), batch_size=64) . . Let&#39;s check it all works ok: x1, y1, x2 and y2 should all be of size bs by bptt. The texts in each row of x1 should continue in x2. y1 and y2 should have the same texts as their x counterpart, shifted of one position to the right. . #collapse_show iter_dl = iter(dl) x1,y1 = next(iter_dl) x2,y2 = next(iter_dl) . . #collapse_show x1.size(),y1.size() . . #collapse_show vocab = proc_num.vocab . . #collapse_show &quot; &quot;.join(vocab[o] for o in x1[0]) . . #collapse_show &quot; &quot;.join(vocab[o] for o in y1[0]) . . #collapse_show &quot; &quot;.join(vocab[o] for o in x2[0]) . . And let&#39;s prepare some convenience function to do this quickly. . #collapse_show def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs): return (DataLoader(LM_PreLoader(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs), DataLoader(LM_PreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs)) def lm_databunchify(sd, bs, bptt, **kwargs): return DataBunch(*get_lm_dls(sd.train, sd.valid, bs, bptt, **kwargs)) . . #collapse_show bs,bptt = 64,70 data = lm_databunchify(ll, bs, bptt) . . Batching for classification . When we will want to tackle classification, gathering the data will be a bit different: first we will label our texts with the folder they come from, and then we will need to apply padding to batch them together. To avoid mixing very long texts with very short ones, we will also use Sampler to sort (with a bit of randomness for the training set) our samples by length. . First the data block API calls shold look familiar. . Jump_to lesson 12 video . #collapse_show proc_cat = CategoryProcessor() . . #collapse_show il = TextList.from_files(path, include=[&#39;train&#39;, &#39;test&#39;]) sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=&#39;test&#39;)) ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat) . . #collapse_show pickle.dump(ll, open(path/&#39;ll_clas.pkl&#39;, &#39;wb&#39;)) . . #collapse_show ll = pickle.load(open(path/&#39;ll_clas.pkl&#39;, &#39;rb&#39;)) . . Let&#39;s check the labels seem consistent with the texts. . #collapse_show [(ll.train.x_obj(i), ll.train.y_obj(i)) for i in [1,12552]] . . We saw samplers in notebook 03. For the validation set, we will simply sort the samples by length, and we begin with the longest ones for memory reasons (it&#39;s better to always have the biggest tensors first). . #collapse_show from torch.utils.data import Sampler class SortSampler(Sampler): def __init__(self, data_source, key): self.data_source,self.key = data_source,key def __len__(self): return len(self.data_source) def __iter__(self): return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True)) . . For the training set, we want some kind of randomness on top of this. So first, we shuffle the texts and build megabatches of size 50 * bs. We sort those megabatches by length before splitting them in 50 minibatches. That way we will have randomized batches of roughly the same length. . Then we make sure to have the biggest batch first and shuffle the order of the other batches. We also make sure the last batch stays at the end because its size is probably lower than batch size. . #collapse_show class SortishSampler(Sampler): def __init__(self, data_source, key, bs): self.data_source,self.key,self.bs = data_source,key,bs def __len__(self) -&gt; int: return len(self.data_source) def __iter__(self): idxs = torch.randperm(len(self.data_source)) megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)] sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches]) batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)] max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches])) # find the chunk with the largest key, batches[0],batches[max_idx] = batches[max_idx],batches[0] # then make sure it goes first. batch_idxs = torch.randperm(len(batches)-2) sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) &gt; 1 else LongTensor([]) sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]]) return iter(sorted_idx) . . Padding: we had the padding token (that as an id of 1) at the end of each sequence to make them all the same size when batching them. Note that we need padding at the end to be able to use PyTorch convenience functions that will let us ignore that padding (see 12c). . #collapse_show def pad_collate(samples, pad_idx=1, pad_first=False): max_len = max([len(s[0]) for s in samples]) res = torch.zeros(len(samples), max_len).long() + pad_idx for i,s in enumerate(samples): if pad_first: res[i, -len(s[0]):] = LongTensor(s[0]) else: res[i, :len(s[0]) ] = LongTensor(s[0]) return res, tensor([s[1] for s in samples]) . . #collapse_show bs = 64 train_sampler = SortishSampler(ll.train.x, key=lambda t: len(ll.train[int(t)][0]), bs=bs) train_dl = DataLoader(ll.train, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate) . . #collapse_show iter_dl = iter(train_dl) x,y = next(iter_dl) . . #collapse_show lengths = [] for i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item()) lengths[:5], lengths[-1] . . The last one is the minimal length. This is the first batch so it has the longest sequence, but if look at the next one that is more random, we see lengths are roughly the sames. . #collapse_show x,y = next(iter_dl) lengths = [] for i in range(x.size(0)): lengths.append(x.size(1) - (x[i]==1).sum().item()) lengths[:5], lengths[-1] . . We can see the padding at the end: . #collapse_show x . . And we add a convenience function: . #collapse_show def get_clas_dls(train_ds, valid_ds, bs, **kwargs): train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs) valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t])) return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs), DataLoader(valid_ds, batch_size=bs*2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs)) def clas_databunchify(sd, bs, **kwargs): return DataBunch(*get_clas_dls(sd.train, sd.valid, bs, **kwargs)) . . #collapse_show bs,bptt = 64,70 data = clas_databunchify(ll, bs) . .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/21/5text.html",
            "relUrl": "/jupyter/2020/04/21/5text.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Fastai Course DL from the Foundations Train on Imagenette",
            "content": "Fastai Imagenet(te) training . FP16 should allow 2x speed ups in theory, practicaly it also depends on the number of fp16 vs fp32 cores on your GPU. . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . #collapse from exp.nb_10c import * . . Jump_to lesson 12 video . #collapse path = Path(&quot;/media/cedric/Datasets/imagenette2-160/&quot;) . . size = 128 tfms = [make_rgb, RandomResizedCrop(size,scale=(0.35,1)),np_to_float,PilRandomFlip()] bs = 64 il = ImageList.from_files(path,tfms=tfms) sd = SplitData.split_by_func(il,partial(grandparent_splitter,valid_name=&#39;val&#39;)) ll = label_by_func(sd,parent_labeler,proc_y=CategoryProcessor()) ll.valid.x.tfms = [make_rgb,CenterCrop(size),np_to_float] data = ll.to_databunch(bs,c_in=3,c_out=10,num_workers=8) . XResNet . Jump_to lesson 12 video . #collapse_show def pass_through(x): return x class Flatten(nn.Module): def forward(self,x): return x.view(x.size(0),-1) def conv(cin,cout,ks=3, stride=1,bias=False): return nn.Conv2d(cin,cout,kernel_size=ks,stride=stride,padding=ks//2,bias=bias) . . #collapse_show activation = nn.ReLU(inplace=True) def init_cnn(m): if getattr(m,&#39;bias&#39;,None) is not None : nn.init.constant(m.bias,0) if isinstance(m,(nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight) for l in m.children(): init_cnn(l) def conv_layer(cin,cout,ks=3,stride=1,zero_bn=False,act=True): bn = nn.BatchNorm2d(cout) nn.init.constant_(bn.weight,0. if zero_bn else 1.) layers = [conv(cin,cout,ks,stride),bn] if act : layers.append(activation) return nn.Sequential(*layers) . . #collapse_show class ResBlock(nn.Module): def __init__(self,expansion,ni,nh,stride=1): super().__init__() nf,ni = nh*expansion,ni*expansion #layers #smaller nets if expansion == 1 : layers = [conv_layer(ni,nh,3,stride=stride), conv_layer(nh,nf,3,zero_bn=True,act=False)] #larger Nets ResNet-D Path A else : layers = [conv_layer(ni,nh,1), conv_layer(ni,nh,3,stride=stride), conv_layer(ni,nf,1,zero_bn=True,act=False)] self.convs = nn.Sequential(*layers) self.idconv = noop if ni == nf else conv_layer(ni,nf,1,act=False) self.pool = noop if stride == 1 else nn.AvgPool2d(2,ceil_mode=False) def forward(self,x): return act_fn(self.convs(x)+ self.idconv(self.pool(x))) . . ResBlock Details . Batch Norm sometimes has weights of 0 and sometimes 1 during init. It allows us to init the Conv branch to 0 and the identity mapping to 1. The gradient won&#39;t explode that way. ResNet 50 and onwards use 3 convs, smaller ones use 2. They also use Bottleneck layers (64 filters-&gt; 16filters -&gt; 64 filters), the normal block for larger ResNets. ResNet-D also uses downsample to make sure the two branches can be added. So when stride is not 1, and AvgPool layer with stride of 2 is deployed for different grid size, and 1x1 conv to change the number of filters (if not equal). . #collapse_show class XResNet(nn.Sequential): @classmethod def create(cls,expansion,layers,c_in = 3,c_out=1000): nfs = [c_in,(c_in+1)*8,64,64] stem = [conv_layer(nfs[i],nfs[i+1],stride=2 if i==0 else 1) for i in range(3)] nfs = [64//expansion,64,128,256,512] res_layers = [cls._make_layer(expansion,nfs[i],nfs[i+1], n_blocks=l,stride=1 if i==0 else 2) for i,l in enumerate(layers)] res = cls(*stem,nn.MaxPool2d(kernel_size=3,stride=2,padding=1),*res_layers, nn.AdaptiveAvgPool2d(1),Flatten(),nn.Linear(nfs[-1]*expansion,c_out)) init_cnn(res) return res @staticmethod def _make_layer(expansion,ni,nf,n_blocks,stride): return nn.Sequential(*[ResBlock(expansion,ni if i==0 else nf,nf,stride if i==0 else 1) for i in range(n_blocks)]) . . #collapse_show def xresnet18 (**kwargs): return XResNet.create(1, [2, 2, 2, 2], **kwargs) def xresnet34 (**kwargs): return XResNet.create(1, [3, 4, 6, 3], **kwargs) def xresnet50 (**kwargs): return XResNet.create(4, [3, 4, 6, 3], **kwargs) def xresnet101(**kwargs): return XResNet.create(4, [3, 4, 23, 3], **kwargs) def xresnet152(**kwargs): return XResNet.create(4, [3, 8, 36, 3], **kwargs) . . Train . Jump_to lesson 12 video . #collapse_show cbfs = [partial(AvgStatsCallback,accuracy), ProgressCallback, CudaCallback, partial(BatchTransformXCallback, norm_imagenette), # partial(MixUp, alpha=0.2) ] . . #collapse_show loss_func = LabelSmoothingCrossEntropy() arch = partial(xresnet18, c_out=10) opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2) . . #collapse_show def get_batch(dl, learn): learn.xb,learn.yb = next(iter(dl)) learn.do_begin_fit(0) learn(&#39;begin_batch&#39;) learn(&#39;after_fit&#39;) return learn.xb,learn.yb . . We need to replace the old model_summary since it used to take a Runner. . #collapse_show def model_summary(model, data, find_all=False, print_mod=False): xb,yb = get_batch(data.valid_dl, learn) mods = find_modules(model, is_lin_layer) if find_all else model.children() f = lambda hook,mod,inp,out: print(f&quot;==== n{mod} n&quot; if print_mod else &quot;&quot;, out.shape) with Hooks(mods, f) as hooks: learn.model(xb) . . #collapse learn = Learner(arch(), data, loss_func, lr=1, cb_funcs=cbfs, opt_func=opt_func) . . /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_. &#34;&#34;&#34; . #collapse learn.model = learn.model.cuda() model_summary(learn.model, data, print_mod=False) . . epoch train_loss train_accuracy valid_loss valid_accuracy time . torch.Size([128, 32, 64, 64]) torch.Size([128, 64, 64, 64]) torch.Size([128, 64, 64, 64]) torch.Size([128, 64, 32, 32]) torch.Size([128, 64, 32, 32]) torch.Size([128, 128, 16, 16]) torch.Size([128, 256, 8, 8]) torch.Size([128, 512, 4, 4]) torch.Size([128, 512, 1, 1]) torch.Size([128, 512]) torch.Size([128, 10]) . #collapse arch = partial(xresnet34, c_out=10) . . #collapse learn = Learner(arch(), data, loss_func, lr=1, cb_funcs=cbfs, opt_func=opt_func) . . /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_. &#34;&#34;&#34; . #collapse learn.fit(1, cbs=[LR_Find(), Recorder()]) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . #collapse learn.recorder.plot(3) . . #collapse_show def create_phases(phases): phases = listify(phases) return phases + [1-sum(phases)] . . #collapse_show print(create_phases(0.3)) print(create_phases([0.3,0.2])) . . [0.3, 0.7] [0.3, 0.2, 0.5] . #collapse_show lr = 1e-2 pct_start = 0.5 phases = create_phases(pct_start) sched_lr = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5)) sched_mom = combine_scheds(phases, cos_1cycle_anneal(0.95, 0.85, 0.95)) . . #collapse_show cbsched = [ ParamScheduler(&#39;lr&#39;, sched_lr), ParamScheduler(&#39;mom&#39;, sched_mom)] . . #collapse_show learn = Learner(arch(), data, loss_func, lr=lr, cb_funcs=cbfs, opt_func=opt_func) . . /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_. &#34;&#34;&#34; . #collapse_show learn.fit(5, cbs=cbsched) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;5&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 1.747138 | 0.461189 | 2.137488 | 0.448408 | 00:20 | . 1 | 1.530650 | 0.573239 | 1.779503 | 0.470064 | 00:20 | . 2 | 1.386309 | 0.634280 | 1.324897 | 0.672357 | 00:20 | . 3 | 1.207779 | 0.713275 | 1.158664 | 0.729172 | 00:20 | . 4 | 1.045097 | 0.778435 | 0.997302 | 0.799745 | 00:20 | . cnn_learner . Jump_to lesson 12 video . #collapse_show def cnn_learner(arch, data, loss_func, opt_func, c_in=None, c_out=None, lr=1e-2, cuda=True, norm=None, progress=True, mixup=0, xtra_cb=None, **kwargs): cbfs = [partial(AvgStatsCallback,accuracy)]+listify(xtra_cb) if progress: cbfs.append(ProgressCallback) if cuda: cbfs.append(CudaCallback) if norm: cbfs.append(partial(BatchTransformXCallback, norm)) if mixup: cbfs.append(partial(MixUp, mixup)) arch_args = {} if not c_in : c_in = data.c_in if not c_out: c_out = data.c_out if c_in: arch_args[&#39;c_in&#39; ]=c_in if c_out: arch_args[&#39;c_out&#39;]=c_out return Learner(arch(**arch_args), data, loss_func, opt_func=opt_func, lr=lr, cb_funcs=cbfs, **kwargs) . . #collapse_show learn = cnn_learner(xresnet34, data, loss_func, opt_func, norm=norm_imagenette) . . #collapse_show learn.fit(5, cbsched) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;5&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 1.734203 | 0.474813 | 1.960827 | 0.469299 | 00:18 | . 1 | 1.525025 | 0.574506 | 2.046430 | 0.447643 | 00:18 | . 2 | 1.396191 | 0.626888 | 1.789930 | 0.493248 | 00:18 | . 3 | 1.216749 | 0.706833 | 1.156404 | 0.729427 | 00:18 | . 4 | 1.043234 | 0.776956 | 1.005148 | 0.792102 | 00:18 | . Imagenet . You can see all this put together in the fastai imagenet training script. It&#39;s the same as what we&#39;ve seen so far, except it also handles multi-GPU training. So how well does this work? . We trained for 60 epochs, and got an error of 5.9%, compared to the official PyTorch resnet which gets 7.5% error in 90 epochs! Our xresnet 50 training even surpasses standard resnet 152, which trains for 50% more epochs and has 3x as many layers. .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/21/4trainimagenette.html",
            "relUrl": "/jupyter/2020/04/21/4trainimagenette.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Fastai Course DL from the Foundations Transfer Learning",
            "content": "#collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_11 import * . . Serializing the model . Jump_to lesson 12 video . #collapse path = datasets.untar_data(datasets.URLs.IMAGEWOOF_160) . . #collapse size = 128 bs = 64 tfms = [make_rgb, RandomResizedCrop(size, scale=(0.35,1)), np_to_float, PilRandomFlip()] val_tfms = [make_rgb, CenterCrop(size), np_to_float] il = ImageList.from_files(path, tfms=tfms) sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=&#39;val&#39;)) ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) ll.valid.x.tfms = val_tfms data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=8) . . #collapse len(il) . . 12954 . #collapse_show loss_func = LabelSmoothingCrossEntropy() opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2) . . #collapse_show learn = cnn_learner(xresnet18, data, loss_func, opt_func, norm=norm_imagenette) . . #collapse_show def sched_1cycle(lr, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95): phases = create_phases(pct_start) sched_lr = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5)) sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end)) return [ParamScheduler(&#39;lr&#39;, sched_lr), ParamScheduler(&#39;mom&#39;, sched_mom)] . . #collapse_show lr = 3e-3 pct_start = 0.5 cbsched = sched_1cycle(lr, pct_start) . . #collapse_show learn.fit(40, cbsched) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;40&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 2.130161 | 0.248089 | 2.085567 | 0.276152 | 00:15 | . 1 | 1.993560 | 0.317562 | 2.078038 | 0.288878 | 00:13 | . 2 | 1.907229 | 0.362105 | 1.908168 | 0.367269 | 00:13 | . 3 | 1.827499 | 0.405319 | 1.952905 | 0.351234 | 00:13 | . 4 | 1.774419 | 0.423823 | 1.992134 | 0.336472 | 00:13 | . 5 | 1.722007 | 0.449197 | 1.837551 | 0.401883 | 00:13 | . 6 | 1.686701 | 0.470803 | 1.984103 | 0.358361 | 00:13 | . 7 | 1.677165 | 0.473684 | 2.270435 | 0.356834 | 00:13 | . 8 | 1.638770 | 0.499280 | 2.088254 | 0.376686 | 00:13 | . 9 | 1.610834 | 0.505042 | 2.130411 | 0.363451 | 00:13 | . 10 | 1.567782 | 0.529086 | 1.999561 | 0.408755 | 00:13 | . 11 | 1.524520 | 0.548255 | 3.583080 | 0.271061 | 00:13 | . 12 | 1.484032 | 0.571080 | 1.894512 | 0.450496 | 00:13 | . 13 | 1.446350 | 0.597341 | 1.581483 | 0.526852 | 00:14 | . 14 | 1.407801 | 0.603546 | 1.724309 | 0.484856 | 00:13 | . 15 | 1.367980 | 0.626814 | 2.098016 | 0.422499 | 00:13 | . 16 | 1.339640 | 0.637784 | 1.608337 | 0.543904 | 00:13 | . 17 | 1.307783 | 0.650416 | 1.544712 | 0.570629 | 00:13 | . 18 | 1.273955 | 0.669252 | 1.716685 | 0.533469 | 00:13 | . 19 | 1.243974 | 0.685429 | 1.656472 | 0.565538 | 00:13 | . 20 | 1.215433 | 0.698393 | 1.512875 | 0.574701 | 00:13 | . 21 | 1.175064 | 0.715125 | 1.317181 | 0.658183 | 00:13 | . 22 | 1.149006 | 0.728310 | 1.489734 | 0.611606 | 00:13 | . 23 | 1.107240 | 0.743712 | 1.476818 | 0.602952 | 00:13 | . 24 | 1.080577 | 0.756787 | 1.456552 | 0.625095 | 00:13 | . 25 | 1.058459 | 0.768089 | 1.338278 | 0.660473 | 00:13 | . 26 | 1.028792 | 0.782271 | 1.194180 | 0.711886 | 00:13 | . 27 | 0.997885 | 0.795789 | 1.130299 | 0.744464 | 00:13 | . 28 | 0.960315 | 0.816067 | 1.161608 | 0.731738 | 00:13 | . 29 | 0.935516 | 0.829363 | 1.069859 | 0.773479 | 00:13 | . 30 | 0.906349 | 0.841219 | 1.066289 | 0.776533 | 00:13 | . 31 | 0.878236 | 0.849307 | 1.067750 | 0.777552 | 00:13 | . 32 | 0.850478 | 0.868255 | 1.027464 | 0.790277 | 00:12 | . 33 | 0.828395 | 0.877895 | 1.023876 | 0.789005 | 00:12 | . 34 | 0.800545 | 0.892078 | 1.019160 | 0.796386 | 00:12 | . 35 | 0.790883 | 0.899280 | 0.999217 | 0.803512 | 00:12 | . 36 | 0.774763 | 0.904377 | 1.007500 | 0.801985 | 00:12 | . 37 | 0.768861 | 0.902936 | 0.998095 | 0.804530 | 00:12 | . 38 | 0.757434 | 0.912133 | 0.995890 | 0.802749 | 00:12 | . 39 | 0.762366 | 0.909252 | 0.995066 | 0.803003 | 00:12 | . #collapse_show st = learn.model.state_dict() . . #collapse_show type(st) . . collections.OrderedDict . #collapse_show &#39;, &#39;.join(st.keys()) . . &#39;0.0.weight, 0.1.weight, 0.1.bias, 0.1.running_mean, 0.1.running_var, 0.1.num_batches_tracked, 1.0.weight, 1.1.weight, 1.1.bias, 1.1.running_mean, 1.1.running_var, 1.1.num_batches_tracked, 2.0.weight, 2.1.weight, 2.1.bias, 2.1.running_mean, 2.1.running_var, 2.1.num_batches_tracked, 4.0.convs.0.0.weight, 4.0.convs.0.1.weight, 4.0.convs.0.1.bias, 4.0.convs.0.1.running_mean, 4.0.convs.0.1.running_var, 4.0.convs.0.1.num_batches_tracked, 4.0.convs.1.0.weight, 4.0.convs.1.1.weight, 4.0.convs.1.1.bias, 4.0.convs.1.1.running_mean, 4.0.convs.1.1.running_var, 4.0.convs.1.1.num_batches_tracked, 4.1.convs.0.0.weight, 4.1.convs.0.1.weight, 4.1.convs.0.1.bias, 4.1.convs.0.1.running_mean, 4.1.convs.0.1.running_var, 4.1.convs.0.1.num_batches_tracked, 4.1.convs.1.0.weight, 4.1.convs.1.1.weight, 4.1.convs.1.1.bias, 4.1.convs.1.1.running_mean, 4.1.convs.1.1.running_var, 4.1.convs.1.1.num_batches_tracked, 5.0.convs.0.0.weight, 5.0.convs.0.1.weight, 5.0.convs.0.1.bias, 5.0.convs.0.1.running_mean, 5.0.convs.0.1.running_var, 5.0.convs.0.1.num_batches_tracked, 5.0.convs.1.0.weight, 5.0.convs.1.1.weight, 5.0.convs.1.1.bias, 5.0.convs.1.1.running_mean, 5.0.convs.1.1.running_var, 5.0.convs.1.1.num_batches_tracked, 5.0.idconv.0.weight, 5.0.idconv.1.weight, 5.0.idconv.1.bias, 5.0.idconv.1.running_mean, 5.0.idconv.1.running_var, 5.0.idconv.1.num_batches_tracked, 5.1.convs.0.0.weight, 5.1.convs.0.1.weight, 5.1.convs.0.1.bias, 5.1.convs.0.1.running_mean, 5.1.convs.0.1.running_var, 5.1.convs.0.1.num_batches_tracked, 5.1.convs.1.0.weight, 5.1.convs.1.1.weight, 5.1.convs.1.1.bias, 5.1.convs.1.1.running_mean, 5.1.convs.1.1.running_var, 5.1.convs.1.1.num_batches_tracked, 6.0.convs.0.0.weight, 6.0.convs.0.1.weight, 6.0.convs.0.1.bias, 6.0.convs.0.1.running_mean, 6.0.convs.0.1.running_var, 6.0.convs.0.1.num_batches_tracked, 6.0.convs.1.0.weight, 6.0.convs.1.1.weight, 6.0.convs.1.1.bias, 6.0.convs.1.1.running_mean, 6.0.convs.1.1.running_var, 6.0.convs.1.1.num_batches_tracked, 6.0.idconv.0.weight, 6.0.idconv.1.weight, 6.0.idconv.1.bias, 6.0.idconv.1.running_mean, 6.0.idconv.1.running_var, 6.0.idconv.1.num_batches_tracked, 6.1.convs.0.0.weight, 6.1.convs.0.1.weight, 6.1.convs.0.1.bias, 6.1.convs.0.1.running_mean, 6.1.convs.0.1.running_var, 6.1.convs.0.1.num_batches_tracked, 6.1.convs.1.0.weight, 6.1.convs.1.1.weight, 6.1.convs.1.1.bias, 6.1.convs.1.1.running_mean, 6.1.convs.1.1.running_var, 6.1.convs.1.1.num_batches_tracked, 7.0.convs.0.0.weight, 7.0.convs.0.1.weight, 7.0.convs.0.1.bias, 7.0.convs.0.1.running_mean, 7.0.convs.0.1.running_var, 7.0.convs.0.1.num_batches_tracked, 7.0.convs.1.0.weight, 7.0.convs.1.1.weight, 7.0.convs.1.1.bias, 7.0.convs.1.1.running_mean, 7.0.convs.1.1.running_var, 7.0.convs.1.1.num_batches_tracked, 7.0.idconv.0.weight, 7.0.idconv.1.weight, 7.0.idconv.1.bias, 7.0.idconv.1.running_mean, 7.0.idconv.1.running_var, 7.0.idconv.1.num_batches_tracked, 7.1.convs.0.0.weight, 7.1.convs.0.1.weight, 7.1.convs.0.1.bias, 7.1.convs.0.1.running_mean, 7.1.convs.0.1.running_var, 7.1.convs.0.1.num_batches_tracked, 7.1.convs.1.0.weight, 7.1.convs.1.1.weight, 7.1.convs.1.1.bias, 7.1.convs.1.1.running_mean, 7.1.convs.1.1.running_var, 7.1.convs.1.1.num_batches_tracked, 10.weight, 10.bias&#39; . #collapse_show st[&#39;10.bias&#39;] . . tensor([-0.0070, 0.0070, -0.0086, -0.0081, 0.0253, 0.0061, 0.0274, 0.0104, -0.0421, -0.0088], device=&#39;cuda:0&#39;) . #collapse_show mdl_path = path/&#39;models&#39; mdl_path.mkdir(exist_ok=True) . . It&#39;s also possible to save the whole model, including the architecture, but it gets quite fiddly and we don&#39;t recommend it. Instead, just save the parameters, and recreate the model directly. . #collapse_show torch.save(st, mdl_path/&#39;iw5&#39;) . . Pets . Jump_to lesson 12 video . #collapse_show pets = datasets.untar_data(datasets.URLs.PETS) . . #collapse_show pets.ls() . . [PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/annotations&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images&#39;)] . #collapse_show pets_path = pets/&#39;images&#39; . . #collapse_show il = ImageList.from_files(pets_path, tfms=tfms) . . #collapse_show il . . ImageList (7390 items) [PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/British_Shorthair_45.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/Siamese_128.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_185.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/basset_hound_98.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/basset_hound_136.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/Birman_136.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/scottish_terrier_40.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/saint_bernard_96.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/german_shorthaired_27.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/chihuahua_5.jpg&#39;)...] Path: /home/cedric/.fastai/data/oxford-iiit-pet/images . #collapse_show def random_splitter(fn, p_valid): return random.random() &lt; p_valid . . #collapse_show random.seed(42) . . #collapse_show sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1)) . . #collapse_show sd . . SplitData Train: ImageList (6667 items) [PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/British_Shorthair_45.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_185.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/basset_hound_98.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/basset_hound_136.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/Birman_136.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/scottish_terrier_40.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/german_shorthaired_27.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_62.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_71.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/Bombay_113.jpg&#39;)...] Path: /home/cedric/.fastai/data/oxford-iiit-pet/images Valid: ImageList (723 items) [PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/Siamese_128.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/saint_bernard_96.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/chihuahua_5.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/keeshond_163.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_86.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/yorkshire_terrier_113.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/japanese_chin_94.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/staffordshire_bull_terrier_115.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_71.jpg&#39;), PosixPath(&#39;/home/cedric/.fastai/data/oxford-iiit-pet/images/great_pyrenees_195.jpg&#39;)...] Path: /home/cedric/.fastai/data/oxford-iiit-pet/images . #collapse_show n = il.items[0].name; n . . &#39;British_Shorthair_45.jpg&#39; . #collapse_show re.findall(r&#39;^(.*)_ d+.jpg$&#39;, n)[0] . . &#39;British_Shorthair&#39; . #collapse_show def pet_labeler(fn): return re.findall(r&#39;^(.*)_ d+.jpg$&#39;, fn.name)[0] . . #collapse_show proc = CategoryProcessor() . . #collapse_show ll = label_by_func(sd, pet_labeler, proc_y=proc) . . #collapse_show &#39;, &#39;.join(proc.vocab) . . &#39;British_Shorthair, staffordshire_bull_terrier, basset_hound, Birman, scottish_terrier, german_shorthaired, Egyptian_Mau, yorkshire_terrier, Bombay, great_pyrenees, english_cocker_spaniel, leonberger, Siamese, american_bulldog, japanese_chin, Maine_Coon, newfoundland, Abyssinian, pug, Russian_Blue, beagle, samoyed, havanese, wheaten_terrier, Bengal, boxer, american_pit_bull_terrier, miniature_pinscher, Sphynx, chihuahua, shiba_inu, english_setter, saint_bernard, pomeranian, Persian, keeshond, Ragdoll&#39; . #collapse_show ll.valid.x.tfms = val_tfms . . #collapse_show c_out = len(proc.vocab) . . #collapse_show data = ll.to_databunch(bs, c_in=3, c_out=c_out, num_workers=8) . . #collapse_show learn = cnn_learner(xresnet18, data, loss_func, opt_func, norm=norm_imagenette) . . #collapse_show learn.fit(5, cbsched) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;5&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 3.460010 | 0.087896 | 3.508121 | 0.081604 | 00:09 | . 1 | 3.291101 | 0.138443 | 4.057820 | 0.084371 | 00:09 | . 2 | 3.074502 | 0.194390 | 3.341131 | 0.146611 | 00:09 | . 3 | 2.764267 | 0.287986 | 2.808986 | 0.251729 | 00:09 | . 4 | 2.467706 | 0.386681 | 2.570934 | 0.344398 | 00:09 | . Custom head . Jump_to lesson 12 video . #collapse_show learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette) . . #collapse_show st = torch.load(mdl_path/&#39;iw5&#39;) . . #collapse_show m = learn.model . . #collapse_show m.load_state_dict(st) . . &lt;All keys matched successfully&gt; . #collapse_show cut = next(i for i,o in enumerate(m.children()) if isinstance(o,nn.AdaptiveAvgPool2d)) m_cut = m[:cut] . . #collapse_show xb,yb = get_batch(data.valid_dl, learn) . . epoch train_loss train_accuracy valid_loss valid_accuracy time . #collapse_show pred = m_cut(xb) . . #collapse_show pred.shape . . torch.Size([128, 512, 4, 4]) . #collapse_show ni = pred.shape[1] . . #collapse_show class AdaptiveConcatPool2d(nn.Module): def __init__(self, sz=1): super().__init__() self.output_size = sz self.ap = nn.AdaptiveAvgPool2d(sz) self.mp = nn.AdaptiveMaxPool2d(sz) def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1) . . #collapse_show nh = 40 m_new = nn.Sequential( m_cut, AdaptiveConcatPool2d(), Flatten(), nn.Linear(ni*2, data.c_out)) . . #collapse_show learn.model = m_new . . #collapse_show learn.fit(5, cbsched) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;5&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 2.869728 | 0.286336 | 2.255571 | 0.448133 | 00:09 | . 1 | 2.109874 | 0.496625 | 2.265567 | 0.448133 | 00:09 | . 2 | 1.930034 | 0.561272 | 2.033231 | 0.504841 | 00:09 | . 3 | 1.674324 | 0.657567 | 1.723327 | 0.641770 | 00:09 | . 4 | 1.474969 | 0.736463 | 1.574200 | 0.699862 | 00:09 | . adapt_model and gradual unfreezing . Jump_to lesson 12 video . #collapse_show def adapt_model(learn, data): cut = next(i for i,o in enumerate(learn.model.children()) if isinstance(o,nn.AdaptiveAvgPool2d)) m_cut = learn.model[:cut] xb,yb = get_batch(data.valid_dl, learn) pred = m_cut(xb) ni = pred.shape[1] m_new = nn.Sequential( m_cut, AdaptiveConcatPool2d(), Flatten(), nn.Linear(ni*2, data.c_out)) learn.model = m_new . . #collapse_show learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette) learn.model.load_state_dict(torch.load(mdl_path/&#39;iw5&#39;)) . . &lt;All keys matched successfully&gt; . #collapse_show adapt_model(learn, data) . . epoch train_loss train_accuracy valid_loss valid_accuracy time . #collapse_show for p in learn.model[0].parameters(): p.requires_grad_(False) . . #collapse_show learn.fit(3, sched_1cycle(1e-2, 0.5)) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;3&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 2.782441 | 0.298035 | 2.739205 | 0.340249 | 00:05 | . 1 | 2.570200 | 0.401830 | 2.523606 | 0.439834 | 00:05 | . 2 | 2.133533 | 0.512674 | 2.141393 | 0.496542 | 00:05 | . #collapse_show for p in learn.model[0].parameters(): p.requires_grad_(True) . . #collapse_show learn.fit(5, cbsched, reset_opt=True) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;5&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 1.907010 | 0.585271 | 1.969608 | 0.571231 | 00:09 | . 1 | 1.861008 | 0.595770 | 2.103309 | 0.510373 | 00:09 | . 2 | 1.801710 | 0.607920 | 2.016537 | 0.508990 | 00:09 | . 3 | 1.609392 | 0.691615 | 1.749493 | 0.634855 | 00:09 | . 4 | 1.424160 | 0.761212 | 1.586875 | 0.695712 | 00:09 | . Batch norm transfer . Jump_to lesson 12 video . #collapse_show learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette) learn.model.load_state_dict(torch.load(mdl_path/&#39;iw5&#39;)) adapt_model(learn, data) . . epoch train_loss train_accuracy valid_loss valid_accuracy time . #collapse_show def apply_mod(m, f): f(m) for l in m.children(): apply_mod(l, f) def set_grad(m, b): if isinstance(m, (nn.Linear,nn.BatchNorm2d)): return if hasattr(m, &#39;weight&#39;): for p in m.parameters(): p.requires_grad_(b) . . #collapse_show apply_mod(learn.model, partial(set_grad, b=False)) . . #collapse_show learn.fit(3, sched_1cycle(1e-2, 0.5)) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;3&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 2.690954 | 0.320084 | 2.427597 | 0.403873 | 00:06 | . 1 | 2.192524 | 0.474726 | 2.123527 | 0.484094 | 00:06 | . 2 | 1.914695 | 0.569222 | 1.958007 | 0.557400 | 00:06 | . #collapse_show apply_mod(learn.model, partial(set_grad, b=True)) . . #collapse_show learn.fit(5, cbsched, reset_opt=True) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;5&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 1.819283 | 0.609569 | 1.912626 | 0.580913 | 00:09 | . 1 | 1.793303 | 0.617219 | 2.100043 | 0.493776 | 00:09 | . 2 | 1.751841 | 0.628919 | 2.360935 | 0.394191 | 00:09 | . 3 | 1.569761 | 0.704065 | 1.744866 | 0.626556 | 00:09 | . 4 | 1.408960 | 0.767062 | 1.579932 | 0.692946 | 00:08 | . Pytorch already has an apply method we can use: . #collapse_show learn.model.apply(partial(set_grad, b=False)); . . Discriminative LR and param groups . Jump_to lesson 12 video . #collapse_show learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette) . . #collapse_show learn.model.load_state_dict(torch.load(mdl_path/&#39;iw5&#39;)) adapt_model(learn, data) . . epoch train_loss train_accuracy valid_loss valid_accuracy time . #collapse_show def bn_splitter(m): def _bn_splitter(l, g1, g2): if isinstance(l, nn.BatchNorm2d): g2 += l.parameters() elif hasattr(l, &#39;weight&#39;): g1 += l.parameters() for ll in l.children(): _bn_splitter(ll, g1, g2) g1,g2 = [],[] _bn_splitter(m[0], g1, g2) g2 += m[1:].parameters() return g1,g2 . . #collapse_show a,b = bn_splitter(learn.model) . . #collapse_show test_eq(len(a)+len(b), len(list(m.parameters()))) . . #collapse_show Learner.ALL_CBS . . {&#39;after_backward&#39;, &#39;after_batch&#39;, &#39;after_cancel_batch&#39;, &#39;after_cancel_epoch&#39;, &#39;after_cancel_train&#39;, &#39;after_epoch&#39;, &#39;after_fit&#39;, &#39;after_loss&#39;, &#39;after_pred&#39;, &#39;after_step&#39;, &#39;begin_batch&#39;, &#39;begin_epoch&#39;, &#39;begin_fit&#39;, &#39;begin_validate&#39;} . #collapse_show from types import SimpleNamespace cb_types = SimpleNamespace(**{o:o for o in Learner.ALL_CBS}) . . #collapse_show cb_types.after_backward . . &#39;after_backward&#39; . #collapse_show class DebugCallback(Callback): _order = 999 def __init__(self, cb_name, f=None): self.cb_name,self.f = cb_name,f def __call__(self, cb_name): if cb_name==self.cb_name: if self.f: self.f(self.run) else: set_trace() . . #collapse_show def sched_1cycle(lrs, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95): phases = create_phases(pct_start) sched_lr = [combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5)) for lr in lrs] sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end)) return [ParamScheduler(&#39;lr&#39;, sched_lr), ParamScheduler(&#39;mom&#39;, sched_mom)] . . #collapse_show disc_lr_sched = sched_1cycle([0,3e-2], 0.5) . . #collapse_show learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette, splitter=bn_splitter) learn.model.load_state_dict(torch.load(mdl_path/&#39;iw5&#39;)) adapt_model(learn, data) . . epoch train_loss train_accuracy valid_loss valid_accuracy time . #collapse_show def _print_det(o): print (len(o.opt.param_groups), o.opt.hypers) raise CancelTrainException() learn.fit(1, disc_lr_sched + [DebugCallback(cb_types.after_batch, _print_det)]) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 2 [{&#39;mom&#39;: 0.9499999999999997, &#39;mom_sqr&#39;: 0.99, &#39;eps&#39;: 1e-06, &#39;wd&#39;: 0.01, &#39;lr&#39;: 0.0, &#39;sqr_mom&#39;: 0.99}, {&#39;mom&#39;: 0.9499999999999997, &#39;mom_sqr&#39;: 0.99, &#39;eps&#39;: 1e-06, &#39;wd&#39;: 0.01, &#39;lr&#39;: 0.0030000000000000512, &#39;sqr_mom&#39;: 0.99}] . #collapse_show learn.fit(3, disc_lr_sched) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;3&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 2.585809 | 0.358032 | 2.342050 | 0.409405 | 00:08 | . 1 | 2.312127 | 0.433328 | 2.434426 | 0.412172 | 00:08 | . 2 | 2.032101 | 0.523324 | 1.978259 | 0.539419 | 00:08 | . #collapse_show disc_lr_sched = sched_1cycle([1e-3,1e-2], 0.3) . . #collapse_show learn.fit(5, disc_lr_sched) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;5&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 1.862455 | 0.596520 | 2.062223 | 0.511757 | 00:08 | . 1 | 1.927012 | 0.558422 | 2.068517 | 0.508990 | 00:08 | . 2 | 1.780360 | 0.621869 | 1.958512 | 0.542185 | 00:08 | . 3 | 1.633498 | 0.674516 | 1.755347 | 0.605809 | 00:08 | . 4 | 1.534966 | 0.709615 | 1.697863 | 0.641770 | 00:08 | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/21/3transfer-learning.html",
            "relUrl": "/jupyter/2020/04/21/3transfer-learning.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Fastai Course DL from the Foundations Mixed Precision Training",
            "content": "Fastai Training in mixed precision . FP16 should allow up to 8-10x speed ups in theory, practically it also depends on the number of specialized cores on the GPU as well as Software configurations of the GPU driver. . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_10b import * . . A little bit of theory . Jump_to lesson 12 video . Continuing the documentation on the fastai_v1 development here is a brief piece about mixed precision training. A very nice and clear introduction to it is this video from NVIDIA. . What&#39;s half precision? . In neural nets, all the computations are usually done in single precision, which means all the floats in all the arrays that represent inputs, activations, weights... are 32-bit floats (FP32 in the rest of this post). An idea to reduce memory usage (and avoid those annoying cuda errors) has been to try and do the same thing in half-precision, which means using 16-bits floats (or FP16 in the rest of this post). By definition, they take half the space in RAM, and in theory could allow you to double the size of your model and double your batch size. . Another very nice feature is that NVIDIA developed its latest GPUs (the Volta generation) to take fully advantage of half-precision tensors. Basically, if you give half-precision tensors to those, they&#39;ll stack them so that each core can do more operations at the same time, and theoretically gives an 8x speed-up (sadly, just in theory). . So training at half precision is better for your memory usage, way faster if you have a Volta GPU (still a tiny bit faster if you don&#39;t since the computations are easiest). How do we do it? Super easily in pytorch, we just have to put .half() everywhere: on the inputs of our model and all the parameters. Problem is that you usually won&#39;t see the same accuracy in the end (so it happens sometimes) because half-precision is... well... not as precise ;). . Problems with half-precision: . To understand the problems with half precision, let&#39;s look briefly at what an FP16 looks like (more information here). . . The sign bit gives us +1 or -1, then we have 5 bits to code an exponent between -14 and 15, while the fraction part has the remaining 10 bits. Compared to FP32, we have a smaller range of possible values (2e-14 to 2e15 roughly, compared to 2e-126 to 2e127 for FP32) but also a smaller offset. . For instance, between 1 and 2, the FP16 format only represents the number 1, 1+2e-10, 1+2*2e-10... which means that 1 + 0.0001 = 1 in half precision. That&#39;s what will cause a certain numbers of problems, specifically three that can occur and mess up your training. . The weight update is imprecise: inside your optimizer, you basically do w = w - lr w.grad for each weight of your network. The problem in performing this operation in half precision is that very often, w.grad is several orders of magnitude below w, and the learning rate is also small. The situation where w=1 and lrw.grad is 0.0001 (or lower) is therefore very common, but the update doesn&#39;t do anything in those cases. | Your gradients can underflow. In FP16, your gradients can easily be replaced by 0 because they are too low. | Your activations or loss can overflow. The opposite problem from the gradients: it&#39;s easier to hit nan (or infinity) in FP16 precision, and your training might more easily diverge. | The solution: mixed precision training . To address those three problems, we don&#39;t fully train in FP16 precision. As the name mixed training implies, some of the operations will be done in FP16, others in FP32. This is mainly to take care of the first problem listed above. For the next two there are additional tricks. . The main idea is that we want to do the forward pass and the gradient computation in half precision (to go fast) but the update in single precision (to be more precise). It&#39;s okay if w and grad are both half floats, but when we do the operation w = w - lr * grad, we need to compute it in FP32. That way our 1 + 0.0001 is going to be 1.0001. . This is why we keep a copy of the weights in FP32 (called master model). Then, our training loop will look like: . compute the output with the FP16 model, then the loss | back-propagate the gradients in half-precision. | copy the gradients in FP32 precision | do the update on the master model (in FP32 precision) | copy the master model in the FP16 model. | Note that we lose precision during step 5, and that the 1.0001 in one of the weights will go back to 1. But if the next update corresponds to add 0.0001 again, since the optimizer step is done on the master model, the 1.0001 will become 1.0002 and if we eventually go like this up to 1.0005, the FP16 model will be able to tell the difference. . That takes care of problem 1. For the second problem, we use something called gradient scaling: to avoid the gradients getting zeroed by the FP16 precision, we multiply the loss by a scale factor (scale=512 for instance). That way we can push the gradients to the right in the next figure, and have them not become zero. . . Of course we don&#39;t want those 512-scaled gradients to be in the weight update, so after converting them into FP32, we can divide them by this scale factor (once they have no risks of becoming 0). This changes the loop to: . compute the output with the FP16 model, then the loss. | multiply the loss by scale then back-propagate the gradients in half-precision. | copy the gradients in FP32 precision then divide them by scale. | do the update on the master model (in FP32 precision). | copy the master model in the FP16 model. | For the last problem, the tricks offered by NVIDIA are to leave the batchnorm layers in single precision (they don&#39;t have many weights so it&#39;s not a big memory challenge) and compute the loss in single precision (which means converting the last output of the model in single precision before passing it to the loss). . . Implementing all of this in the new callback system is surprisingly easy, let&#39;s dig into this! . Util functions . Before going in the main Callback we will need some helper functions. We will refactor using the APEX library util functions. The python-only build is enough for what we will use here if you don&#39;t manage to do the CUDA/C++ installation. . #collapse import apex.fp16_utils as fp16 . . Converting the model to FP16 . Jump_to lesson 12 video . We will need a function to convert all the layers of the model to FP16 precision except the BatchNorm-like layers (since those need to be done in FP32 precision to be stable). We do this in two steps: first we convert the model to FP16, then we loop over all the layers and put them back to FP32 if they are a BatchNorm layer. . #collapse_show bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d) . . #collapse_show def bn_to_float(model): if isinstance(model, bn_types): model.float() for child in model.children(): bn_to_float(child) return model . . #collapse_show def model_to_half(model): model = model.half() return bn_to_float(model) . . Let&#39;s test this: . #collapse_show model = nn.Sequential(nn.Linear(10,30), nn.BatchNorm1d(30), nn.Linear(30,2)).cuda() model = model_to_half(model) . . #collapse_show def check_weights(model): for i,t in enumerate([torch.float16, torch.float32, torch.float16]): assert model[i].weight.dtype == t assert model[i].bias.dtype == t . . #collapse_show check_weights(model) . . In Apex, the function that does this for us is convert_network. We can use it to put the model in FP16 or back to FP32. . #collapse_show model = nn.Sequential(nn.Linear(10,30), nn.BatchNorm1d(30), nn.Linear(30,2)).cuda() model = fp16.convert_network(model, torch.float16) check_weights(model) . . Creating the master copy of the parameters . From our model parameters (mostly in FP16), we&#39;ll want to create a copy in FP32 (master parameters) that we will use for the step in the optimizer. Optionally, we concatenate all the parameters to do one flat big tensor, which can make that step a little bit faster. . #collapse_show from torch.nn.utils import parameters_to_vector def get_master(model, flat_master=False): model_params = [param for param in model.parameters() if param.requires_grad] if flat_master: master_param = parameters_to_vector([param.data.float() for param in model_params]) master_param = torch.nn.Parameter(master_param, requires_grad=True) if master_param.grad is None: master_param.grad = master_param.new(*master_param.size()) return model_params, [master_param] else: master_params = [param.clone().float().detach() for param in model_params] for param in master_params: param.requires_grad_(True) return model_params, master_params . . The util function from Apex to do this is prep_param_lists. . #collapse_show model_p,master_p = get_master(model) model_p1,master_p1 = fp16.prep_param_lists(model) . . #collapse_show def same_lists(ps1, ps2): assert len(ps1) == len(ps2) for (p1,p2) in zip(ps1,ps2): assert p1.requires_grad == p2.requires_grad assert torch.allclose(p1.data.float(), p2.data.float()) . . #collapse_show same_lists(model_p,model_p1) same_lists(model_p,master_p) same_lists(master_p,master_p1) same_lists(model_p1,master_p1) . . We can&#39;t use flat_master when there is a mix of FP32 and FP16 parameters (like batchnorm here). . #collapse_show model1 = nn.Sequential(nn.Linear(10,30), nn.Linear(30,2)).cuda() model1 = fp16.convert_network(model1, torch.float16) . . #collapse_show model_p,master_p = get_master(model1, flat_master=True) model_p1,master_p1 = fp16.prep_param_lists(model1, flat_master=True) . . #collapse_show same_lists(model_p,model_p1) same_lists(master_p,master_p1) . . #collapse_show assert len(master_p[0]) == 10*30 + 30 + 30*2 + 2 assert len(master_p1[0]) == 10*30 + 30 + 30*2 + 2 . . The thing is that we don&#39;t always want all the parameters of our model in the same parameter group, because we might: . want to do transfer learning and freeze some layers | apply discriminative learning rates | don&#39;t apply weight decay to some layers (like BatchNorm) or the bias terms | . So we actually need a function that splits the parameters of an optimizer (and not a model) according to the right parameter groups. . #collapse_show def get_master(opt, flat_master=False): model_params = [[param for param in pg if param.requires_grad] for pg in opt.param_groups] if flat_master: master_params = [] for pg in model_params: mp = parameters_to_vector([param.data.float() for param in pg]) mp = torch.nn.Parameter(mp, requires_grad=True) if mp.grad is None: mp.grad = mp.new(*mp.size()) master_params.append(mp) else: master_params = [[param.clone().float().detach() for param in pg] for pg in model_params] for pg in master_params: for param in pg: param.requires_grad_(True) return model_params, master_params . . Copy the gradients from model params to master params . After the backward pass, all gradients must be copied to the master params before the optimizer step can be done in FP32. We need a function for that (with a bit of adjustement if we have flat master). . #collapse_show def to_master_grads(model_params, master_params, flat_master:bool=False)-&gt;None: if flat_master: if master_params[0].grad is None: master_params[0].grad = master_params[0].data.new(*master_params[0].data.size()) master_params[0].grad.data.copy_(parameters_to_vector([p.grad.data.float() for p in model_params])) else: for model, master in zip(model_params, master_params): if model.grad is not None: if master.grad is None: master.grad = master.data.new(*master.data.size()) master.grad.data.copy_(model.grad.data) else: master.grad = None . . The corresponding function in the Apex utils is model_grads_to_master_grads. . #collapse_show x = torch.randn(20,10).half().cuda() z = model(x) loss = F.cross_entropy(z, torch.randint(0, 2, (20,)).cuda()) loss.backward() . . #collapse_show to_master_grads(model_p, master_p) . . #collapse_show def check_grads(m1, m2): for p1,p2 in zip(m1,m2): if p1.grad is None: assert p2.grad is None else: assert torch.allclose(p1.grad.data, p2.grad.data) . . #collapse_show check_grads(model_p, master_p) . . #collapse_show fp16.model_grads_to_master_grads(model_p, master_p) . . #collapse_show check_grads(model_p, master_p) . . Copy the master params to the model params . After the step, we need to copy back the master parameters to the model parameters for the next update. . #collapse_show from torch._utils import _unflatten_dense_tensors def to_model_params(model_params, master_params, flat_master:bool=False)-&gt;None: if flat_master: for model, master in zip(model_params, _unflatten_dense_tensors(master_params[0].data, model_params)): model.data.copy_(master) else: for model, master in zip(model_params, master_params): model.data.copy_(master.data) . . The corresponding function in Apex is master_params_to_model_params. . But we need to handle param groups . The thing is that we don&#39;t always want all the parameters of our model in the same parameter group, because we might: . want to do transfer learning and freeze some layers | apply discriminative learning rates | don&#39;t apply weight decay to some layers (like BatchNorm) or the bias terms | . So we actually need a function that splits the parameters of an optimizer (and not a model) according to the right parameter groups and the following functions need to handle lists of lists of parameters (one list of each param group in model_pgs and master_pgs) . #collapse_show def get_master(opt, flat_master=False): model_pgs = [[param for param in pg if param.requires_grad] for pg in opt.param_groups] if flat_master: master_pgs = [] for pg in model_pgs: mp = parameters_to_vector([param.data.float() for param in pg]) mp = torch.nn.Parameter(mp, requires_grad=True) if mp.grad is None: mp.grad = mp.new(*mp.size()) master_pgs.append([mp]) else: master_pgs = [[param.clone().float().detach() for param in pg] for pg in model_pgs] for pg in master_pgs: for param in pg: param.requires_grad_(True) return model_pgs, master_pgs . . #collapse_show def to_master_grads(model_pgs, master_pgs, flat_master:bool=False)-&gt;None: for (model_params,master_params) in zip(model_pgs,master_pgs): fp16.model_grads_to_master_grads(model_params, master_params, flat_master=flat_master) . . #collapse_show def to_model_params(model_pgs, master_pgs, flat_master:bool=False)-&gt;None: for (model_params,master_params) in zip(model_pgs,master_pgs): fp16.master_params_to_model_params(model_params, master_params, flat_master=flat_master) . . The main Callback . Jump_to lesson 12 video . #collapse_show class MixedPrecision(Callback): _order = 99 def __init__(self, loss_scale=512, flat_master=False): assert torch.backends.cudnn.enabled, &quot;Mixed precision training requires cudnn.&quot; self.loss_scale,self.flat_master = loss_scale,flat_master def begin_fit(self): self.run.model = fp16.convert_network(self.model, dtype=torch.float16) self.model_pgs, self.master_pgs = get_master(self.opt, self.flat_master) #Changes the optimizer so that the optimization step is done in FP32. self.run.opt.param_groups = self.master_pgs #Put those param groups inside our runner. def after_fit(self): self.model.float() def begin_batch(self): self.run.xb = self.run.xb.half() #Put the inputs to half precision def after_pred(self): self.run.pred = self.run.pred.float() #Compute the loss in FP32 def after_loss(self): self.run.loss *= self.loss_scale #Loss scaling to avoid gradient underflow def after_backward(self): #Copy the gradients to master and unscale to_master_grads(self.model_pgs, self.master_pgs, self.flat_master) for master_params in self.master_pgs: for param in master_params: if param.grad is not None: param.grad.div_(self.loss_scale) def after_step(self): #Zero the gradients of the model since the optimizer is disconnected. self.model.zero_grad() #Update the params from master to model. to_model_params(self.model_pgs, self.master_pgs, self.flat_master) . . Now let&#39;s test this on Imagenette . #collapse_show path = Path(&quot;/media/cedric/Datasets/imagenette2-160/&quot;) . . #collapse_show tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] bs = 64 il = ImageList.from_files(path, tfms=tfms) sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=&#39;val&#39;)) ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4) . . #collapse_show nfs = [32,64,128,256,512] . . #collapse_show def get_learner(nfs, data, lr, layer, loss_func=F.cross_entropy, cb_funcs=None, opt_func=adam_opt(), **kwargs): model = get_cnn_model(data, nfs, layer, **kwargs) init_cnn(model) return Learner(model, data, loss_func, lr=lr, cb_funcs=cb_funcs, opt_func=opt_func) . . Training without mixed precision . #collapse cbfs = [partial(AvgStatsCallback,accuracy), ProgressCallback, CudaCallback, partial(BatchTransformXCallback, norm_imagenette)] . . #collapse learn = get_learner(nfs, data, 1e-2, conv_layer, cb_funcs=cbfs) . . #collapse learn.fit(1) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 1.912490 | 0.384624 | 1.778356 | 0.445096 | 00:06 | . Training with mixed precision . #collapse cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback, ProgressCallback, partial(BatchTransformXCallback, norm_imagenette), MixedPrecision] . . #collapse learn = get_learner(nfs, data, 1e-2, conv_layer, cb_funcs=cbfs) . . #collapse learn.fit(1) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 2.057630 | 0.339423 | 1.800098 | 0.424204 | 00:05 | . #collapse test_eq(next(learn.model.parameters()).type(), &#39;torch.cuda.FloatTensor&#39;) . . Dynamic loss scaling . The only annoying thing with the previous implementation of mixed precision training is that it introduces one new hyper-parameter to tune, the value of the loss scaling. Fortunately for us, there is a way around this. We want the loss scaling to be as high as possible so that our gradients can use the whole range of representation, so let&#39;s first try a really high value. In all likelihood, this will cause our gradients or our loss to overflow, and we will try again with half that big value, and again, until we get to the largest loss scale possible that doesn&#39;t make our gradients overflow. . This value will be perfectly fitted to our model and can continue to be dynamically adjusted as the training goes, if it&#39;s still too high, by just halving it each time we overflow. After a while though, training will converge and gradients will start to get smaller, so we also need a mechanism to get this dynamic loss scale larger if it&#39;s safe to do so. The strategy used in the Apex library is to multiply the loss scale by 2 each time we had a given number of iterations without overflowing. . To check if the gradients have overflowed, we check their sum (computed in FP32). If one term is nan, the sum will be nan. Interestingly, on the GPU, it&#39;s faster than checking torch.isnan: . Jump_to lesson 12 video . #collapse_show def test_overflow(x): s = float(x.float().sum()) return (s == float(&#39;inf&#39;) or s == float(&#39;-inf&#39;) or s != s) . . #collapse x = torch.randn(512,1024).cuda() . . #collapse test_overflow(x) . . False . #collapse x[123,145] = float(&#39;inf&#39;) test_overflow(x) . . True . #collapse %timeit test_overflow(x) . . 35.1 µs ± 3.87 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) . #collapse %timeit torch.isnan(x).any().item() . . 41.1 µs ± 643 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each) . So we can use it in the following function that checks for gradient overflow: . #collapse_show def grad_overflow(param_groups): for group in param_groups: for p in group: if p.grad is not None: s = float(p.grad.data.float().sum()) if s == float(&#39;inf&#39;) or s == float(&#39;-inf&#39;) or s != s: return True return False . . And now we can write a new version of the Callback that handles dynamic loss scaling. . #collapse_show class MixedPrecision(Callback): _order = 99 def __init__(self, loss_scale=512, flat_master=False, dynamic=True, max_loss_scale=2.**24, div_factor=2., scale_wait=500): assert torch.backends.cudnn.enabled, &quot;Mixed precision training requires cudnn.&quot; self.flat_master,self.dynamic,self.max_loss_scale = flat_master,dynamic,max_loss_scale self.div_factor,self.scale_wait = div_factor,scale_wait self.loss_scale = max_loss_scale if dynamic else loss_scale def begin_fit(self): self.run.model = fp16.convert_network(self.model, dtype=torch.float16) self.model_pgs, self.master_pgs = get_master(self.opt, self.flat_master) #Changes the optimizer so that the optimization step is done in FP32. self.run.opt.param_groups = self.master_pgs #Put those param groups inside our runner. if self.dynamic: self.count = 0 def begin_batch(self): self.run.xb = self.run.xb.half() #Put the inputs to half precision def after_pred(self): self.run.pred = self.run.pred.float() #Compute the loss in FP32 def after_loss(self): if self.in_train: self.run.loss *= self.loss_scale #Loss scaling to avoid gradient underflow def after_backward(self): #First, check for an overflow if self.dynamic and grad_overflow(self.model_pgs): #Divide the loss scale by div_factor, zero the grad (after_step will be skipped) self.loss_scale /= self.div_factor self.model.zero_grad() return True #skip step and zero_grad #Copy the gradients to master and unscale to_master_grads(self.model_pgs, self.master_pgs, self.flat_master) for master_params in self.master_pgs: for param in master_params: if param.grad is not None: param.grad.div_(self.loss_scale) #Check if it&#39;s been long enough without overflow if self.dynamic: self.count += 1 if self.count == self.scale_wait: self.count = 0 self.loss_scale *= self.div_factor def after_step(self): #Zero the gradients of the model since the optimizer is disconnected. self.model.zero_grad() #Update the params from master to model. to_model_params(self.model_pgs, self.master_pgs, self.flat_master) . . #collapse cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback, ProgressCallback, partial(BatchTransformXCallback, norm_imagenette), MixedPrecision] . . #collapse learn = get_learner(nfs, data, 1e-2, conv_layer, cb_funcs=cbfs) . . #collapse learn.fit(1) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 2.027682 | 0.328757 | 1.863258 | 0.434650 | 00:06 | . The loss scale used is way higher than our previous number: . #collapse learn.cbs[-1].loss_scale . . 65536.0 .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/21/2fp16.html",
            "relUrl": "/jupyter/2020/04/21/2fp16.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Fastai Course DL from the Foundations Mixup Label Smoothing",
            "content": "Mixup / Label smoothing . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_10 import * . . #collapse path = Path(&quot;/media/cedric/Datasets/imagenette2-160/&quot;) . . path.ls() . [PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/train&#39;), PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/val&#39;)] . Load and Pre Process data . #collapse_show tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] bs = 64 il = ImageList.from_files(path, tfms=tfms) sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=&#39;val&#39;)) ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4) . . Mixup . Jump_to lesson 12 video . What is mixup? . As the name kind of suggests, the authors of the mixup article propose to train the model on a mix of the pictures of the training set. Let&#39;s say we&#39;re on CIFAR10 for instance, then instead of feeding the model the raw images, we take two (which could be in the same class or not) and do a linear combination of them: in terms of tensor it&#39;s . new_image = t * image1 + (1-t) * image2 . where t is a float between 0 and 1. Then the target we assign to that image is the same combination of the original targets: . new_target = t * target1 + (1-t) * target2 . assuming your targets are one-hot encoded (which isn&#39;t the case in pytorch usually). And that&#39;s as simple as this. . #collapse_show img1 = PIL.Image.open(ll.train.x.items[0]) img1 . . #collapse_show img2 = PIL.Image.open(ll.train.x.items[4000]) img2 . . #collapse_show mixed_up = ll.train.x[0] * 0.3 + ll.train.x[4000] * 0.7 plt.imshow(mixed_up.permute(1,2,0)); . . French horn or tench? The right answer is 70% french horn and 30% tench ;) . Implementation . Jump_to lesson 12 video . The implementation relies on something called the beta distribution which in turns uses something which Jeremy still finds mildly terrifying called the gamma function. To get over his fears, Jeremy reminds himself that gamma is just a factorial function that (kinda) interpolates nice and smoothly to non-integers too. How it does that exactly isn&#39;t important... . #collapse_show # PyTorch has a log-gamma but not a gamma, so we&#39;ll create one Γ = lambda x: x.lgamma().exp() . . Use math symbols in code ! . NB: If you see math symbols you don&#39;t know you can google them like this: Γ function. . If you&#39;re not used to typing unicode symbols, on Mac type ctrl-cmd-space to bring up a searchable emoji box. On Linux you can use the compose key. On Windows you can also use a compose key, but you first need to install WinCompose. By default the compose key is the right-hand Alt key. . You can search for symbol names in WinCompose. The greek letters are generally compose- *-letter (where letter is, for instance, a to get greek α alpha). . #collapse_show facts = [math.factorial(i) for i in range(7)] . . #collapse_show plt.plot(range(7), facts, &#39;ro&#39;) plt.plot(torch.linspace(0,6), Γ(torch.linspace(0,6)+1)) plt.legend([&#39;factorial&#39;,&#39;Γ&#39;]); . . #collapse_show torch.linspace(0,0.9,10) . . tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000, 0.9000]) . In the original article, the authors suggested three things: . Create two separate dataloaders and draw a batch from each at every iteration to mix them up | Draw a t value following a beta distribution with a parameter α (0.4 is suggested in their article) | Mix up the two batches with the same value t. | Use one-hot encoded targets | Why the beta distribution with the same parameters α? Well it looks like this: . #collapse_show _,axs = plt.subplots(1,2, figsize=(12,4)) x = torch.linspace(0,1, 100) for α,ax in zip([0.1,0.8], axs): α = tensor(α) # y = (x.pow(α-1) * (1-x).pow(α-1)) / (gamma_func(α ** 2) / gamma_func(α)) y = (x**(α-1) * (1-x)**(α-1)) / (Γ(α)**2 / Γ(2*α)) ax.plot(x,y) ax.set_title(f&quot;α={α:.1}&quot;) . . With a low α, we pick values close to 0. and 1. with a high probability, and the values in the middle all have the same kind of probability. With a greater α, 0. and 1. get a lower probability . . While the approach above works very well, it&#39;s not the fastest way we can do this. The main point that slows down this process is wanting two different batches at every iteration (which means loading twice the amount of images and applying to them the other data augmentation function). To avoid this slow down, we can be a little smarter and mixup a batch with a shuffled version of itself (this way the images mixed up are still different). This was a trick suggested in the MixUp paper. . Then pytorch was very careful to avoid one-hot encoding targets when it could, so it seems a bit of a drag to undo this. Fortunately for us, if the loss is a classic cross-entropy, we have . loss(output, new_target) = t * loss(output, target1) + (1-t) * loss(output, target2) . so we won&#39;t one-hot encode anything and just compute those two losses then do the linear combination. . Using the same parameter t for the whole batch also seemed a bit inefficient. In our experiments, we noticed that the model can train faster if we draw a different t for every image in the batch (both options get to the same result in terms of accuracy, it&#39;s just that one arrives there more slowly). The last trick we have to apply with this is that there can be some duplicates with this strategy: let&#39;s say or shuffle say to mix image0 with image1 then image1 with image0, and that we draw t=0.1 for the first, and t=0.9 for the second. Then . image0 * 0.1 + shuffle0 * (1-0.1) = image0 * 0.1 + image1 * 0.9 image1 * 0.9 + shuffle1 * (1-0.9) = image1 * 0.9 + image0 * 0.1 . will be the same. Of course, we have to be a bit unlucky but in practice, we saw there was a drop in accuracy by using this without removing those near-duplicates. To avoid them, the tricks is to replace the vector of parameters we drew by . t = max(t, 1-t) . The beta distribution with the two parameters equal is symmetric in any case, and this way we insure that the biggest coefficient is always near the first image (the non-shuffled batch). . In Mixup we have handle loss functions that have an attribute reduction (like nn.CrossEntropy()). To deal with the reduction=None with various types of loss function without modifying the actual loss function outside of the scope we need to perform those operations with no reduction, we create a context manager: . #collapse_show class NoneReduce(): def __init__(self, loss_func): self.loss_func,self.old_red = loss_func,None def __enter__(self): if hasattr(self.loss_func, &#39;reduction&#39;): self.old_red = getattr(self.loss_func, &#39;reduction&#39;) setattr(self.loss_func, &#39;reduction&#39;, &#39;none&#39;) return self.loss_func else: return partial(self.loss_func, reduction=&#39;none&#39;) def __exit__(self, type, value, traceback): if self.old_red is not None: setattr(self.loss_func, &#39;reduction&#39;, self.old_red) . . Then we can use it in MixUp: . #collapse_show from torch.distributions.beta import Beta def unsqueeze(input, dims): for dim in listify(dims): input = torch.unsqueeze(input, dim) return input def reduce_loss(loss, reduction=&#39;mean&#39;): return loss.mean() if reduction==&#39;mean&#39; else loss.sum() if reduction==&#39;sum&#39; else loss . . #collapse_show class MixUp(Callback): _order = 90 #Runs after normalization and cuda def __init__(self, α:float=0.4): self.distrib = Beta(tensor([α]), tensor([α])) def begin_fit(self): self.old_loss_func,self.run.loss_func = self.run.loss_func,self.loss_func def begin_batch(self): if not self.in_train: return #Only mixup things during training λ = self.distrib.sample((self.yb.size(0),)).squeeze().to(self.xb.device) λ = torch.stack([λ, 1-λ], 1) self.λ = unsqueeze(λ.max(1)[0], (1,2,3)) shuffle = torch.randperm(self.yb.size(0)).to(self.xb.device) xb1,self.yb1 = self.xb[shuffle],self.yb[shuffle] self.run.xb = lin_comb(self.xb, xb1, self.λ) def after_fit(self): self.run.loss_func = self.old_loss_func def loss_func(self, pred, yb): if not self.in_train: return self.old_loss_func(pred, yb) with NoneReduce(self.old_loss_func) as loss_func: loss1 = loss_func(pred, yb) loss2 = loss_func(pred, self.yb1) loss = lin_comb(loss1, loss2, self.λ) return reduce_loss(loss, getattr(self.old_loss_func, &#39;reduction&#39;, &#39;mean&#39;)) . . #collapse_show nfs = [32,64,128,256,512] . . #collapse_show def get_learner(nfs, data, lr, layer, loss_func=F.cross_entropy, cb_funcs=None, opt_func=optim.SGD, **kwargs): model = get_cnn_model(data, nfs, layer, **kwargs) init_cnn(model) return Learner(model, data, loss_func, lr=lr, cb_funcs=cb_funcs, opt_func=opt_func) . . #collapse_show cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback, ProgressCallback, partial(BatchTransformXCallback, norm_imagenette), MixUp] . . #collapse_show learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs) . . #collapse_show learn.fit(1) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 2.277292 | 0.303094 | 1.947020 | 0.382166 | 00:05 | . Questions: How does softmax interact with all this? Should we jump straight from mixup to inference? . Label smoothing . Another regularization technique that&#39;s often used is label smoothing. It&#39;s designed to make the model a little bit less certain of it&#39;s decision by changing a little bit its target: instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict 1-ε for the correct class and ε for all the others, with ε a (small) positive number and N the number of classes. This can be written as: . $$loss = (1-ε) ce(i) + ε sum ce(j) / N$$ . where ce(x) is cross-entropy of x (i.e. $- log(p_{x})$), and i is the correct class. This can be coded in a loss function: . Jump_to lesson 12 video . #collapse_show class LabelSmoothingCrossEntropy(nn.Module): def __init__(self, ε:float=0.1, reduction=&#39;mean&#39;): super().__init__() self.ε,self.reduction = ε,reduction def forward(self, output, target): c = output.size()[-1] log_preds = F.log_softmax(output, dim=-1) loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction) nll = F.nll_loss(log_preds, target, reduction=self.reduction) return lin_comb(loss/c, nll, self.ε) . . Note: we implement the various reduction attributes so that it plays nicely with MixUp after. . #collapse_show cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback, ProgressCallback, partial(BatchTransformXCallback, norm_imagenette)] . . #collapse_show learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs, loss_func=LabelSmoothingCrossEntropy()) . . #collapse_show learn.fit(1) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;1&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 2.953865 | 0.261802 | 2.730365 | 0.358726 | 00:05 | . And we can check our loss function reduction attribute hasn&#39;t changed outside of the training loop: . #collapse_show assert learn.loss_func.reduction == &#39;mean&#39; . .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/21/1mixup-label-smoothing.html",
            "relUrl": "/jupyter/2020/04/21/1mixup-label-smoothing.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Fastai Course DL from the Foundations Data Augmentation",
            "content": "Fastai Data augmentation . This Post is based on the Notebok by the Fastai Course Part2 | . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_09c import * . . PIL transforms . We start with PIL transforms to resize all our images to the same size. Then, when they are in a batch, we can apply data augmentation to all of them at the same time on the GPU. We have already seen the basics of resizing and putting on the GPU in 08, but we&#39;ll look more into it now. . Jump_to lesson 11 video . View images . #collapse make_rgb._order=0 . . #collapse path = Path(&quot;/media/cedric/Datasets/imagenette2-160/&quot;) tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] . . #collapse def get_il(tfms): return ImageList.from_files(path, tfms=tfms) . . #collapse il = get_il(tfms) . . #collapse show_image(il[0]) . . #collapse img = PIL.Image.open(il.items[0]) . . #collapse img . . #collapse img.getpixel((1,1)) . . (91, 1, 1) . #collapse import numpy as np . . #collapse %timeit -n 10 a = np.array(PIL.Image.open(il.items[0])) . . 499 µs ± 35.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . Be careful of resampling methods, you can quickly lose some textures! Sometimes combining methods can be an advantage ! . #collapse img.resize((128,128), resample=PIL.Image.ANTIALIAS) . . #collapse img.resize((128,128), resample=PIL.Image.BILINEAR) . . #collapse img.resize((128,128), resample=PIL.Image.NEAREST) . . #collapse_show img.resize((256,256), resample=PIL.Image.BICUBIC).resize((128,128), resample=PIL.Image.NEAREST) . . #collapse_show img.resize((256,256),resample=PIL.Image.BICUBIC).resize((128,128),resample=PIL.Image.BICUBIC) . . #collapse %timeit img.resize((224,224), resample=PIL.Image.BICUBIC) . . 646 µs ± 2.68 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . #collapse %timeit img.resize((224,224), resample=PIL.Image.BILINEAR) . . 475 µs ± 3.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) . #collapse %timeit -n 10 img.resize((224,224), resample=PIL.Image.NEAREST) . . 34.7 µs ± 6.38 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . Flip . Flip can be done with PIL very fast. . Jump_to lesson 11 video . #collapse import random . . #collapse random.random() . . 0.2562458266540195 . #collapse_show def pil_random_flip(x): return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()&lt;0.5 else x . . #collapse_show il1 = get_il(tfms) print(il1) il1.items = [il1.items[0]]*64 dl = DataLoader(il1, 8) . . ImageList (13394 items) [PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/train/n03888257/n03888257_25687.JPEG&#39;), PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/train/n03888257/n03888257_68953.JPEG&#39;), PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/train/n03888257/n03888257_21027.JPEG&#39;), PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/train/n03888257/n03888257_23113.JPEG&#39;), PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/train/n03888257/n03888257_11587.JPEG&#39;), PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/train/n03888257/n03888257_18947.JPEG&#39;), PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/train/n03888257/n03888257_50106.JPEG&#39;), PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/train/n03888257/n03888257_9449.JPEG&#39;), PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/train/n03888257/n03888257_25565.JPEG&#39;), PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/train/n03888257/n03888257_32216.JPEG&#39;)...] Path: /media/cedric/Datasets/imagenette2-160 . #collapse x = next(iter(dl)) . . Here is a convenience function to look at images in a batch. . #collapse_show def show_image(im, ax=None, figsize=(3,3)): if ax is None: _,ax = plt.subplots(1, 1, figsize=figsize) ax.axis(&#39;off&#39;) ax.imshow(im.permute(1,2,0)) def show_batch(x, c=4, r=None, figsize=None): n = len(x) (_,h,w) = x[0].shape if r is None: r = int(math.ceil(n/c)) if figsize is None: figsize=(c*3,r*3) fig,axes = plt.subplots(r,c, figsize=figsize) for xi,ax in zip(x,axes.flat): show_image(xi, ax) . . Without data augmentation: . #collapse_show show_batch(x) . . With random flip: . #collapse_show il1.tfms.append(pil_random_flip) . . #collapse_show x = next(iter(dl)) show_batch(x) . . We can also make that transform a class so it&#39;s easier to set the value of the parameter p. As seen before, it also allows us to set the _order attribute. . #collapse_show class PilRandomFlip(Transform): _order=11 def __init__(self, p=0.5): self.p=p def __call__(self, x): return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()&lt;self.p else x . . #collapse_show class PilTransform(Transform): _order=11 class PilRandomFlip(PilTransform): def __init__(self, p=0.5): self.p=p def __call__(self, x): return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()&lt;self.p else x . . #collapse_show del(il1.tfms[-1]) il1.tfms.append(PilRandomFlip(0.8)) . . #collapse_show x = next(iter(dl)) show_batch(x) . . PIL can also do the whole dihedral group of transformations (random horizontal flip, random vertical flip and the four 90 degrees rotation) with the transpose method. Here are the codes of a few transformations: . #collapse_show PIL.Image.FLIP_LEFT_RIGHT,PIL.Image.ROTATE_270,PIL.Image.TRANSVERSE . . (0, 4, 6) . Be careful that img.transpose(0) is already one transform, so doing nothing requires a separate case, then we have 7 different transformations. . #collapse_show img = PIL.Image.open(il.items[0]) img = img.resize((128,128), resample=PIL.Image.NEAREST) _, axs = plt.subplots(2, 4, figsize=(12, 6)) for i,ax in enumerate(axs.flatten()): if i==0: ax.imshow(img) else: ax.imshow(img.transpose(i-1)) ax.axis(&#39;off&#39;) . . And we can implement it like this: . #collapse_show class PilRandomDihedral(PilTransform): def __init__(self, p=0.75): self.p=p*7/8 #Little hack to get the 1/8 identity dihedral transform taken into account. def __call__(self, x): if random.random()&gt;self.p: return x return x.transpose(random.randint(0,6)) . . #collapse_show del(il1.tfms[-1]) il1.tfms.append(PilRandomDihedral()) . . #collapse_show show_batch(next(iter(dl))) . . Random crop . Jump_to lesson 11 video . #collapse_show img = PIL.Image.open(il.items[0]) img.size . . (160, 213) . To crop an image with PIL we have to specify the top/left and bottom/right corner in this format: (left, top, right, bottom). We won&#39;t just crop the size we want, but first crop the section we want of the image and then apply a resize. In what follows, we call the first one the crop_size. . #collapse_show img.crop((60,60,320,320)).resize((128,128), resample=PIL.Image.BILINEAR) . . #collapse cnr2 = (60,60,320,320) resample = PIL.Image.BILINEAR . . This is pretty fast in PIL: . #collapse %timeit -n 10 img.crop(cnr2).resize((128,128), resample=resample) . . 367 µs ± 11.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . Our time budget: aim for 5 mins per batch for imagenet on 8 GPUs. 1.25m images in imagenet. So on one GPU per minute that&#39;s 1250000/8/5 == 31250, or 520 per second. Assuming 4 cores per GPU, then we want ~125 images per second - so try to stay &lt;10ms per image. Here we have time to do more things. For instance, we can do the crop and resize in the same call to transform, which will give a smoother result. . #collapse img.transform((128,128), PIL.Image.EXTENT, cnr2, resample=resample) . . #collapse %timeit -n 10 img.transform((128,128), PIL.Image.EXTENT, cnr2, resample=resample) . . 182 µs ± 2.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . It&#39;s a little bit slower but still fast enough for our purpose, so we will use this. We then define a general crop transform and two subclasses: one to crop at the center (for validation) and one to randomly crop. Each time, the subclass only implements the way to get the four corners passed to PIL. . #collapse_show from random import randint def process_sz(sz): sz = listify(sz) return tuple(sz if len(sz)==2 else [sz[0],sz[0]]) def default_crop_size(w,h): return [w,w] if w &lt; h else [h,h] class GeneralCrop(PilTransform): def __init__(self, size, crop_size=None, resample=PIL.Image.BILINEAR): self.resample,self.size = resample,process_sz(size) self.crop_size = None if crop_size is None else process_sz(crop_size) def default_crop_size(self, w,h): return default_crop_size(w,h) def __call__(self, x): csize = self.default_crop_size(*x.size) if self.crop_size is None else self.crop_size return x.transform(self.size, PIL.Image.EXTENT, self.get_corners(*x.size, *csize), resample=self.resample) def get_corners(self, w, h): return (0,0,w,h) class CenterCrop(GeneralCrop): def __init__(self, size, scale=1.14, resample=PIL.Image.BILINEAR): super().__init__(size, resample=resample) self.scale = scale def default_crop_size(self, w,h): return [w/self.scale,h/self.scale] def get_corners(self, w, h, wc, hc): return ((w-wc)//2, (h-hc)//2, (w-wc)//2+wc, (h-hc)//2+hc) . . #collapse_show il1.tfms = [make_rgb, CenterCrop(128), to_byte_tensor, to_float_tensor] . . #collapse_show show_batch(next(iter(dl))) . . RandomResizeCrop . This is the usual data augmentation used on ImageNet (introduced here) that consists of selecting 8 to 100% of the image area and a scale between 3/4 and 4/3 as a crop, then resizing it to the desired size. It combines some zoom and a bit of squishing at a very low computational cost. . Jump_to lesson 11 video . #collapse_show class RandomResizedCrop(GeneralCrop): def __init__(self, size, scale=(0.08,1.0), ratio=(3./4., 4./3.), resample=PIL.Image.BILINEAR): super().__init__(size, resample=resample) self.scale,self.ratio = scale,ratio def get_corners(self, w, h, wc, hc): area = w*h #Tries 10 times to get a proper crop inside the image. for attempt in range(10): area = random.uniform(*self.scale) * area ratio = math.exp(random.uniform(math.log(self.ratio[0]), math.log(self.ratio[1]))) new_w = int(round(math.sqrt(area * ratio))) new_h = int(round(math.sqrt(area / ratio))) if new_w &lt;= w and new_h &lt;= h: left = random.randint(0, w - new_w) top = random.randint(0, h - new_h) return (left, top, left + new_w, top + new_h) # Fallback to squish if w/h &lt; self.ratio[0]: size = (w, int(w/self.ratio[0])) elif w/h &gt; self.ratio[1]: size = (int(h*self.ratio[1]), h) else: size = (w, h) return ((w-size[0])//2, (h-size[1])//2, (w+size[0])//2, (h+size[1])//2) . . #collapse_show il1.tfms = [make_rgb, RandomResizedCrop(128), to_byte_tensor, to_float_tensor] . . #collapse_show show_batch(next(iter(dl))) . . Perspective warping . To do perspective warping, we map the corners of the image to new points: for instance, if we want to tilt the image so that the top looks closer to us, the top/left corner needs to be shifted to the right and the top/right to the left. To avoid squishing, the bottom/left corner needs to be shifted to the left and the bottom/right corner to the right. For instance, if we have an image with corners in: . (60,60,60,280,280,280,280,60) . (top/left, bottom/left, bottom/right, top/right) then a warped version is: . (90,60,30,280,310,280,250,60) . PIL can do this for us but it requires 8 coefficients we need to calculate. The math isn&#39;t the most important here, as we&#39;ve done it for you. We need to solve this equation. The equation solver is called torch.solve in PyTorch. . Jump_to lesson 11 video . #collapse_show from torch import FloatTensor,LongTensor def find_coeffs(orig_pts, targ_pts): matrix = [] #The equations we&#39;ll need to solve. for p1, p2 in zip(targ_pts, orig_pts): matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]]) matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]]) A = FloatTensor(matrix) B = FloatTensor(orig_pts).view(8, 1) #The 8 scalars we seek are solution of AX = B return list(torch.solve(B,A)[0][:,0]) . . #collapse_show def warp(img, size, src_coords, resample=PIL.Image.BILINEAR): w,h = size targ_coords = ((0,0),(0,h),(w,h),(w,0)) c = find_coeffs(src_coords,targ_coords) res = img.transform(size, PIL.Image.PERSPECTIVE, list(c), resample=resample) return res . . #collapse_show targ = ((0,0),(0,128),(128,128),(128,0)) src = ((90,60),(30,280),(310,280),(250,60)) . . #collapse_show c = find_coeffs(src, targ) img.transform((128,128), PIL.Image.PERSPECTIVE, list(c), resample=resample) . . #collapse %timeit -n 10 warp(img, (128,128), src) . . 292 µs ± 13.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . #collapse %timeit -n 10 warp(img, (128,128), src, resample=PIL.Image.NEAREST) . . 192 µs ± 12.6 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . #collapse warp(img, (64,64), src, resample=PIL.Image.BICUBIC) . . #collapse warp(img, (64,64), src, resample=PIL.Image.NEAREST) . . #collapse_show def uniform(a,b): return a + (b-a) * random.random() . . We can add a transform to do this perspective warping automatically with the rand resize and crop. . #collapse_show class PilTiltRandomCrop(PilTransform): def __init__(self, size, crop_size=None, magnitude=0., resample=PIL.Image.NEAREST): self.resample,self.size,self.magnitude = resample,process_sz(size),magnitude self.crop_size = None if crop_size is None else process_sz(crop_size) def __call__(self, x): csize = default_crop_size(*x.size) if self.crop_size is None else self.crop_size up_t,lr_t = uniform(-self.magnitude, self.magnitude),uniform(-self.magnitude, self.magnitude) left,top = randint(0,x.size[0]-csize[0]),randint(0,x.size[1]-csize[1]) src_corners = tensor([[-up_t, -lr_t], [up_t, 1+lr_t], [1-up_t, 1-lr_t], [1+up_t, lr_t]]) src_corners = src_corners * tensor(csize).float() + tensor([left,top]).float() src_corners = tuple([(int(o[0].item()), int(o[1].item())) for o in src_corners]) return warp(x, self.size, src_corners, resample=self.resample) . . #collapse_show il1.tfms = [make_rgb, PilTiltRandomCrop(128, magnitude=0.1), to_byte_tensor, to_float_tensor] . . #collapse_show x = next(iter(dl)) show_batch(x) . . Problem is that black padding appears as soon as our target points are outside of the image, so we have to limit the magnitude if we want to avoid that. . #collapse_show class PilTiltRandomCrop(PilTransform): def __init__(self, size, crop_size=None, magnitude=0., resample=PIL.Image.BILINEAR): self.resample,self.size,self.magnitude = resample,process_sz(size),magnitude self.crop_size = None if crop_size is None else process_sz(crop_size) def __call__(self, x): csize = default_crop_size(*x.size) if self.crop_size is None else self.crop_size left,top = randint(0,x.size[0]-csize[0]),randint(0,x.size[1]-csize[1]) top_magn = min(self.magnitude, left/csize[0], (x.size[0]-left)/csize[0]-1) lr_magn = min(self.magnitude, top /csize[1], (x.size[1]-top) /csize[1]-1) up_t,lr_t = uniform(-top_magn, top_magn),uniform(-lr_magn, lr_magn) src_corners = tensor([[-up_t, -lr_t], [up_t, 1+lr_t], [1-up_t, 1-lr_t], [1+up_t, lr_t]]) src_corners = src_corners * tensor(csize).float() + tensor([left,top]).float() src_corners = tuple([(int(o[0].item()), int(o[1].item())) for o in src_corners]) return warp(x, self.size, src_corners, resample=self.resample) . . #collapse_show il1.tfms = [make_rgb, PilTiltRandomCrop(128, 200, magnitude=0.2), to_byte_tensor, to_float_tensor] . . Faster tensor creation . Jump_to lesson 11 video . #collapse_show [(o._order,o) for o in sorted(tfms, key=operator.attrgetter(&#39;_order&#39;))] . . [(0, &lt;function exp.nb_08.make_rgb(item)&gt;), (10, &lt;exp.nb_08.ResizeFixed at 0x7f25a9abe710&gt;), (11, &lt;__main__.PilRandomDihedral at 0x7f25442f0908&gt;), (20, &lt;function exp.nb_08.to_byte_tensor(item)&gt;), (30, &lt;function exp.nb_08.to_float_tensor(item)&gt;)] . #collapse_show import numpy as np def np_to_float(x): return torch.from_numpy(np.array(x, dtype=np.float32, copy=False)).permute(2,0,1).contiguous()/255. np_to_float._order = 30 . . It is actually faster to combine to_float_tensor and to_byte_tensor in one transform using numpy. . #collapse %timeit -n 10 to_float_tensor(to_byte_tensor(img)) . . 138 µs ± 10.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . #collapse %timeit -n 10 np_to_float(img) . . 224 µs ± 17.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . Batch data augmentation . You can write your own augmentation for your domain&#39;s data types, and have them run on the GPU, by using regular PyTorch tensor operations. Here&#39;s an example for images. The key is to do them on a whole batch at a time. Nearly all PyTorch operations can be done batch-wise. . Once we have resized our images so that we can batch them together, we can apply more data augmentation on a batch level. For the affine/coord transforms, we proceed like this: . generate a grid map of the size of our batch (bs x height x width x 2) that contains the coordinates of a grid of size height x width (this will be the final size of the image, and doesn&#39;t have to be the same as the current size in the batch) | apply the affine transforms (which is a matrix multiplication) and the coord transforms to that grid map | interpolate the values of the final pixels we want from the initial images in the batch, according to the transformed grid map | For 1. and 3. there are PyTorch functions: F.affine_grid and F.grid_sample. F.affine_grid can even combine 1 and 2 if we just want to do an affine transformation. . Jump_to lesson 11 video . Step 1: generate the grid . #collapse il1.tfms = [make_rgb, PilTiltRandomCrop(128, magnitude=0.2), to_byte_tensor, to_float_tensor] . . #collapse dl = DataLoader(il1, 64) . . #collapse x = next(iter(dl)) . . #collapse from torch import FloatTensor . . #collapse_show def affine_grid_cpu(size): N, C, H, W = size grid = FloatTensor(N, H, W, 2) linear_points = torch.linspace(-1, 1, W) if W &gt; 1 else tensor([-1]) grid[:, :, :, 0] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, :, 0]) linear_points = torch.linspace(-1, 1, H) if H &gt; 1 else tensor([-1]) grid[:, :, :, 1] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, :, 1]) return grid . . #collapse_show grid = affine_grid_cpu(x.size()) . . #collapse_show grid.shape . . torch.Size([64, 128, 128, 2]) . #collapse_show grid[0,:5,:5] . . tensor([[[-1.0000, -1.0000], [-0.9843, -1.0000], [-0.9685, -1.0000], [-0.9528, -1.0000], [-0.9370, -1.0000]], [[-1.0000, -0.9843], [-0.9843, -0.9843], [-0.9685, -0.9843], [-0.9528, -0.9843], [-0.9370, -0.9843]], [[-1.0000, -0.9685], [-0.9843, -0.9685], [-0.9685, -0.9685], [-0.9528, -0.9685], [-0.9370, -0.9685]], [[-1.0000, -0.9528], [-0.9843, -0.9528], [-0.9685, -0.9528], [-0.9528, -0.9528], [-0.9370, -0.9528]], [[-1.0000, -0.9370], [-0.9843, -0.9370], [-0.9685, -0.9370], [-0.9528, -0.9370], [-0.9370, -0.9370]]]) . #collapse %timeit -n 10 grid = affine_grid_cpu(x.size()) . . 1.43 ms ± 245 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . Coords in the grid go from -1, to 1 (PyTorch convention). . PyTorch version is slower on the CPU but optimized to go very fast on the GPU . #collapse_show m = tensor([[1., 0., 0.], [0., 1., 0.]]) theta = m.expand(x.size(0), 2, 3) . . #collapse_show theta.shape . . torch.Size([64, 2, 3]) . #collapse_show %timeit -n 10 grid = F.affine_grid(theta, x.size()) . . /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; . 5.88 ms ± 731 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . #collapse_show %timeit -n 10 grid = F.affine_grid(theta.cuda(), x.size()) . . The slowest run took 15.01 times longer than the fastest. This could mean that an intermediate result is being cached. 3.76 ms ± 5.95 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . So we write our own version that dispatches on the CPU with our function and uses PyTorch&#39;s on the GPU. . #collapse_show def affine_grid(x, size): size = (size,size) if isinstance(size, int) else tuple(size) size = (x.size(0),x.size(1)) + size if x.device.type == &#39;cpu&#39;: return affine_grid_cpu(size) m = tensor([[1., 0., 0.], [0., 1., 0.]], device=x.device) return F.affine_grid(m.expand(x.size(0), 2, 3), size) . . #collapse_show grid = affine_grid(x, 128) . . Step 2: Affine multiplication . In 2D an affine transformation has the form y = Ax + b where A is a 2x2 matrix and b a vector with 2 coordinates. It&#39;s usually represented by the 3x3 matrix . A[0,0] A[0,1] b[0] A[1,0] A[1,1] b[1] 0 0 1 . because then the composition of two affine transforms can be computed with the matrix product of their 3x3 representations. . #collapse_show from torch import stack,zeros_like,ones_like . . The matrix for a rotation that has an angle of theta is: . cos(theta) -sin(theta) 0 sin(theta) cos(theta) 0 0 0 1 . Here we have to apply the reciprocal of a regular rotation (exercise: find why!) so we use this matrix: . cos(theta) sin(theta) 0 -sin(theta) cos(theta) 0 0 0 1 . then we draw a different theta for each version of the image in the batch to return a batch of rotation matrices (size bs x 3 x 3). . #collapse_show def rotation_matrix(thetas): thetas.mul_(math.pi/180) rows = [stack([thetas.cos(), thetas.sin(), torch.zeros_like(thetas)], dim=1), stack([-thetas.sin(), thetas.cos(), torch.zeros_like(thetas)], dim=1), stack([torch.zeros_like(thetas), torch.zeros_like(thetas), torch.ones_like(thetas)], dim=1)] return stack(rows, dim=1) . . #collapse thetas = torch.empty(x.size(0)).uniform_(-30,30) . . #collapse thetas[:5] . . tensor([ -8.2746, 9.1014, 26.1341, -27.8994, -28.8064]) . #collapse m = rotation_matrix(thetas) . . #collapse m.shape, m[:,None].shape, grid.shape . . (torch.Size([64, 3, 3]), torch.Size([64, 1, 3, 3]), torch.Size([64, 128, 128, 2])) . #collapse grid.view(64,-1,2).shape . . torch.Size([64, 16384, 2]) . We have to apply our rotation to every point in the grid. The matrix a is given by the first two rows and two columns of m and the vector b is the first two coefficients of the last column. Of course we have to deal with the fact that here m is a batch of matrices. . #collapse a = m[:,:2,:2] b = m[:, 2:,:2] tfm_grid = (grid.view(64,-1,2) @ a + b).view(64, 128, 128, 2) . . We can also do this without the view by using broadcasting. . #collapse %timeit -n 10 tfm_grid = grid @ m[:,None,:2,:2] + m[:,2,:2][:,None,None] . . 11 ms ± 1.61 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . #collapse %timeit -n 10 tfm_grid = torch.einsum(&#39;bijk,bkl-&gt;bijl&#39;, grid, m[:,:2,:2]) + m[:,2,:2][:,None,None] . . 10 ms ± 417 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . #collapse %timeit -n 10 tfm_grid = torch.matmul(grid, m[:,:2,:2].unsqueeze(1)) + m[:,2,:2][:,None,None] . . 10.3 ms ± 404 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . #collapse %timeit -n 10 tfm_grid = (torch.bmm(grid.view(64,-1,2), m[:,:2,:2]) + m[:,2,:2][:,None]).view(-1, 128, 128, 2) . . 9.77 ms ± 63.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . And on the GPU . #collapse grid = grid.cuda() m = m.cuda() . . #collapse %timeit -n 10 tfm_grid = grid @ m[:,None,:2,:2] + m[:,2,:2][:,None,None] . . 88 µs ± 51 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . #collapse %timeit -n 10 tfm_grid = torch.einsum(&#39;bijk,bkl-&gt;bijl&#39;, grid, m[:,:2,:2]) + m[:,2,:2][:,None,None] . . 75.7 µs ± 11.5 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . #collapse %timeit -n 10 tfm_grid = torch.matmul(grid, m[:,:2,:2].unsqueeze(1)) + m[:,2,:2][:,None,None] . . 74.3 µs ± 10.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . #collapse %timeit -n 10 tfm_grid = (torch.bmm(grid.view(64,-1,2), m[:,:2,:2]) + m[:,2,:2][:,None]).view(-1, 128, 128, 2) . . 46.9 µs ± 10.7 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . Step 3: interpolate . Since bmm is always the fastest, we use this one for the matrix multiplication. . #collapse_show tfm_grid = torch.bmm(grid.view(64,-1,2), m[:,:2,:2]).view(-1, 128, 128, 2) . . The interpolation to find our coordinates back is done by grid_sample. . #collapse_show tfm_x = F.grid_sample(x, tfm_grid.cpu()) . . /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; . #collapse_show show_batch(tfm_x, r=2) . . It takes a padding_mode argument. . #collapse_show tfm_x = F.grid_sample(x, tfm_grid.cpu(), padding_mode=&#39;reflection&#39;) . . #collapse_show show_batch(tfm_x, r=2) . . Timing . Let&#39;s look at the speed now! . #collapse_show def rotate_batch(x, size, degrees): grid = affine_grid(x, size) thetas = x.new(x.size(0)).uniform_(-degrees,degrees) m = rotation_matrix(thetas) tfm_grid = grid @ m[:,:2,:2].unsqueeze(1) + m[:,2,:2][:,None,None] return F.grid_sample(x, tfm_grid) . . #collapse show_batch(rotate_batch(x, 128, 30), r=2) . . #collapse %timeit -n 10 tfm_x = rotate_batch(x, 128, 30) . . 25 ms ± 673 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) . #collapse %timeit -n 10 tfm_x = rotate_batch(x.cuda(), 128, 30) . . The slowest run took 22.41 times longer than the fastest. This could mean that an intermediate result is being cached. 17.5 ms ± 31.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . Not bad for 64 rotations! . Jit version . But we can be even faster! . #collapse from torch import Tensor . . #collapse_show from torch.jit import script @script def rotate_batch(x:Tensor, size:int, degrees:float) -&gt; Tensor: sz = (x.size(0),x.size(1)) + (size,size) idm = torch.zeros(2,3, device=x.device) idm[0,0] = 1. idm[1,1] = 1. grid = F.affine_grid(idm.expand(x.size(0), 2, 3), sz) thetas = torch.zeros(x.size(0), device=x.device).uniform_(-degrees,degrees) m = rotation_matrix(thetas) tfm_grid = torch.matmul(grid, m[:,:2,:2].unsqueeze(1)) + m[:,2,:2].unsqueeze(1).unsqueeze(2) return F.grid_sample(x, tfm_grid) . . #collapse m = tensor([[1., 0., 0.], [0., 1., 0.]], device=x.device) . . #collapse %timeit -n 10 tfm_x = rotate_batch(x.cuda(), 128, 30) . . /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; . The slowest run took 6.32 times longer than the fastest. This could mean that an intermediate result is being cached. 7.75 ms ± 7.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; /home/cedric/.conda/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details. warnings.warn(&#34;Default grid_sample and affine_grid behavior will be changed &#34; . The speed of this depends a lot on what card you have. On a V100 it is generally about 3x faster than non-JIT (as at April 2019) although PyTorch JIT is rapidly improving. . affine multiplication with affine_grid . And even faster if we give the matrix rotation to affine_grid. . #collapse_show def rotate_batch(x, size, degrees): size = (size,size) if isinstance(size, int) else tuple(size) size = (x.size(0),x.size(1)) + size thetas = x.new(x.size(0)).uniform_(-degrees,degrees) m = rotation_matrix(thetas) grid = F.affine_grid(m[:,:2], size) return F.grid_sample(x.cuda(), grid) . . #collapse %timeit -n 10 tfm_x = rotate_batch(x.cuda(), 128, 30) . . 3.74 ms ± 104 µs per loop (mean ± std. dev. of 7 runs, 10 loops each) .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/20/augmentation.html",
            "relUrl": "/jupyter/2020/04/20/augmentation.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Fastai Course DL from the Foundations Learner",
            "content": "Fastai Learner . This Post is based on the Notebok by the Fastai Course Part2 | . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_09 import * . . #collapse AvgStats . . exp.nb_04.AvgStats . Imagenette data . Jump_to lesson 11 video . #collapse path = datasets.untar_data(datasets.URLs.IMAGENETTE_160) . . #collapse tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] bs=64 il = ImageList.from_files(path, tfms=tfms) sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=&#39;val&#39;)) ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4) . . #collapse cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback, partial(BatchTransformXCallback, norm_imagenette)] . . #collapse nfs = [32]*4 . . Having a Runner is great but not essential when the Learner already has everything needed in its state. We implement everything inside it directly instead of building a second object. . In Lesson 12 Jeremy Howard revisited material in the cell below Jump_to lesson 12 video . #collapse_show def param_getter(m): return m.parameters() class Learner(): def __init__(self, model, data, loss_func, opt_func=sgd_opt, lr=1e-2, splitter=param_getter, cbs=None, cb_funcs=None): self.model,self.data,self.loss_func,self.opt_func,self.lr,self.splitter = model,data,loss_func,opt_func,lr,splitter self.in_train,self.logger,self.opt = False,print,None # NB: Things marked &quot;NEW&quot; are covered in lesson 12 # NEW: avoid need for set_runner self.cbs = [] self.add_cb(TrainEvalCallback()) self.add_cbs(cbs) self.add_cbs(cbf() for cbf in listify(cb_funcs)) def add_cbs(self, cbs): for cb in listify(cbs): self.add_cb(cb) def add_cb(self, cb): cb.set_runner(self) setattr(self, cb.name, cb) self.cbs.append(cb) def remove_cbs(self, cbs): for cb in listify(cbs): self.cbs.remove(cb) def one_batch(self, i, xb, yb): try: self.iter = i self.xb,self.yb = xb,yb; self(&#39;begin_batch&#39;) self.pred = self.model(self.xb); self(&#39;after_pred&#39;) self.loss = self.loss_func(self.pred, self.yb); self(&#39;after_loss&#39;) if not self.in_train: return self.loss.backward(); self(&#39;after_backward&#39;) self.opt.step(); self(&#39;after_step&#39;) self.opt.zero_grad() except CancelBatchException: self(&#39;after_cancel_batch&#39;) finally: self(&#39;after_batch&#39;) def all_batches(self): self.iters = len(self.dl) try: for i,(xb,yb) in enumerate(self.dl): self.one_batch(i, xb, yb) except CancelEpochException: self(&#39;after_cancel_epoch&#39;) def do_begin_fit(self, epochs): self.epochs,self.loss = epochs,tensor(0.) self(&#39;begin_fit&#39;) def do_begin_epoch(self, epoch): self.epoch,self.dl = epoch,self.data.train_dl return self(&#39;begin_epoch&#39;) def fit(self, epochs, cbs=None, reset_opt=False): # NEW: pass callbacks to fit() and have them removed when done self.add_cbs(cbs) # NEW: create optimizer on fit(), optionally replacing existing if reset_opt or not self.opt: self.opt = self.opt_func(self.splitter(self.model), lr=self.lr) try: self.do_begin_fit(epochs) for epoch in range(epochs): if not self.do_begin_epoch(epoch): self.all_batches() with torch.no_grad(): self.dl = self.data.valid_dl if not self(&#39;begin_validate&#39;): self.all_batches() self(&#39;after_epoch&#39;) except CancelTrainException: self(&#39;after_cancel_train&#39;) finally: self(&#39;after_fit&#39;) self.remove_cbs(cbs) ALL_CBS = {&#39;begin_batch&#39;, &#39;after_pred&#39;, &#39;after_loss&#39;, &#39;after_backward&#39;, &#39;after_step&#39;, &#39;after_cancel_batch&#39;, &#39;after_batch&#39;, &#39;after_cancel_epoch&#39;, &#39;begin_fit&#39;, &#39;begin_epoch&#39;, &#39;begin_validate&#39;, &#39;after_epoch&#39;, &#39;after_cancel_train&#39;, &#39;after_fit&#39;} def __call__(self, cb_name): res = False assert cb_name in self.ALL_CBS for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res return res . . #collapse_show class AvgStatsCallback(Callback): def __init__(self, metrics): self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False) def begin_epoch(self): self.train_stats.reset() self.valid_stats.reset() def after_loss(self): stats = self.train_stats if self.in_train else self.valid_stats with torch.no_grad(): stats.accumulate(self.run) def after_epoch(self): #We use the logger function of the `Learner` here, it can be customized to write in a file or in a progress bar self.logger(self.train_stats) self.logger(self.valid_stats) . . #collapse_show cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback, partial(BatchTransformXCallback, norm_imagenette)] . . #collapse_show def get_learner(nfs, data, lr, layer, loss_func=F.cross_entropy, cb_funcs=None, opt_func=sgd_opt, **kwargs): model = get_cnn_model(data, nfs, layer, **kwargs) init_cnn(model) return Learner(model, data, loss_func, lr=lr, cb_funcs=cb_funcs, opt_func=opt_func) . . #collapse_show learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs) . . #collapse_show %time learn.fit(1) . . train: [1.8311652646095082, tensor(0.3570, device=&#39;cuda:0&#39;)] valid: [1.5547332763671875, tensor(0.4880, device=&#39;cuda:0&#39;)] CPU times: user 4.05 s, sys: 1.93 s, total: 5.98 s Wall time: 18 s . Check everything works . Let&#39;s check our previous callbacks still work. . #collapse_show cbfs += [Recorder] . . #collapse_show learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs) . . #collapse_show phases = combine_scheds([0.3, 0.7], cos_1cycle_anneal(0.2, 0.6, 0.2)) sched = ParamScheduler(&#39;lr&#39;, phases) . . #collapse_show learn.fit(1, sched) . . train: [1.8860618007891268, tensor(0.3378, device=&#39;cuda:0&#39;)] valid: [1.775871337890625, tensor(0.3620, device=&#39;cuda:0&#39;)] . #collapse_show learn.recorder.plot_lr() . . #collapse_show learn.recorder.plot_loss() . .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/19/Learner.html",
            "relUrl": "/jupyter/2020/04/19/Learner.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Fastai Course DL from the Foundations Progress Bar",
            "content": "Fastai Adding progress bars to Learner . This Post is based on the Notebok by the Fastai Course Part2 | . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_09b import * import time from fastai.gen_doc.nbdoc import doc from fastai import * from fastprogress.fastprogress import format_time,master_bar, progress_bar . . One thing has been missing all this time, and as fun as it is to stare at a blank screen waiting for the results, it&#39;s nicer to have some tool to track progress. . Imagenette data . Jump_to lesson 11 video . #collapse doc(datasets.untar_data) . . #collapse datasets.URLs.IMAGENETTE_160 . . &#39;https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160&#39; . path = Path(&quot;/media/cedric/Datasets/imagenette2-160/&quot;) . #collapse path.ls() . . [PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/train&#39;), PosixPath(&#39;/media/cedric/Datasets/imagenette2-160/val&#39;)] . #collapse tfms = [make_rgb,ResizeFixed(128),to_byte_tensor,to_float_tensor] bs = 64 il = ImageList.from_files(path, tfms=tfms) sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=&#39;val&#39;)) ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4) . . #collapse nfs = [32]*4 . . We rewrite the AvgStatsCallback to add a line with the names of the things measured and keep track of the time per epoch. . #collapse_show class AvgStatsCallback(Callback): def __init__(self, metrics): self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False) def begin_fit(self): met_names = [&#39;loss&#39;] + [m.__name__ for m in self.train_stats.metrics] names = [&#39;epoch&#39;] + [f&#39;train_{n}&#39; for n in met_names] + [ f&#39;valid_{n}&#39; for n in met_names] + [&#39;time&#39;] self.logger(names) def begin_epoch(self): self.train_stats.reset() self.valid_stats.reset() self.start_time = time.time() def after_loss(self): stats = self.train_stats if self.in_train else self.valid_stats with torch.no_grad(): stats.accumulate(self.run) def after_epoch(self): stats = [str(self.epoch)] for o in [self.train_stats, self.valid_stats]: stats += [f&#39;{v:.6f}&#39; for v in o.avg_stats] stats += [format_time(time.time() - self.start_time)] self.logger(stats) . . Then we add the progress bars... with a Callback of course! master_bar handles the count over the epochs while its child progress_bar is looping over all the batches. We just create one at the beginning or each epoch/validation phase, and update it at the end of each batch. By changing the logger of the Learner to the write function of the master bar, everything is automatically written there. . Note: this requires fastprogress v0.1.21 or later. . #collapse_show class ProgressCallback(Callback): _order=-1 def begin_fit(self): self.mbar = master_bar(range(self.epochs)) self.mbar.on_iter_begin() self.run.logger = partial(self.mbar.write, table=True) def after_fit(self): self.mbar.on_iter_end() def after_batch(self): self.pb.update(self.iter) def begin_epoch (self): self.set_pb() def begin_validate(self): self.set_pb() def set_pb(self): self.pb = progress_bar(self.dl, parent=self.mbar) self.mbar.update(self.epoch) . . By making the progress bar a callback, you can easily choose if you want to have them shown or not. . #collapse_show cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback, ProgressCallback, partial(BatchTransformXCallback, norm_imagenette)] . . #collapse_show learn = get_learner(nfs, data, 0.4, conv_layer, cb_funcs=cbfs) . . #collapse_show learn.fit(2) . . &lt;progress value=&#39;0&#39; class=&#39;&#39; max=&#39;2&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; epoch train_loss train_accuracy valid_loss valid_accuracy time . 0 | 1.249549 | 0.582110 | 1.224612 | 0.592102 | 00:05 | . 1 | 1.079944 | 0.643679 | 1.478166 | 0.528917 | 00:04 | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/18/progress_bar.html",
            "relUrl": "/jupyter/2020/04/18/progress_bar.html",
            "date": " • Apr 18, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Fastai Course DL from the Foundations Optimizer Functions",
            "content": "Fastai Optimizer tweaks . This Post is based on the Notebok by the Fastai Course Part2 | . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_08 import * . . #collapse datasets.URLs.IMAGENETTE_160 . . &#39;https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160&#39; . Imagenette data . We grab the data from the previous notebook. . Jump_to lesson 11 video . #collapse path = Path(&quot;/home/cedric/.fastai/data/imagenette2-160&quot;) . . #collapse_show tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] bs=128 il = ImageList.from_files(path, tfms=tfms) sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=&#39;val&#39;)) ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4) . . Then a model: . #collapse nfs = [32,64,128,256] . . #collapse cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback, partial(BatchTransformXCallback, norm_imagenette)] . . This is the baseline of training with vanilla SGD. . #collapse learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs) . . #collapse run.fit(1, learn) . . train: [1.799063679836836, tensor(0.3704, device=&#39;cuda:0&#39;)] valid: [1.7272854050557325, tensor(0.4257, device=&#39;cuda:0&#39;)] . Refining the optimizer . In PyTorch, the base optimizer in torch.optim is just a dictionary that stores the hyper-parameters and references to the parameters of the model we want to train in parameter groups (different groups can have different learning rates/momentum/weight decay... which is what lets us do discriminative learning rates). . It contains a method step that will update our parameters with the gradients and a method zero_grad to detach and zero the gradients of all our parameters. . We build the equivalent from scratch, only ours will be more flexible. In our implementation, the step function loops over all the parameters to execute the step using stepper functions that we have to provide when initializing the optimizer. . Jump_to lesson 11 video . #collapse_show class Optimizer(): def __init__(self, params, steppers, **defaults): # might be a generator self.param_groups = list(params) # ensure params is a list of lists if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups] self.hypers = [{**defaults} for p in self.param_groups] self.steppers = listify(steppers) def grad_params(self): return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers) for p in pg if p.grad is not None] def zero_grad(self): for p,hyper in self.grad_params(): p.grad.detach_() p.grad.zero_() def step(self): for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper) . . To do basic SGD, this what a step looks like: . #collapse_show def sgd_step(p, lr, **kwargs): p.data.add_(-lr, p.grad.data) return p . . #collapse_show opt_func = partial(Optimizer, steppers=[sgd_step]) . . Now that we have changed the optimizer, we will need to adjust the callbacks that were using properties from the PyTorch optimizer: in particular the hyper-parameters are in the list of dictionaries opt.hypers (PyTorch has everything in the the list of param groups). . #collapse_show class Recorder(Callback): def begin_fit(self): self.lrs,self.losses = [],[] def after_batch(self): if not self.in_train: return self.lrs.append(self.opt.hypers[-1][&#39;lr&#39;]) self.losses.append(self.loss.detach().cpu()) def plot_lr (self): plt.plot(self.lrs) def plot_loss(self): plt.plot(self.losses) def plot(self, skip_last=0): losses = [o.item() for o in self.losses] n = len(losses)-skip_last plt.xscale(&#39;log&#39;) plt.plot(self.lrs[:n], losses[:n]) class ParamScheduler(Callback): _order=1 def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,listify(sched_funcs) def begin_batch(self): if not self.in_train: return fs = self.sched_funcs if len(fs)==1: fs = fs*len(self.opt.param_groups) pos = self.n_epochs/self.epochs for f,h in zip(fs,self.opt.hypers): h[self.pname] = f(pos) class LR_Find(Callback): _order=1 def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10): self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr self.best_loss = 1e9 def begin_batch(self): if not self.in_train: return pos = self.n_iter/self.max_iter lr = self.min_lr * (self.max_lr/self.min_lr) ** pos for pg in self.opt.hypers: pg[&#39;lr&#39;] = lr def after_step(self): if self.n_iter&gt;=self.max_iter or self.loss&gt;self.best_loss*10: raise CancelTrainException() if self.loss &lt; self.best_loss: self.best_loss = self.loss . . So let&#39;s check we didn&#39;t break anything and that recorder and param scheduler work properly. . #collapse_show sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) . . #collapse_show cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback, Recorder, partial(ParamScheduler, &#39;lr&#39;, sched)] . . #collapse_show learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs, opt_func=opt_func) . . #collapse_show %time run.fit(1, learn) . . train: [1.7935766134887527, tensor(0.3815, device=&#39;cuda:0&#39;)] valid: [1.428013908240446, tensor(0.5289, device=&#39;cuda:0&#39;)] CPU times: user 22.2 s, sys: 2.2 s, total: 24.4 s Wall time: 32 s . #collapse_show run.recorder.plot_loss() . . #collapse_show run.recorder.plot_lr() . . Weight decay . Jump_to lesson 11 video . By letting our model learn high parameters, it might fit all the data points in the training set with an over-complex function that has very sharp changes, which will lead to overfitting. . . Weight decay comes from the idea of L2 regularization, which consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible. . Limiting our weights from growing too much is going to hinder the training of the model, but it will yield to a state where it generalizes better. Going back to the theory a little bit, weight decay (or just wd) is a parameter that controls that sum of squares we add to our loss: . loss_with_wd = loss + (wd/2) * (weights**2).sum() . In practice though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high school math, the derivative of p**2 with respect to p is 2*p. So adding that big sum to our loss is exactly the same as doing: . weight.grad += wd * weight . for every weight in our model, which in the case of vanilla SGD is equivalent to updating the parameters with: . weight = weight - lr*(weight.grad + wd*weight) . This technique is called &quot;weight decay&quot;, as each weight is decayed by a factor lr * wd, as it&#39;s shown in this last formula. . This only works for standard SGD, as we have seen that with momentum, RMSProp and Adam, the update has some additional formulas around the gradient. In those cases, the formula that comes from L2 regularization: . weight.grad += wd * weight . is different than weight decay . new_weight = weight - lr * weight.grad - lr * wd * weight . Most libraries use the first one, but as it was pointed out in Decoupled Weight Regularization by Ilya Loshchilov and Frank Hutter, it is better to use the second one with the Adam optimizer, which is why fastai made it its default. . Weight decay is subtracting lr*wd*weight from the weights. We need this function to have an attribute _defaults so that we are sure there is an hyper-parameter of the same name in our Optimizer. . #collapse_show def weight_decay(p, lr, wd, **kwargs): p.data.mul_(1 - lr*wd) return p weight_decay._defaults = dict(wd=0.) . . L2 regularization is adding wd*weight to the gradients. . #collapse_show def l2_reg(p, lr, wd, **kwargs): p.grad.data.add_(wd, p.data) return p l2_reg._defaults = dict(wd=0.) . . Let&#39;s allow steppers to add to our defaults (which are the default values of all the hyper-parameters). This helper function adds in dest the key/values it finds while going through os and applying f when they was no key of the same name. . #collapse_show def maybe_update(os, dest, f): for o in os: for k,v in f(o).items(): if k not in dest: dest[k] = v def get_defaults(d): return getattr(d,&#39;_defaults&#39;,{}) . . This is the same as before, we just take the default values of the steppers when none are provided in the kwargs. . #collapse_show class Optimizer(): def __init__(self, params, steppers, **defaults): self.steppers = listify(steppers) maybe_update(self.steppers, defaults, get_defaults) # might be a generator self.param_groups = list(params) # ensure params is a list of lists if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups] self.hypers = [{**defaults} for p in self.param_groups] def grad_params(self): return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers) for p in pg if p.grad is not None] def zero_grad(self): for p,hyper in self.grad_params(): p.grad.detach_() p.grad.zero_() def step(self): for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper) . . #collapse_show sgd_opt = partial(Optimizer, steppers=[weight_decay, sgd_step]) . . #collapse_show learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs, opt_func=sgd_opt) . . Before trying to train, let&#39;s check the behavior works as intended: when we don&#39;t provide a value for wd, we pull the corresponding default from weight_decay. . #collapse model = learn.model . . #collapse_show opt = sgd_opt(model.parameters(), lr=0.1) test_eq(opt.hypers[0][&#39;wd&#39;], 0.) test_eq(opt.hypers[0][&#39;lr&#39;], 0.1) . . But if we provide a value, it overrides the default. . #collapse_show opt = sgd_opt(model.parameters(), lr=0.1, wd=1e-4) test_eq(opt.hypers[0][&#39;wd&#39;], 1e-4) test_eq(opt.hypers[0][&#39;lr&#39;], 0.1) . . Now let&#39;s fit. . #collapse cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback] . . #collapse learn,run = get_learn_run(nfs, data, 0.3, conv_layer, cbs=cbfs, opt_func=partial(sgd_opt, wd=0.01)) . . #collapse_show run.fit(1, learn) . . train: [1.784850152471222, tensor(0.3859, device=&#39;cuda:0&#39;)] valid: [1.7827735619028662, tensor(0.3890, device=&#39;cuda:0&#39;)] . This is already better than the baseline! . With momentum . Jump_to lesson 11 video . Momentum requires to add some state. We need to save the moving average of the gradients to be able to do the step and store this inside the optimizer state. To do this, we introduce statistics. Statistics are object with two methods: . init_state, that returns the initial state (a tensor of 0. for the moving average of gradients) | update, that updates the state with the new gradient value | . We also read the _defaults values of those objects, to allow them to provide default values to hyper-parameters. . #collapse_show class StatefulOptimizer(Optimizer): def __init__(self, params, steppers, stats=None, **defaults): self.stats = listify(stats) maybe_update(self.stats, defaults, get_defaults) super().__init__(params, steppers, **defaults) self.state = {} def step(self): for p,hyper in self.grad_params(): if p not in self.state: #Create a state for p and call all the statistics to initialize it. self.state[p] = {} maybe_update(self.stats, self.state[p], lambda o: o.init_state(p)) state = self.state[p] for stat in self.stats: state = stat.update(p, state, **hyper) compose(p, self.steppers, **state, **hyper) self.state[p] = state . . #collapse_show class Stat(): _defaults = {} def init_state(self, p): raise NotImplementedError def update(self, p, state, **kwargs): raise NotImplementedError . . Here is an example of Stat: . #collapse_show class AverageGrad(Stat): _defaults = dict(mom=0.9) def init_state(self, p): return {&#39;grad_avg&#39;: torch.zeros_like(p.grad.data)} def update(self, p, state, mom, **kwargs): state[&#39;grad_avg&#39;].mul_(mom).add_(p.grad.data) return state . . Then we add the momentum step (instead of using the gradients to perform the step, we use the average). . #collapse_show def momentum_step(p, lr, grad_avg, **kwargs): p.data.add_(-lr, grad_avg) return p . . #collapse_show sgd_mom_opt = partial(StatefulOptimizer, steppers=[momentum_step,weight_decay], stats=AverageGrad(), wd=0.01) . . #collapse_show learn,run = get_learn_run(nfs, data, 0.3, conv_layer, cbs=cbfs, opt_func=sgd_mom_opt) . . #collapse_show run.fit(1, learn) . . train: [1.9840235241313762, tensor(0.3516, device=&#39;cuda:0&#39;)] valid: [1.7593307125796178, tensor(0.4066, device=&#39;cuda:0&#39;)] . Jump_to lesson 11 video for discussion about weight decay interaction with batch normalisation . Momentum experiments . What does momentum do to the gradients exactly? Let&#39;s do some plots to find out! . Jump_to lesson 11 video . #collapse_show x = torch.linspace(-4, 4, 200) y = torch.randn(200) + 0.3 betas = [0.5, 0.7, 0.9, 0.99] . . #collapse_show def plot_mom(f): _,axs = plt.subplots(2,2, figsize=(12,8)) for beta,ax in zip(betas, axs.flatten()): ax.plot(y, linestyle=&#39;None&#39;, marker=&#39;.&#39;) avg,res = None,[] for i,yi in enumerate(y): avg,p = f(avg, beta, yi, i) res.append(p) ax.plot(res, color=&#39;red&#39;) ax.set_title(f&#39;beta={beta}&#39;) . . This is the regular momentum. . #collapse_show def mom1(avg, beta, yi, i): if avg is None: avg=yi res = beta*avg + yi return res,res plot_mom(mom1) . . As we can see, with a too high value, it may go way too high with no way to change its course. . Another way to smooth noisy data is to do an exponentially weighted moving average. In this case, there is a dampening of (1-beta) in front of the new value, which is less trusted than the current average. We&#39;ll define lin_comb (linear combination) to make this easier (note that in the lesson this was named ewma). . #collapse_show def lin_comb(v1, v2, beta): return beta*v1 + (1-beta)*v2 . . def mom2(avg, beta, yi, i): if avg is None: avg=yi avg = lin_comb(avg, yi, beta) return avg, avg plot_mom(mom2) . We can see it gets to a zero-constant when the data is purely random. If the data has a certain shape, it will get that shape (with some delay for high beta). . #collapse_show y = 1 - (x/3) ** 2 + torch.randn(200) * 0.1 . . #collapse_show y[0]=0.5 . . #collapse_show plot_mom(mom2) . . Debiasing is here to correct the wrong information we may have in the very first batch. The debias term corresponds to the sum of the coefficient in our moving average. At the time step i, our average is: . $ begin{align*} avg_{i} &amp;= beta avg_{i-1} + (1- beta) v_{i} = beta ( beta avg_{i-2} + (1- beta) v_{i-1}) + (1- beta) v_{i} &amp;= beta^{2} avg_{i-2} + (1- beta) beta v_{i-1} + (1- beta) v_{i} &amp;= beta^{3} avg_{i-3} + (1- beta) beta^{2} v_{i-2} + (1- beta) beta v_{i-1} + (1- beta) v_{i} &amp; vdots &amp;= (1- beta) beta^{i} v_{0} + (1- beta) beta^{i-1} v_{1} + cdots + (1- beta) beta^{2} v_{i-2} + (1- beta) beta v_{i-1} + (1- beta) v_{i} end{align*}$ . and so the sum of the coefficients is . $ begin{align*} S &amp;=(1- beta) beta^{i} + (1- beta) beta^{i-1} + cdots + (1- beta) beta^{2} + (1- beta) beta + (1- beta) &amp;= ( beta^{i} - beta^{i+1}) + ( beta^{i-1} - beta^{i}) + cdots + ( beta^{2} - beta^{3}) + ( beta - beta^{2}) + (1- beta) &amp;= 1 - beta^{i+1} end{align*}$ . since all the other terms cancel out each other. . By dividing by this term, we make our moving average a true average (in the sense that all the coefficients we used for the average sum up to 1). . #collapse_show def mom3(avg, beta, yi, i): if avg is None: avg=0 avg = lin_comb(avg, yi, beta) return avg, avg/(1-beta**(i+1)) plot_mom(mom3) . . Adam and friends . In Adam, we use the gradient averages but with dampening (not like in SGD with momentum), so let&#39;s add this to the AverageGrad class. . Jump_to lesson 11 video . #collapse_show class AverageGrad(Stat): _defaults = dict(mom=0.9) def __init__(self, dampening:bool=False): self.dampening=dampening def init_state(self, p): return {&#39;grad_avg&#39;: torch.zeros_like(p.grad.data)} def update(self, p, state, mom, **kwargs): state[&#39;mom_damp&#39;] = 1-mom if self.dampening else 1. state[&#39;grad_avg&#39;].mul_(mom).add_(state[&#39;mom_damp&#39;], p.grad.data) return state . . We also need to track the moving average of the gradients squared. . #collapse_show class AverageSqrGrad(Stat): _defaults = dict(sqr_mom=0.99) def __init__(self, dampening:bool=True): self.dampening=dampening def init_state(self, p): return {&#39;sqr_avg&#39;: torch.zeros_like(p.grad.data)} def update(self, p, state, sqr_mom, **kwargs): state[&#39;sqr_damp&#39;] = 1-sqr_mom if self.dampening else 1. state[&#39;sqr_avg&#39;].mul_(sqr_mom).addcmul_(state[&#39;sqr_damp&#39;], p.grad.data, p.grad.data) return state . . We will also need the number of steps done during training for the debiasing. . #collapse_show class StepCount(Stat): def init_state(self, p): return {&#39;step&#39;: 0} def update(self, p, state, **kwargs): state[&#39;step&#39;] += 1 return state . . This helper function computes the debias term. If we dampening, damp = 1 - mom and we get the same result as before. If we don&#39;t use dampening, (damp = 1) we will need to divide by 1 - mom because that term is missing everywhere. . #collapse_show def debias(mom, damp, step): return damp * (1 - mom**step) / (1-mom) . . Then the Adam step is just the following: . #collapse_show def adam_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, **kwargs): debias1 = debias(mom, mom_damp, step) debias2 = debias(sqr_mom, sqr_damp, step) p.data.addcdiv_(-lr / debias1, grad_avg, (sqr_avg/debias2).sqrt() + eps) return p adam_step._defaults = dict(eps=1e-5) . . #collapse_show def adam_opt(xtra_step=None, **kwargs): return partial(StatefulOptimizer, steppers=[adam_step,weight_decay]+listify(xtra_step), stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()], **kwargs) . . #collapse_show learn,run = get_learn_run(nfs, data, 0.001, conv_layer, cbs=cbfs, opt_func=adam_opt()) . . #collapse_show run.fit(3, learn) . . train: [1.7405600677209843, tensor(0.4025, device=&#39;cuda:0&#39;)] valid: [1.5315888734076433, tensor(0.4904, device=&#39;cuda:0&#39;)] train: [1.2346337596697117, tensor(0.5938, device=&#39;cuda:0&#39;)] valid: [1.3684276721735669, tensor(0.5437, device=&#39;cuda:0&#39;)] train: [0.9368180873112261, tensor(0.7014, device=&#39;cuda:0&#39;)] valid: [1.3147652517914012, tensor(0.5766, device=&#39;cuda:0&#39;)] . LAMB . Jump_to lesson 11 video . It&#39;s then super easy to implement a new optimizer. This is LAMB from a very recent paper: . $ begin{align} g_{t}^{l} &amp;= nabla L(w_{t-1}^{l}, x_{t}) m_{t}^{l} &amp;= beta_{1} m_{t-1}^{l} + (1- beta_{1}) g_{t}^{l} v_{t}^{l} &amp;= beta_{2} v_{t-1}^{l} + (1- beta_{2}) g_{t}^{l} odot g_{t}^{l} m_{t}^{l} &amp;= m_{t}^{l} / (1 - beta_{1}^{t}) v_{t}^{l} &amp;= v_{t}^{l} / (1 - beta_{2}^{t}) r_{1} &amp;= |w_{t-1}^{l} |_{2} s_{t}^{l} &amp;= frac{m_{t}^{l}}{ sqrt{v_{t}^{l} + epsilon}} + lambda w_{t-1}^{l} r_{2} &amp;= | s_{t}^{l} |_{2} eta^{l} &amp;= eta * r_{1}/r_{2} w_{t}^{l} &amp;= w_{t}^{l-1} - eta_{l} * s_{t}^{l} end{align}$ . #collapse_show def lamb_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, wd, **kwargs): debias1 = debias(mom, mom_damp, step) debias2 = debias(sqr_mom, sqr_damp, step) r1 = p.data.pow(2).mean().sqrt() step = (grad_avg/debias1) / ((sqr_avg/debias2).sqrt()+eps) + wd*p.data r2 = step.pow(2).mean().sqrt() p.data.add_(-lr * min(r1/r2,10), step) return p lamb_step._defaults = dict(eps=1e-6, wd=0.) . . #collapse_show lamb = partial(StatefulOptimizer, steppers=lamb_step, stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()]) . . #collapse_show learn,run = get_learn_run(nfs, data, 0.003, conv_layer, cbs=cbfs, opt_func=lamb) . . #collapse_show run.fit(3, learn) . . train: [1.862844999141937, tensor(0.3480, device=&#39;cuda:0&#39;)] valid: [1.7505729996019108, tensor(0.3931, device=&#39;cuda:0&#39;)] train: [1.3384966321021228, tensor(0.5593, device=&#39;cuda:0&#39;)] valid: [1.4524099323248407, tensor(0.5159, device=&#39;cuda:0&#39;)] train: [1.036301331152973, tensor(0.6672, device=&#39;cuda:0&#39;)] valid: [1.3779652667197453, tensor(0.5536, device=&#39;cuda:0&#39;)] . Other recent variants of optimizers: . Large Batch Training of Convolutional Networks (LARS also uses weight statistics, not just gradient statistics. Can you add that to this class?) | Adafactor: Adaptive Learning Rates with Sublinear Memory Cost (Adafactor combines stats over multiple sets of axes) | Adaptive Gradient Methods with Dynamic Bound of Learning Rate | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/17/optimizers.html",
            "relUrl": "/jupyter/2020/04/17/optimizers.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Fastai Course DL from the Foundations Datablock API",
            "content": "Fastai Data block API foundations . This Post is based on the Notebok by the Fastai Course Part2 | . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_07a import * . . Jump_to lesson 11 video . #collapse datasets.URLs.IMAGENETTE_160 . . &#39;https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160&#39; . Image ItemList . Previously we were reading in to RAM the whole MNIST dataset at once, loading it as a pickle file. We can&#39;t do that for datasets larger than our RAM capacity, so instead we leave the images on disk and just grab the ones we need for each mini-batch as we use them. . Let&#39;s use the imagenette dataset and build the data blocks we need along the way. Imagenette is a subset of 10 easy ImageNet classes, as ImageNet takes to long to train and smaller Datasets like CIFAR 10 have very small (32x32) images, therefore a CNN trained on CIFAR will not perform that well on ImageNet and vice versa. 128x128 images however scale to full ImageNet resolution very well. Jeremy also created ImageWoof, a tougher version of ImageNet. . Get images . #collapse path = datasets.untar_data(datasets.URLs.IMAGENETTE_160) path . . PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160&#39;) . To be able to look at what&#39;s inside a directory from a notebook, we add the .ls method to Path with a monkey-patch. . #collapse import PIL,os,mimetypes Path.ls = lambda x: list(x.iterdir()) . . #collapse path.ls() . . [PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train&#39;)] . #collapse (path/&#39;val&#39;).ls() . . [PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n03028079&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02102040&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n03417042&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n03425413&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n03000684&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n03888257&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n03394916&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n01440764&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n03445777&#39;)] . Let&#39;s have a look inside a class folder (the first class is tench): . #collapse path_tench = path/&#39;val&#39;/&#39;n01440764&#39; . . #collapse img_fn = path_tench.ls()[0] img_fn . . PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n01440764/n01440764_4942.JPEG&#39;) . #collapse img = PIL.Image.open(img_fn) img . . #collapse plt.imshow(img) . . &lt;matplotlib.image.AxesImage at 0x7fdaf8c5a358&gt; . #collapse import numpy imga = numpy.array(img) . . #collapse imga.shape . . (160, 213, 3) . #collapse imga[:10,:10,0] . . array([[ 7, 23, 1, 9, ..., 22, 10, 1, 7], [ 14, 10, 0, 20, ..., 10, 22, 20, 22], [ 9, 8, 18, 21, ..., 17, 61, 54, 29], [ 2, 14, 35, 14, ..., 19, 51, 31, 59], ..., [ 15, 28, 23, 22, ..., 41, 99, 105, 79], [ 16, 32, 7, 9, ..., 47, 51, 78, 44], [ 17, 26, 19, 13, ..., 45, 100, 25, 26], [ 30, 18, 12, 19, ..., 64, 132, 79, 0]], dtype=uint8) . Just in case there are other files in the directory (models, texts...) we want to keep only the images. Let&#39;s not write it out by hand, but instead use what&#39;s already on our computer (the MIME types database). . #collapse_show image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith(&#39;image/&#39;)) . . #collapse_show &#39; &#39;.join(image_extensions) . . &#39;.rgb .jpe .bmp .ppm .pnm .pgm .svg .gif .tif .tiff .ief .jpg .pbm .xbm .ras .xpm .xwd .png .ico .jpeg&#39; . #collapse_show def setify(o): return o if isinstance(o,set) else set(listify(o)) . . #collapse_show test_eq(setify(&#39;aa&#39;), {&#39;aa&#39;}) test_eq(setify([&#39;aa&#39;,1]), {&#39;aa&#39;,1}) test_eq(setify(None), set()) test_eq(setify(1), {1}) test_eq(setify({1}), {1}) . . Now let&#39;s walk through the directories and grab all the images. The first private function grabs all the images inside a given directory and the second one walks (potentially recursively) through all the folder in path. . Jump_to lesson 11 video . #collapse_show def _get_files(p, fs, extensions=None): p = Path(p) res = [p/f for f in fs if not f.startswith(&#39;.&#39;) and ((not extensions) or f&#39;.{f.split(&quot;.&quot;)[-1].lower()}&#39; in extensions)] return res . . #collapse_show t = [o.name for o in os.scandir(path_tench)] t = _get_files(path, t, extensions=image_extensions) t[:3] . . [PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/n01440764_4942.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/ILSVRC2012_val_00017472.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/n01440764_20451.JPEG&#39;)] . Putting these together, in a function, that can optionally recurse : Scandir is a Python wrapper over a C API, so it runs super fast. os.walk can be used to recurse, it uses scandir as well. . #collapse_show def get_files(path, extensions=None, recurse=False, include=None): path = Path(path) extensions = setify(extensions) extensions = {e.lower() for e in extensions} if recurse: res = [] for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames) if include is not None and i==0: d[:] = [o for o in d if o in include] else: d[:] = [o for o in d if not o.startswith(&#39;.&#39;)] res += _get_files(p, f, extensions) return res else: f = [o.name for o in os.scandir(path) if o.is_file()] return _get_files(path, f, extensions) . . #collapse get_files(path_tench, image_extensions)[:3] . . [PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n01440764/n01440764_4942.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n01440764/ILSVRC2012_val_00017472.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n01440764/n01440764_20451.JPEG&#39;)] . We need the recurse argument when we start from path since the pictures are two level below in directories. . #collapse get_files(path, image_extensions, recurse=True)[:3] . . [PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_12590.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_23911.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_7371.JPEG&#39;)] . #collapse all_fns = get_files(path, image_extensions, recurse=True) len(all_fns) . . 13394 . Imagenet is 100 times bigger than imagenette, so we need this to be fast. . #collapse %timeit -n 10 get_files(path, image_extensions, recurse=True) . . 58.2 ms ± 2.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) . As we can see, grabbing the images is very fast now ! . Prepare for modeling . What we need to do: . Get files | Split validation set random%, folder name, csv, ... | . | Label: folder name, file name/re, csv, ... | . | Transform per image (optional) | Transform to tensor | DataLoader | Transform per batch (optional) | DataBunch | Add test set (optional) | . Jump_to lesson 11 video . Get files . We use the ListContainer class from notebook 06 to store our objects in an ItemList. The get method will need to be subclassed to explain how to access an element (open an image for instance), then the private _get method can allow us to apply any additional transform to it. . new will be used in conjunction with __getitem__ (that works for one index or a list of indices) to create training and validation set from a single stream when we split the data. . #collapse_show def compose(x, funcs, *args, order_key=&#39;_order&#39;, **kwargs): key = lambda o: getattr(o, order_key, 0) for f in sorted(listify(funcs), key=key): x = f(x, **kwargs) return x class ItemList(ListContainer): def __init__(self, items, path=&#39;.&#39;, tfms=None): super().__init__(items) self.path,self.tfms = Path(path),tfms def __repr__(self): return f&#39;{super().__repr__()} nPath: {self.path}&#39; def new(self, items, cls=None): if cls is None: cls=self.__class__ return cls(items, self.path, tfms=self.tfms) def get(self, i): return i def _get(self, i): return compose(self.get(i), self.tfms) def __getitem__(self, idx): res = super().__getitem__(idx) if isinstance(res,list): return [self._get(o) for o in res] return self._get(res) class ImageList(ItemList): @classmethod def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs): if extensions is None: extensions = image_extensions return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs) def get(self, fn): return PIL.Image.open(fn) . . Transforms aren&#39;t only used for data augmentation. To allow total flexibility, ImageList returns the raw PIL image. The first thing is to convert it to &#39;RGB&#39; (or something else). . Transforms only need to be functions that take an element of the ItemList and transform it. If they need state, they can be defined as a class. Also, having them as a class allows to define an _order attribute (default 0) that is used to sort the transforms. . #collapse_show class Transform(): _order=0 class MakeRGB(Transform): def __call__(self, item): return item.convert(&#39;RGB&#39;) def make_rgb(item): return item.convert(&#39;RGB&#39;) . . #collapse_show il = ImageList.from_files(path, tfms=make_rgb) . . #collapse il . . ImageList (13394 items) [PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_12590.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_23911.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_7371.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_9740.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_14440.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_4032.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_2772.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_9112.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_11550.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_16192.JPEG&#39;)...] Path: /home/cedric/.fastai/data/imagenette2-160 . #collapse_show img = il[0]; img . . We can also index with a range or a list of integers: . #collapse il[:1] . . [&lt;PIL.Image.Image image mode=RGB size=213x160 at 0x7FDB0141F198&gt;] . Split validation set . Here, we need to split the files between those in the folder train and those in the folder val. . #collapse fn = il.items[0]; fn . . PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_12590.JPEG&#39;) . Since our filenames are path object, we can find the directory of the file with .parent. We need to go back two folders before since the last folders are the class names. . #collapse fn.parent.parent.name . . &#39;val&#39; . Jump_to lesson 11 video . #collapse_show def grandparent_splitter(fn, valid_name=&#39;valid&#39;, train_name=&#39;train&#39;): gp = fn.parent.parent.name return True if gp==valid_name else False if gp==train_name else None def split_by_func(items, f): mask = [f(o) for o in items] # `None` values will be filtered out f = [o for o,m in zip(items,mask) if m==False] t = [o for o,m in zip(items,mask) if m==True ] return f,t . . #collapse splitter = partial(grandparent_splitter, valid_name=&#39;val&#39;) . . #collapse %time train,valid = split_by_func(il, splitter) . . CPU times: user 27.6 ms, sys: 0 ns, total: 27.6 ms Wall time: 27.4 ms . #collapse len(train),len(valid) . . (9469, 3925) . Now that we can split our data, let&#39;s create the class that will contain it. It just needs two ItemList to be initialized, and we create a shortcut to all the unknown attributes by trying to grab them in the train ItemList. . #collapse_show class SplitData(): def __init__(self, train, valid): self.train,self.valid = train,valid def __getattr__(self,k): return getattr(self.train,k) #This is needed if we want to pickle SplitData and be able to load it back without recursion errors def __setstate__(self,data:Any): self.__dict__.update(data) @classmethod def split_by_func(cls, il, f): lists = map(il.new, split_by_func(il.items, f)) return cls(*lists) def __repr__(self): return f&#39;{self.__class__.__name__} nTrain: {self.train} nValid: {self.valid} n&#39; . . #collapse sd = SplitData.split_by_func(il, splitter); sd . . SplitData Train: ImageList (9469 items) [PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_11916.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_26816.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_11383.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_6495.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_16277.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_11074.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_20733.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_2949.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_16193.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_495.JPEG&#39;)...] Path: /home/cedric/.fastai/data/imagenette2-160 Valid: ImageList (3925 items) [PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_12590.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_23911.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_7371.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_9740.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_14440.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_4032.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_2772.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_9112.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_11550.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_16192.JPEG&#39;)...] Path: /home/cedric/.fastai/data/imagenette2-160 . Labeling . Labeling has to be done after splitting, because it uses training set information to apply to the validation set, using a Processor. . A Processor is a transformation that is applied to all the inputs once at initialization, with some state computed on the training set that is then applied without modification on the validation set (and maybe the test set or at inference time on a single item). For instance, it could be processing texts to tokenize, then numericalize them. In that case we want the validation set to be numericalized with exactly the same vocabulary as the training set. . Another example is in tabular data, where we fill missing values with (for instance) the median computed on the training set. That statistic is stored in the inner state of the Processor and applied on the validation set. . In our case, we want to convert label strings to numbers in a consistent and reproducible way. So we create a list of possible labels in the training set, and then convert our labels to numbers based on this vocab. . Jump_to lesson 11 video . #collapse_show from collections import OrderedDict def uniqueify(x, sort=False): res = list(OrderedDict.fromkeys(x).keys()) if sort: res.sort() return res . . First, let&#39;s define the processor. We also define a ProcessedItemList with an obj method that can get the unprocessed items: for instance a processed label will be an index between 0 and the number of classes - 1, the corresponding obj will be the name of the class. The first one is needed by the model for the training, but the second one is better for displaying the objects. . #collapse_show class Processor(): def process(self, items): return items class CategoryProcessor(Processor): def __init__(self): self.vocab=None def __call__(self, items): #The vocab is defined on the first use. if self.vocab is None: self.vocab = uniqueify(items) self.otoi = {v:k for k,v in enumerate(self.vocab)} return [self.proc1(o) for o in items] def proc1(self, item): return self.otoi[item] def deprocess(self, idxs): assert self.vocab is not None return [self.deproc1(idx) for idx in idxs] def deproc1(self, idx): return self.vocab[idx] . . Here we label according to the folders of the images, so simply fn.parent.name. We label the training set first with a newly created CategoryProcessor so that it computes its inner vocab on that set. Then we label the validation set using the same processor, which means it uses the same vocab. The end result is another SplitData object. . #collapse_show def parent_labeler(fn): return fn.parent.name def _label_by_func(ds, f, cls=ItemList): return cls([f(o) for o in ds.items], path=ds.path) #This is a slightly different from what was seen during the lesson, # we&#39;ll discuss the changes in lesson 11 class LabeledData(): def process(self, il, proc): return il.new(compose(il.items, proc)) def __init__(self, x, y, proc_x=None, proc_y=None): self.x,self.y = self.process(x, proc_x),self.process(y, proc_y) self.proc_x,self.proc_y = proc_x,proc_y def __repr__(self): return f&#39;{self.__class__.__name__} nx: {self.x} ny: {self.y} n&#39; def __getitem__(self,idx): return self.x[idx],self.y[idx] def __len__(self): return len(self.x) def x_obj(self, idx): return self.obj(self.x, idx, self.proc_x) def y_obj(self, idx): return self.obj(self.y, idx, self.proc_y) def obj(self, items, idx, procs): isint = isinstance(idx, int) or (isinstance(idx,torch.LongTensor) and not idx.ndim) item = items[idx] for proc in reversed(listify(procs)): item = proc.deproc1(item) if isint else proc.deprocess(item) return item @classmethod def label_by_func(cls, il, f, proc_x=None, proc_y=None): return cls(il, _label_by_func(il, f), proc_x=proc_x, proc_y=proc_y) def label_by_func(sd, f, proc_x=None, proc_y=None): train = LabeledData.label_by_func(sd.train, f, proc_x=proc_x, proc_y=proc_y) valid = LabeledData.label_by_func(sd.valid, f, proc_x=proc_x, proc_y=proc_y) return SplitData(train,valid) . . #collapse ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) . . #collapse assert ll.train.proc_y is ll.valid.proc_y . . #collapse ll.train.y . . ItemList (9469 items) [0, 0, 0, 0, 0, 0, 0, 0, 0, 0...] Path: /home/cedric/.fastai/data/imagenette2-160 . #collapse ll.train.y.items[0], ll.train.y_obj(0), ll.train.y_obj(slice(2)) . . (0, &#39;n02979186&#39;, [&#39;n02979186&#39;, &#39;n02979186&#39;]) . #collapse ll . . SplitData Train: LabeledData x: ImageList (9469 items) [PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_11916.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_26816.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_11383.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_6495.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_16277.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_11074.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_20733.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_2949.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_16193.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/train/n02979186/n02979186_495.JPEG&#39;)...] Path: /home/cedric/.fastai/data/imagenette2-160 y: ItemList (9469 items) [0, 0, 0, 0, 0, 0, 0, 0, 0, 0...] Path: /home/cedric/.fastai/data/imagenette2-160 Valid: LabeledData x: ImageList (3925 items) [PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_12590.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_23911.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_7371.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_9740.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_14440.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_4032.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_2772.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_9112.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_11550.JPEG&#39;), PosixPath(&#39;/home/cedric/.fastai/data/imagenette2-160/val/n02979186/n02979186_16192.JPEG&#39;)...] Path: /home/cedric/.fastai/data/imagenette2-160 y: ItemList (3925 items) [0, 0, 0, 0, 0, 0, 0, 0, 0, 0...] Path: /home/cedric/.fastai/data/imagenette2-160 . Transform to tensor . Jump_to lesson 11 video . #collapse ll.train[0] . . (&lt;PIL.Image.Image image mode=RGB size=284x160 at 0x7FDAF8754630&gt;, 0) . #collapse ll.train[0][0] . . To be able to put all our images in a batch, we need them to have all the same size. We can do this easily in PIL. . #collapse ll.train[0][0].resize((128,128)) . . The first transform resizes to a given size, then we convert the image to a by tensor before converting it to float and dividing by 255. We will investigate data augmentation transforms at length in notebook 10. . #collapse_show class ResizeFixed(Transform): _order=10 def __init__(self,size): if isinstance(size,int): size=(size,size) self.size = size def __call__(self, item): return item.resize(self.size, PIL.Image.BILINEAR) def to_byte_tensor(item): res = torch.ByteTensor(torch.ByteStorage.from_buffer(item.tobytes())) w,h = item.size return res.view(h,w,-1).permute(2,0,1) to_byte_tensor._order=20 def to_float_tensor(item): return item.float().div_(255.) to_float_tensor._order=30 . . #collapse_show tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] il = ImageList.from_files(path, tfms=tfms) sd = SplitData.split_by_func(il, splitter) ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) . . Here is a little convenience function to show an image from the corresponding tensor. . #collapse_show def show_image(im, figsize=(3,3)): plt.figure(figsize=figsize) plt.axis(&#39;off&#39;) plt.imshow(im.permute(1,2,0)) . . #collapse x,y = ll.train[0] x.shape . . torch.Size([3, 128, 128]) . #collapse show_image(x) . . Modeling . DataBunch . Now we are ready to put our datasets together in a DataBunch. . Jump_to lesson 11 video . #collapse bs=64 . . #collapse train_dl,valid_dl = get_dls(ll.train,ll.valid,bs, num_workers=4) . . #collapse x,y = next(iter(train_dl)) . . #collapse x.shape . . torch.Size([64, 3, 128, 128]) . We can still see the images in a batch and get the corresponding classes. . #collapse show_image(x[0]) ll.train.proc_y.vocab[y[0]] . . &#39;n03000684&#39; . #collapse y . . tensor([5, 4, 6, 2, 1, 6, 9, 7, 9, 3, 0, 6, 3, 7, 9, 4, 3, 4, 4, 8, 0, 9, 6, 2, 2, 4, 1, 8, 3, 0, 5, 1, 8, 6, 6, 6, 1, 3, 0, 7, 7, 4, 5, 2, 6, 0, 9, 1, 7, 6, 6, 8, 9, 8, 4, 9, 9, 9, 6, 8, 5, 2, 0, 9]) . We change a little bit our DataBunch to add a few attributes: c_in (for channel in) and c_out (for channel out) instead of just c. This will help when we need to build our model. . #collapse_show class DataBunch(): def __init__(self, train_dl, valid_dl, c_in=None, c_out=None): self.train_dl,self.valid_dl,self.c_in,self.c_out = train_dl,valid_dl,c_in,c_out @property def train_ds(self): return self.train_dl.dataset @property def valid_ds(self): return self.valid_dl.dataset . . Then we define a function that goes directly from the SplitData to a DataBunch. . #collapse_show def databunchify(sd, bs, c_in=None, c_out=None, **kwargs): dls = get_dls(sd.train, sd.valid, bs, **kwargs) return DataBunch(*dls, c_in=c_in, c_out=c_out) SplitData.to_databunch = databunchify . . This gives us the full summary on how to grab our data and put it in a DataBunch: . #collapse_show path = datasets.untar_data(datasets.URLs.IMAGENETTE_160) tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor] il = ImageList.from_files(path, tfms=tfms) sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name=&#39;val&#39;)) ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor()) data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=4) . . Model . Jump_to lesson 11 video . #collapse_show cbfs = [partial(AvgStatsCallback,accuracy), CudaCallback] . . We will normalize with the statistics from a batch. . #collapse_show m,s = x.mean((0,2,3)).cuda(),x.std((0,2,3)).cuda() m,s . . (tensor([0.4519, 0.4527, 0.4195], device=&#39;cuda:0&#39;), tensor([0.2799, 0.2731, 0.2981], device=&#39;cuda:0&#39;)) . #collapse_show def normalize_chan(x, mean, std): return (x-mean[...,None,None]) / std[...,None,None] _m = tensor([0.47, 0.48, 0.45]) _s = tensor([0.29, 0.28, 0.30]) norm_imagenette = partial(normalize_chan, mean=_m.cuda(), std=_s.cuda()) . . #collapse cbfs.append(partial(BatchTransformXCallback, norm_imagenette)) . . #collapse_show nfs = [64,64,128,256] . . We build our model using Bag of Tricks for Image Classification with Convolutional Neural Networks, in particular: we don&#39;t use a big conv 7x7 at first but three 3x3 convs, and don&#39;t go directly from 3 channels to 64 but progressively add those. . #collapse_show import math def prev_pow_2(x): return 2**math.floor(math.log2(x)) def get_cnn_layers(data, nfs, layer, **kwargs): def f(ni, nf, stride=2): return layer(ni, nf, 3, stride=stride, **kwargs) l1 = data.c_in l2 = prev_pow_2(l1*3*3) layers = [f(l1 , l2 , stride=1), f(l2 , l2*2, stride=2), f(l2*2, l2*4, stride=2)] nfs = [l2*4] + nfs layers += [f(nfs[i], nfs[i+1]) for i in range(len(nfs)-1)] layers += [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c_out)] return layers def get_cnn_model(data, nfs, layer, **kwargs): return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs)) def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, **kwargs): model = get_cnn_model(data, nfs, layer, **kwargs) init_cnn(model) return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func) . . #collapse_show sched = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.1,0.3,0.05)) . . #collapse_show learn,run = get_learn_run(nfs, data, 0.2, conv_layer, cbs=cbfs+[ partial(ParamScheduler, &#39;lr&#39;, sched) ]) . . Let&#39;s have a look at our model using Hooks. We print the layers and the shapes of their outputs. . #collapse_show def model_summary(run, learn, data, find_all=False): xb,yb = get_batch(data.valid_dl, run) device = next(learn.model.parameters()).device#Model may not be on the GPU yet xb,yb = xb.to(device),yb.to(device) mods = find_modules(learn.model, is_lin_layer) if find_all else learn.model.children() f = lambda hook,mod,inp,out: print(f&quot;{mod} n{out.shape} n&quot;) with Hooks(mods, f) as hooks: learn.model(xb) . . #collapse_show model_summary(run, learn, data) . . Sequential( (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): GeneralRelu() (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) torch.Size([128, 16, 128, 128]) Sequential( (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) torch.Size([128, 32, 64, 64]) Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) torch.Size([128, 64, 32, 32]) Sequential( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) torch.Size([128, 64, 16, 16]) Sequential( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) torch.Size([128, 64, 8, 8]) Sequential( (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) torch.Size([128, 128, 4, 4]) Sequential( (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) torch.Size([128, 256, 2, 2]) AdaptiveAvgPool2d(output_size=1) torch.Size([128, 256, 1, 1]) Lambda() torch.Size([128, 256]) Linear(in_features=256, out_features=10, bias=True) torch.Size([128, 10]) . And we can train the model: . #collapse %time run.fit(5, learn) . . train: [1.7670042952661316, tensor(0.3925, device=&#39;cuda:0&#39;)] valid: [1.6285718550955415, tensor(0.4591, device=&#39;cuda:0&#39;)] train: [1.3664484518560567, tensor(0.5442, device=&#39;cuda:0&#39;)] valid: [1.325954418789809, tensor(0.5806, device=&#39;cuda:0&#39;)] train: [0.9718254138504594, tensor(0.6806, device=&#39;cuda:0&#39;)] valid: [1.156678194665605, tensor(0.6283, device=&#39;cuda:0&#39;)] train: [0.6003031995029834, tensor(0.8128, device=&#39;cuda:0&#39;)] valid: [1.0788345939490447, tensor(0.6683, device=&#39;cuda:0&#39;)] train: [0.3126513212934972, tensor(0.9207, device=&#39;cuda:0&#39;)] valid: [1.0871226861066878, tensor(0.6736, device=&#39;cuda:0&#39;)] CPU times: user 1min 11s, sys: 9.73 s, total: 1min 20s Wall time: 1min 44s . The leaderboard as this notebook is written has ~85% accuracy for 5 epochs at 128px size, so we&#39;re definitely on the right track! .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/16/DataBlock.html",
            "relUrl": "/jupyter/2020/04/16/DataBlock.html",
            "date": " • Apr 16, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Fastai Course DL from the Foundations LSUV",
            "content": "Fastai Layerwise Sequential Unit Variance (LSUV) . This Post is based on the Notebok by the Fastai Course Part2 =&gt; With LSUV the computer will figure out how to init our net proberly, and will initalize our network with unit Variance | . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_07 import * . . Getting the MNIST data and a CNN . Jump_to lesson 11 video . #collapse x_train,y_train,x_valid,y_valid = get_data() x_train,x_valid = normalize_to(x_train,x_valid) train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) nh,bs = 50,512 c = y_train.max().item()+1 loss_func = F.cross_entropy data = DataBunch(*get_dls(train_ds, valid_ds, bs), c) . . #collapse_show mnist_view = view_tfm(1,28,28) cbfs = [Recorder, partial(AvgStatsCallback,accuracy), CudaCallback, partial(BatchTransformXCallback, mnist_view)] . . #collapse nfs = [8,16,32,64,64] . . #collapse_show class ConvLayer(nn.Module): def __init__(self, ni, nf, ks=3, stride=2, sub=0., **kwargs): super().__init__() self.conv = nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True) self.relu = GeneralRelu(sub=sub, **kwargs) def forward(self, x): return self.relu(self.conv(x)) @property def bias(self): return -self.relu.sub @bias.setter def bias(self,v): self.relu.sub = -v @property def weight(self): return self.conv.weight . . #collapse learn,run = get_learn_run(nfs, data, 0.6, ConvLayer, cbs=cbfs) . . Now we&#39;re going to look at the paper All You Need is a Good Init, which introduces Layer-wise Sequential Unit-Variance (LSUV). We initialize our neural net with the usual technique, then we pass a batch through the model and check the outputs of the linear and convolutional layers. We can then rescale the weights according to the actual variance we observe on the activations, and subtract the mean we observe from the initial bias. That way we will have activations that stay normalized. . We repeat this process until we are satisfied with the mean/variance we observe. . Let&#39;s start by looking at a baseline: . #collapse_show run.fit(2, learn) . . train: [2.31547421875, tensor(0.1591, device=&#39;cuda:0&#39;)] valid: [2.2347482421875, tensor(0.2266, device=&#39;cuda:0&#39;)] train: [1.050069921875, tensor(0.6582, device=&#39;cuda:0&#39;)] valid: [0.2111361328125, tensor(0.9348, device=&#39;cuda:0&#39;)] . Now we recreate our model and we&#39;ll try again with LSUV. Hopefully, we&#39;ll get better results! . #collapse_show learn,run = get_learn_run(nfs, data, 0.6, ConvLayer, cbs=cbfs) . . Helper function to get one batch of a given dataloader, with the callbacks called to preprocess it. . #collapse_show def get_batch(dl, run): run.xb,run.yb = next(iter(dl)) for cb in run.cbs: cb.set_runner(run) run(&#39;begin_batch&#39;) return run.xb,run.yb . . #collapse xb,yb = get_batch(data.train_dl, run) . . We only want the outputs of convolutional or linear layers. To find them, we need a recursive function. We can use sum(list, []) to concatenate the lists the function finds (sum applies the + operate between the elements of the list you pass it, beginning with the initial state in the second argument). . #collapse_show def find_modules(m, cond): if cond(m): return [m] a = sum([find_modules(o,cond) for o in m.children()],[]) print(a) return a def is_lin_layer(l): lin_layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear, nn.ReLU) return isinstance(l, lin_layers) . . #collapse_show mods = find_modules(learn.model, lambda o: isinstance(o,ConvLayer)) . . [] [] [] [ConvLayer( (conv): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)) (relu): GeneralRelu() ), ConvLayer( (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (relu): GeneralRelu() ), ConvLayer( (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (relu): GeneralRelu() ), ConvLayer( (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (relu): GeneralRelu() ), ConvLayer( (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (relu): GeneralRelu() )] . #collapse_show mods . . [ConvLayer( (conv): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)) (relu): GeneralRelu() ), ConvLayer( (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (relu): GeneralRelu() ), ConvLayer( (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (relu): GeneralRelu() ), ConvLayer( (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (relu): GeneralRelu() ), ConvLayer( (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (relu): GeneralRelu() )] . This is a helper function to grab the mean and std of the output of a hooked layer. . #collapse_show def append_stat(hook, mod, inp, outp): d = outp.data hook.mean,hook.std = d.mean().item(),d.std().item() . . #collapse_show mdl = learn.model.cuda() . . So now we can look at the mean and std of the conv layers of our model. . #collapse_show with Hooks(mods, append_stat) as hooks: mdl(xb) for hook in hooks: print(hook.mean,hook.std) . . 0.4263218641281128 1.0437159538269043 0.4034019708633423 0.8199178576469421 0.3987013101577759 0.6961370706558228 0.3476980924606323 0.5620085000991821 0.24456343054771423 0.3626357316970825 . We first adjust the bias terms to make the means 0, then we adjust the standard deviations to make the stds 1 (with a threshold of 1e-3). The mdl(xb) is not None clause is just there to pass xb through mdl and compute all the activations so that the hooks get updated. . #collapse_show def lsuv_module(m, xb): h = Hook(m, append_stat) while mdl(xb) is not None and abs(h.mean) &gt; 1e-3: m.bias -= h.mean while mdl(xb) is not None and abs(h.std-1) &gt; 1e-3: m.weight.data /= h.std h.remove() return h.mean,h.std . . We execute that initialization on all the conv layers in order: . #collapse_show for m in mods: print(lsuv_module(m, xb)) . . (-0.017856407910585403, 0.9999998807907104) (0.16777339577674866, 0.9999999403953552) (0.14251837134361267, 1.0) (0.152313232421875, 0.9999999403953552) (0.2786397337913513, 1.0000001192092896) . Note that the mean doesn&#39;t exactly stay at 0. since we change the standard deviation after by scaling the weight. . Then training is beginning on better grounds. . #collapse_show %time run.fit(2, learn) . . train: [0.5833533203125, tensor(0.8165, device=&#39;cuda:0&#39;)] valid: [0.14450025634765626, tensor(0.9564, device=&#39;cuda:0&#39;)] train: [0.111139541015625, tensor(0.9656, device=&#39;cuda:0&#39;)] valid: [0.09887520751953124, tensor(0.9700, device=&#39;cuda:0&#39;)] CPU times: user 2.33 s, sys: 24 ms, total: 2.35 s Wall time: 1.96 s . LSUV is particularly useful for more complex and deeper architectures that are hard to initialize to get unit variance at the last layer. .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/15/LSUV.html",
            "relUrl": "/jupyter/2020/04/15/LSUV.html",
            "date": " • Apr 15, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Summary of the BatchNorm paper",
            "content": "Summary of BatchNorm . What did the authors want to achieve ? . make normalization a part of the model | allow the use of higher learning rates by ensuring a stable distribution of nonlinear inputs =&gt; faster training, less iterations needed | improve robustness to initialization (more independent of good init) : reduce dependence of gradients on parameter scale and of the initial values | normalize the activations and preserve information in the network | . Key elements . Old approaches . whitening (linearly transforming inputs to have zero mean and unit variance and beingdecorrelated), has several problems. If the whitening modifiactions are interspersed with the optimization technique, gradient descent might try to update the parameters in a way that needs the normalization to be updated as well. This greatly reduces the effect of the backward pass step. In the paper this is shown by using considering a layer and normalizing the result with the mean of the training data. (see picture above) The authors show that the bias b will grow indefinitely while the loss remains the same. This was also observed in experiments, where the model blew up when the normalization parameters where computed outside of the backward pass. This is due to that approach not considering that during gradient descent, the normalization is taking place. | . Batch Norm . the idea is to normalize the activations during training, by normalizing the training samples (batches), relative to the statistics of the entire train set . as normalization may change what the layer already represents (Sigmoid normalization would constrain it to the linear part in between the saturation), the inserted transformation needs to be able to represent an identity tansformation. This is done by introducing two new learnable parameters for each batch for scaling and shifting the normalized value : | . . | . With $ gamma ^{k} = sqrt{Var[x^{k}]}$ and $ beta ^{k} = E[x^{k}]$, the original activation can be restored . for each mini-batch mean and covariance is computed seperately, therefore the name Batch Normalization, the small parameter eta is used in order to avoid division by zero, when the standard deviation is 0 (this could happen in case of bad init for example) : | . BN can be applied to every activation (at least in feedforward networks and as long as there is a high enough batch size), as BN is differentiable, the chain rule can be used to consider the BN transformation : | . . During training the following pseudocode applies : | . During testing a running moving average of mean and variance is used (linear transform), as the normalization based on a mini-batch is not desirable . | Batch Norm prevents small changes of parameters to amplify larger changes in our network. Higher learning rates also don&#39;t influence the scale of the parameters during backprop, therefore amplification is prevented as the layer Jacobian is unaffected. The singular values of the Jacobian are also close to 1, which helps preserve gradient magnitudes. Even though the transformation is not linear and the normalizations are not guaranteed to be Gaussian or independent, BN is still expected to improve gradient characterisitcs. . | . Implementation . Batch Norm can be implemented as follows in PyTorch : Also check out my summary of the Batch Norm part of the DL course by fastai for more normalization techniques such as running batch norm, layer and group norm, and a small Residual Net with Batch Norm. This is the same as the torch.nn module would do it, but it&#39;s always great to see it from scratch. . #collapse_show class BatchNorm(nn.Module): def __init__(self, nf, mom=0.1, eps=1e-5): super().__init__() # NB: pytorch bn mom is opposite of what you&#39;d expect self.mom,self.eps = mom,eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) self.register_buffer(&#39;vars&#39;, torch.ones(1,nf,1,1)) self.register_buffer(&#39;means&#39;, torch.zeros(1,nf,1,1)) def update_stats(self, x): #we average over all batches (0) and over x,y(2,3) coordinates (each filter) #keepdim=True means we can still broadcast nicely as these dimensions will be left empty m = x.mean((0,2,3), keepdim=True) v = x.var ((0,2,3), keepdim=True) self.means.lerp_(m, self.mom) self.vars.lerp_ (v, self.mom) return m,v def forward(self, x): if self.training: with torch.no_grad(): m,v = self.update_stats(x) else: m,v = self.means,self.vars x = (x-m) / (v+self.eps).sqrt() return x*self.mults + self.adds . . Results and Conclusion . Batch Norm allows to use only 7% of the training steps to match previous state of the art models on ImageNet without it | Batch Norm Inception beats the state of the art on the ImageNet challenge | Batch Norm reduces the need for Dropput greatly as claimed by the authors, however it was still used with the traditional dropout set up used by the Inception architects | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/14/Batch-Norm.html",
            "relUrl": "/jupyter/2020/04/14/Batch-Norm.html",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Summary of the ResNet paper",
            "content": "Summary of the ResNet paper . The architecture that won the 1st place on the ILSVRC 2015 classification task. . What did the authors want to achieve ? . Allow training of deep Networks | Improve Computer Vision Performance, by enabling the use of deeper architectures as depth is of crucial importance (as can be seen with to other ImageNet winners) =&gt; This applies to different vision tasks | Normalization and Initialization have largely adressed vanishing/exploding gradients in deep networks, however accuracy gets saturated and then degrates quickly with more depth : deep residual learning should help adress this issue and build on existing ideas such as shortcut connections | . Key elements . Residual Learning . $H(X)$ is considered to be the mapping of a few stacked layers. The idea of Residual Blocks, is that this mapping function $H(x) = F(x) + x$ can more easily be learned when we let it approximate this Residual funciton. This is based on the degradation problem, which suggests that the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers. The identity mapping should then allow deeper models to have an error no greater than that of a shallow counterpart. In reality, the identity mapping probably is not optimal, but it can still allow us to pre-condition the problem. This is shown by the small responese of the learned residual functions, which suggest that these mappings provide reasonable preconditioning. If the residual funciton has only a single layer, it resembles a linear function $y = W*x +x$, which is why an advantage can only be observed with more than one layer. | . The comparison between a plain 34-layer and 34-layer residual network looks like this : . Shortcuts . 3 different Shortcuts are used : . A) zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameterfree B) ) projection shortcuts are used for increasing dimensions, and other shortcuts are identity C) all shortcuts are projections . All 3 above are better than the plain counterpart, however C is slightly better than B, which is also slighlty better than A (see Table 4, under Classification). But the small differences are not essential for the degradation problem, and C adds parameters which has a negative impact on time/memory complexity. . Deeper Bottleneck Architectures . Because of training time, the block is redesigned as a bottleneck with smaller I/O dimensions. The three layers are 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers are responsible for reducing and then increasing (restoring) dimensions, leaving the 3×3 layer a bottleneck with smaller input/output dimensions. Identity shortcut connections are key, as with projection the model size and time complexity would double. . Implementation Details . 224x224 random crop is randomly sampled from an image, horizontal flips are used | SGD is used with lr starting at 0.1 and being divided by 10 as the error plateaus | training for up to 60x1e5 iterations | weight decay : 0.0001 and momentum of 0.9 | . Implementation of a ResBlock . A Residual Block can be implemented as follows in PyTorch : . def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True): &quot;&quot;&quot;Creates a convolutional layer, with optional batch normalization. &quot;&quot;&quot; layers = [] conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,kernel_size=kernel_size, stride=stride, padding=padding,bias=False) layers.append(conv_layer) if batch_norm: layers.append(nn.BatchNorm2d(out_channels)) return nn.Sequential(*layers) class Resblock(nn.Module): def __init__(self,conv_dim): super(Resblock,self).__init__() self.conv1 = conv(conv_dim, conv_dim, kernel_size=3, stride=2, batch_norm=True) self.conv2 = conv(conv_dim, conv_dim, kernel_size=3, stride=2, batch_norm=True) def forward(self,x): out1 = F.leaky_relu(self.conv1(x)) out = x + F.leaky_relu(self.conv2(out1)) return out def resblocks_create(conv_dim,n_res_blocks): res_layers = [] for l in range(0,n_res_blocks): res_layers.append(Resblock(conv_dim)) return nn.Sequential(*res_layers) . Instead of the conv function, you could obviously use the Impl . Results and Conclusion . Classification : . the models are trained on the 1.28 million training images of ImageNet 2012. The 50k images val set and 100k images test set is used. At first Plain Networks are compared to it&#39;s respective ResNet with identity mapping and 0 padding, which does not add extra parameters. The results show that the degradation error is adressed better which allows better performance with increased depth and allowed them to win the ImageNet Challenge in 2015. The experiments show that depth matters as it allows lower classification error : | . Object Detection : . Faster R-CNN is used, but unlike VGG-16, no hidden layers are used. A full images shared feature map is computed, using layers whose stride on the image is not greater than 16 pixels. (i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv layers in ResNet-101) These layers are anologous to VGG-16&#39;s 13 conv-layers and thereby have the same total stride (16 pixels). In consequence these layers are shared by a RPN with 300 proposals. . RoI pooling is performed before conv5_1. On this RoI-pooled feature, all layers of conv5 x and up are adopted for each region, playing the roles of VGG-16’s fc layers. Sibling layers (classification and bounding box regression) are used to replace the final classification layer. The BN layers are fixed, based on each layers Image Net mean and variance statistic. With this technique, using ResNet-101 the mAP (@0.5 IOU) can be improved a lot : | . Object localization : . a per class regression strategy is used (Bounding Box regressor for each class is learned). The Region Proposal Network ends with two sibling 1x1 convs for binary classification. So the Classification layer has a 1000 Dimension output, and for each dimension we predict if it is this object or not (binary). The regression layer has a 1000x4 d output, with box regressors for all 1000 classes based on multiple translation-invariant anchor boxes. Usually 8 acnhors are randomply sampled from the image, this avoids the dominance of negative samples. Data Augmentation is used, by sampling random 224x224 crops. Using ResNet-101 the state of the art can be improved : | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/13/Resnets.html",
            "relUrl": "/jupyter/2020/04/13/Resnets.html",
            "date": " • Apr 13, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Fastai Course DL from the Foundations Batch Norm",
            "content": "#collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_06 import * . . ConvNet . Let&#39;s get the data and training interface from where we left in the last notebook. . Jump_to lesson 10 video . #collapse x_train,y_train,x_valid,y_valid = get_data() x_train,x_valid = normalize_to(x_train,x_valid) train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) nh,bs = 50,512 c = y_train.max().item()+1 loss_func = F.cross_entropy data = DataBunch(*get_dls(train_ds, valid_ds, bs), c) . . #collapse mnist_view = view_tfm(1,28,28) cbfs = [Recorder, partial(AvgStatsCallback,accuracy), CudaCallback, partial(BatchTransformXCallback, mnist_view)] . . #collapse nfs = [8,16,32,64,64] . . #collapse learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs) . . #collapse %time run.fit(2, learn) . . train: [1.7780409375, tensor(0.3814, device=&#39;cuda:0&#39;)] valid: [0.457603125, tensor(0.8491, device=&#39;cuda:0&#39;)] train: [0.2377014453125, tensor(0.9279, device=&#39;cuda:0&#39;)] valid: [0.1322774658203125, tensor(0.9603, device=&#39;cuda:0&#39;)] CPU times: user 3.81 s, sys: 782 ms, total: 4.59 s Wall time: 5.29 s . Batchnorm . Custom . Let&#39;s start by building our own BatchNorm layer from scratch. We should be able to improve performance a lot. While training we keep a running exponentially weighted mean and variance average in the update_stats function. During inference we only use running mean and variance that we keep track off. We use register_bufferto create vars and means, this still creates self.vars and self.means, but if the model is moved to the GPU so will all buffers. Also it will be saved along with everything else in the model. . Jump_to lesson 10 video . #collapse_show class BatchNorm(nn.Module): def __init__(self, nf, mom=0.1, eps=1e-5): super().__init__() # NB: pytorch bn mom is opposite of what you&#39;d expect self.mom,self.eps = mom,eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) self.register_buffer(&#39;vars&#39;, torch.ones(1,nf,1,1)) self.register_buffer(&#39;means&#39;, torch.zeros(1,nf,1,1)) def update_stats(self, x): #we average over all batches (0) and over x,y(2,3) coordinates (each filter) #keepdim=True means we can still broadcast nicely as these dimensions will be left empty m = x.mean((0,2,3), keepdim=True) v = x.var ((0,2,3), keepdim=True) self.means.lerp_(m, self.mom) self.vars.lerp_ (v, self.mom) return m,v def forward(self, x): if self.training: with torch.no_grad(): m,v = self.update_stats(x) else: m,v = self.means,self.vars x = (x-m) / (v+self.eps).sqrt() return x*self.mults + self.adds . . Exponential moving average . We use exp. moving average, that way we only need to keep track of one element.The next value is computed with linear interpolation. PyTorch mom=0.1 is actually 0.9 in math terms. (1-mom) . Now we define our batch norm conv_layer, when we use batch norm we can remove the bias layer as batch norm adds are a bias. . #collapse_show def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs): # No bias needed if using bn layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn), GeneralRelu(**kwargs)] if bn: layers.append(BatchNorm(nf)) return nn.Sequential(*layers) . . #collapse_show def init_cnn_(m, f): if isinstance(m, nn.Conv2d): f(m.weight, a=0.1) if getattr(m, &#39;bias&#39;, None) is not None: m.bias.data.zero_() for l in m.children(): init_cnn_(l, f) def init_cnn(m, uniform=False): f = init.kaiming_uniform_ if uniform else init.kaiming_normal_ init_cnn_(m, f) def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs): model = get_cnn_model(data, nfs, layer, **kwargs) init_cnn(model, uniform=uniform) return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func) . . Above the modules are initalized recursively. We can then use it in training and see how it helps keep the activations means to 0 and the std to 1. . #collapse learn,run = get_learn_run(nfs, data, 0.9, conv_layer, cbs=cbfs) . . Train with Hooks : . #collapse_show with Hooks(learn.model, append_stats) as hooks: run.fit(1, learn) fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks[:-1]: ms,ss = h.stats ax0.plot(ms[:10]) ax1.plot(ss[:10]) h.remove() plt.legend(range(6)); fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks[:-1]: ms,ss = h.stats ax0.plot(ms) ax1.plot(ss) . . train: [0.29335978515625, tensor(0.9089, device=&#39;cuda:0&#39;)] valid: [0.197817431640625, tensor(0.9373, device=&#39;cuda:0&#39;)] . #collapse learn,run = get_learn_run(nfs, data, 1.0, conv_layer, cbs=cbfs) . . #collapse %time run.fit(3, learn) . . train: [0.26908765625, tensor(0.9149, device=&#39;cuda:0&#39;)] valid: [0.151610400390625, tensor(0.9536, device=&#39;cuda:0&#39;)] train: [0.086416064453125, tensor(0.9732, device=&#39;cuda:0&#39;)] valid: [0.1458560791015625, tensor(0.9568, device=&#39;cuda:0&#39;)] train: [0.0614888330078125, tensor(0.9807, device=&#39;cuda:0&#39;)] valid: [0.08743798828125, tensor(0.9744, device=&#39;cuda:0&#39;)] CPU times: user 3.12 s, sys: 221 ms, total: 3.34 s Wall time: 3.28 s . Builtin batchnorm . Jump_to lesson 10 video . #collapse_show def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs): layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn), GeneralRelu(**kwargs)] if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1)) return nn.Sequential(*layers) . . #collapse learn,run = get_learn_run(nfs, data, 1., conv_layer, cbs=cbfs) . . #collapse %time run.fit(3, learn) . . train: [0.21728392578125, tensor(0.9316, device=&#39;cuda:0&#39;)] valid: [0.08256806640625, tensor(0.9763, device=&#39;cuda:0&#39;)] train: [0.0592223681640625, tensor(0.9822, device=&#39;cuda:0&#39;)] valid: [0.06992761840820312, tensor(0.9806, device=&#39;cuda:0&#39;)] train: [0.038743759765625, tensor(0.9881, device=&#39;cuda:0&#39;)] valid: [0.05885752563476562, tensor(0.9837, device=&#39;cuda:0&#39;)] CPU times: user 3.12 s, sys: 92.9 ms, total: 3.22 s Wall time: 3.17 s . With scheduler . Now let&#39;s add the usual warm-up/annealing. . #collapse sched = combine_scheds([0.3, 0.7], [sched_lin(0.6, 2.), sched_lin(2., 0.1)]) . . #collapse_show learn,run = get_learn_run(nfs, data, 0.9, conv_layer, cbs=cbfs +[partial(ParamScheduler,&#39;lr&#39;, sched)]) . . #collapse run.fit(8, learn) . . train: [0.24476150390625, tensor(0.9275, device=&#39;cuda:0&#39;)] valid: [0.107988427734375, tensor(0.9694, device=&#39;cuda:0&#39;)] train: [0.081850380859375, tensor(0.9743, device=&#39;cuda:0&#39;)] valid: [0.11897130126953125, tensor(0.9625, device=&#39;cuda:0&#39;)] train: [0.057883427734375, tensor(0.9814, device=&#39;cuda:0&#39;)] valid: [0.0801966064453125, tensor(0.9750, device=&#39;cuda:0&#39;)] train: [0.0324224951171875, tensor(0.9900, device=&#39;cuda:0&#39;)] valid: [0.05837650146484375, tensor(0.9829, device=&#39;cuda:0&#39;)] train: [0.01931252197265625, tensor(0.9942, device=&#39;cuda:0&#39;)] valid: [0.048959814453125, tensor(0.9843, device=&#39;cuda:0&#39;)] train: [0.011390489501953125, tensor(0.9969, device=&#39;cuda:0&#39;)] valid: [0.04460760192871094, tensor(0.9857, device=&#39;cuda:0&#39;)] train: [0.00623075439453125, tensor(0.9988, device=&#39;cuda:0&#39;)] valid: [0.04336626892089844, tensor(0.9878, device=&#39;cuda:0&#39;)] train: [0.004166948547363282, tensor(0.9995, device=&#39;cuda:0&#39;)] valid: [0.042969384765625, tensor(0.9882, device=&#39;cuda:0&#39;)] . More norms . Layer norm . From the paper: &quot;batch normalization cannot be applied to online learning tasks or to extremely large distributed models where the minibatches have to be small&quot;. This is the case for large Nets that only allow for small batch sizes. Also RNNs are a problem, as our for loop can not vary the batch size easily. . General equation for a norm layer with learnable affine: . $$y = frac{x - mathrm{E}[x]}{ sqrt{ mathrm{Var}[x] + epsilon}} * gamma + beta$$ . The difference with BatchNorm is . we don&#39;t keep a moving average | we don&#39;t average over the batches dimension but over the hidden dimension, so it&#39;s independent of the batch size | Jump_to lesson 10 video . #collapse_show class LayerNorm(nn.Module): __constants__ = [&#39;eps&#39;] def __init__(self, eps=1e-5): super().__init__() self.eps = eps self.mult = nn.Parameter(tensor(1.)) self.add = nn.Parameter(tensor(0.)) def forward(self, x): m = x.mean((1,2,3), keepdim=True) v = x.var ((1,2,3), keepdim=True) x = (x-m) / ((v+self.eps).sqrt()) return x*self.mult + self.add . . Keep in mind that compared to BN we use m = x.mean((1,2,3), keepdim=True) instead of m=x.mean((0,2,3), keepdim=True) and we do not use the exp. moving average. The reason is that every image has it&#39;s own mean as we do not use batches anymore. . #collapse def conv_ln(ni, nf, ks=3, stride=2, bn=True, **kwargs): layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True), GeneralRelu(**kwargs)] if bn: layers.append(LayerNorm()) return nn.Sequential(*layers) . . #collapse_show learn,run = get_learn_run(nfs, data, 0.8, conv_ln, cbs=cbfs) . . #collapse_show %time run.fit(3, learn) . . train: [nan, tensor(0.1330, device=&#39;cuda:0&#39;)] valid: [nan, tensor(0.0991, device=&#39;cuda:0&#39;)] train: [nan, tensor(0.0986, device=&#39;cuda:0&#39;)] valid: [nan, tensor(0.0991, device=&#39;cuda:0&#39;)] train: [nan, tensor(0.0986, device=&#39;cuda:0&#39;)] valid: [nan, tensor(0.0991, device=&#39;cuda:0&#39;)] CPU times: user 3.81 s, sys: 56.3 ms, total: 3.87 s Wall time: 3.82 s . Thought experiment: can this distinguish foggy days from sunny days (assuming you&#39;re using it before the first conv)? No we can not, layer norm will lead to the same normalization for both pictures. As we can see LN is not as good as BN, but it can be used for RNNS. . Instance norm . From the paper: . The key difference between contrast and batch normalization is that the latter applies the normalization to a whole batch of images instead for single ones: . begin{equation} label{eq:bnorm} y_{tijk} = frac{x_{tijk} - mu_{i}}{ sqrt{ sigma_i^2 + epsilon}}, quad mu_i = frac{1}{HWT} sum_{t=1}^T sum_{l=1}^W sum_{m=1}^H x_{tilm}, quad sigma_i^2 = frac{1}{HWT} sum_{t=1}^T sum_{l=1}^W sum_{m=1}^H (x_{tilm} - mu_i)^2. end{equation}In order to combine the effects of instance-specific normalization and batch normalization, we propose to replace the latter by the instance normalization (also known as contrast normalization) layer: . begin{equation} label{eq:inorm} y_{tijk} = frac{x_{tijk} - mu_{ti}}{ sqrt{ sigma_{ti}^2 + epsilon}}, quad mu_{ti} = frac{1}{HW} sum_{l=1}^W sum_{m=1}^H x_{tilm}, quad sigma_{ti}^2 = frac{1}{HW} sum_{l=1}^W sum_{m=1}^H (x_{tilm} - mu_{ti})^2. end{equation} Jump_to lesson 10 video . #collapse_show class InstanceNorm(nn.Module): __constants__ = [&#39;eps&#39;] def __init__(self, nf, eps=1e-0): super().__init__() self.eps = eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) def forward(self, x): m = x.mean((2,3), keepdim=True) v = x.var ((2,3), keepdim=True) res = (x-m) / ((v+self.eps).sqrt()) return res*self.mults + self.adds . . Keep in mind that compared to LN we use m = x.mean((2,3), keepdim=True) instead of m=x.mean((1,2,3), keepdim=True). . #collapse def conv_in(ni, nf, ks=3, stride=2, bn=True, **kwargs): layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True), GeneralRelu(**kwargs)] if bn: layers.append(InstanceNorm(nf)) return nn.Sequential(*layers) . . #collapse learn,run = get_learn_run(nfs, data, 0.1, conv_in, cbs=cbfs) . . #collapse %time run.fit(3, learn) . . train: [nan, tensor(0.0986, device=&#39;cuda:0&#39;)] valid: [nan, tensor(0.0991, device=&#39;cuda:0&#39;)] train: [nan, tensor(0.0986, device=&#39;cuda:0&#39;)] valid: [nan, tensor(0.0991, device=&#39;cuda:0&#39;)] train: [nan, tensor(0.0986, device=&#39;cuda:0&#39;)] valid: [nan, tensor(0.0991, device=&#39;cuda:0&#39;)] CPU times: user 3.74 s, sys: 69.5 ms, total: 3.81 s Wall time: 3.78 s . Question: why can&#39;t this classify anything? We are now using the mean and variance for every image and every channel, throwing away the things that allow classification. It was not designed for Classification, but rather for style transfer where the differences in contrast and overall amount are not important according to the authors. . Lost in all those norms? The authors from the group norm paper have you covered: . . Group norm . Jump_to lesson 10 video . From the PyTorch docs: . GroupNorm(num_groups, num_channels, eps=1e-5, affine=True) . The input channels are separated into num_groups groups, each containing num_channels / num_groups channels. The mean and standard-deviation are calculated separately over the each group. $ gamma$ and $ beta$ are learnable per-channel affine transform parameter vectorss of size num_channels if affine is True. . This layer uses statistics computed from input data in both training and evaluation modes. . Args: . num_groups (int): number of groups to separate the channels into | num_channels (int): number of channels expected in input | eps: a value added to the denominator for numerical stability. Default: 1e-5 | affine: a boolean value that when set to True, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: True. | . Shape: . Input: (N, num_channels, *) | Output: (N, num_channels, *) (same shape as input) | . Examples:: . &gt;&gt;&gt; input = torch.randn(20, 6, 10, 10) &gt;&gt;&gt; # Separate 6 channels into 3 groups &gt;&gt;&gt; m = nn.GroupNorm(3, 6) &gt;&gt;&gt; # Separate 6 channels into 6 groups (equivalent with InstanceNorm) &gt;&gt;&gt; m = nn.GroupNorm(6, 6) &gt;&gt;&gt; # Put all 6 channels into a single group (equivalent with LayerNorm) &gt;&gt;&gt; m = nn.GroupNorm(1, 6) &gt;&gt;&gt; # Activating the module &gt;&gt;&gt; output = m(input) . Fix small batch sizes . What&#39;s the problem? . When we compute the statistics (mean and std) for a BatchNorm Layer on a small batch, it is possible that we get a standard deviation very close to 0. because there aren&#39;t many samples (the variance of one thing is 0. since it&#39;s equal to its mean). . Jump_to lesson 10 video . #collapse data = DataBunch(*get_dls(train_ds, valid_ds, 2), c) . . #collapse_show def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs): layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn), GeneralRelu(**kwargs)] if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1)) return nn.Sequential(*layers) . . #collapse learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs) . . #collapse %time run.fit(1, learn) . . train: [2.3492684375, tensor(0.1668, device=&#39;cuda:0&#39;)] valid: [27557812.6336, tensor(0.1274, device=&#39;cuda:0&#39;)] CPU times: user 1min 32s, sys: 766 ms, total: 1min 33s Wall time: 1min 34s . Running Batch Norm . To solve this problem we introduce a Running BatchNorm that uses smoother running mean and variance for the mean and std. Eps is used to avoid divergence, it is used as a hyperparameter. Running Batch Norm is a good solution (best so far according to Jeremy) for the small batch size problem. . 1) It does not divide by the batch standard deviation, but it uses the moving average stats at training time as well, just like during inference. Accuracy increases a lot ! As we should not compute the running average of the variances, especially as there might be different batch sizes as well. We use the formula : $E[X^{2}]-E[X]^{2}$ So we use the squares and the sums with a buffer. . 2) And then we take the exp. moving average of these and interpolate. We also take the exponential moving average of the batch sizes, it tells us what we need to divide by : (total number of elements by the mini batch divided by number of channels) . 3) Debiasing Make sure that at every point, no observation is weighted too much. (early elements have more relevance as they appear more often) . 4) For the first elements : We might be unlucky, so that our first mini batch is very close to zero. So we clamp the first few elements (for example 20) variance to be 0.01. . Jump_to lesson 10 video . #collapse_show class RunningBatchNorm(nn.Module): def __init__(self, nf, mom=0.1, eps=1e-5): super().__init__() self.mom,self.eps = mom,eps self.mults = nn.Parameter(torch.ones (nf,1,1)) self.adds = nn.Parameter(torch.zeros(nf,1,1)) self.register_buffer(&#39;sums&#39;, torch.zeros(1,nf,1,1)) self.register_buffer(&#39;sqrs&#39;, torch.zeros(1,nf,1,1)) self.register_buffer(&#39;batch&#39;, tensor(0.)) self.register_buffer(&#39;count&#39;, tensor(0.)) self.register_buffer(&#39;step&#39;, tensor(0.)) self.register_buffer(&#39;dbias&#39;, tensor(0.)) def update_stats(self, x): bs,nc,*_ = x.shape self.sums.detach_() self.sqrs.detach_() dims = (0,2,3) s = x.sum(dims, keepdim=True) ss = (x*x).sum(dims, keepdim=True) c = self.count.new_tensor(x.numel()/nc) mom1 = 1 - (1-self.mom)/math.sqrt(bs-1) self.mom1 = self.dbias.new_tensor(mom1) self.sums.lerp_(s, self.mom1) self.sqrs.lerp_(ss, self.mom1) self.count.lerp_(c, self.mom1) self.dbias = self.dbias*(1-self.mom1) + self.mom1 self.batch += bs self.step += 1 def forward(self, x): if self.training: self.update_stats(x) sums = self.sums sqrs = self.sqrs c = self.count if self.step&lt;100: sums = sums / self.dbias sqrs = sqrs / self.dbias c = c / self.dbias means = sums/c vars = (sqrs/c).sub_(means*means) if bool(self.batch &lt; 20): vars.clamp_min_(0.01) x = (x-means).div_((vars.add_(self.eps)).sqrt()) return x.mul_(self.mults).add_(self.adds) . . #collapse_show def get_cnn_layers(data, nfs, layer,conv_dim,n_res_block, **kwargs): nfs = [1] + nfs res = resblocks_create(conv_dim,1) print(res) layers= [layer(nfs[i], nfs[i+1], 5 if i==0 else 3, **kwargs) for i in range(len(nfs)-1)] + list(res) + [nn.AdaptiveAvgPool2d(1), Lambda(flatten),nn.Dropout(0.4), nn.Linear(nfs[-1], data.c)] print(layers) return layers def conv_layer(ni, nf, ks=3, stride=2, **kwargs): return nn.Sequential( nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), GeneralRelu(**kwargs)) class GeneralRelu(nn.Module): def __init__(self, leak=None, sub=None, maxv=None): super().__init__() self.leak,self.sub,self.maxv = leak,sub,maxv def forward(self, x): x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x) if self.sub is not None: x.sub_(self.sub) if self.maxv is not None: x.clamp_max_(self.maxv) return x def init_cnn_(m,f): if isinstance(m,nn.Conv2d): f(m.weight,a = 0.1) if getattr(m,&#39;bias&#39;,None) is not None : m.bias.data.zero_() for l in m.children() : init_cnn_(l,f) def init_cnn(m, uniform=False): f = init.kaiming_uniform_ if uniform else init.kaiming_normal_ init_cnn_(m,f) def get_cnn_model(data, nfs, layer,conv_dim,n_res_block, **kwargs): return nn.Sequential(*get_cnn_layers(data, nfs, layer,conv_dim,n_res_block, **kwargs)) . . #collapse_show class Runner(): def __init__(self, cbs=None, cb_funcs=None): cbs = listify(cbs) for cbf in listify(cb_funcs): cb = cbf() setattr(self, cb.name, cb) cbs.append(cb) self.stop,self.cbs = False,[TrainEvalCallback()]+cbs @property def opt(self): return self.learn.opt @property def model(self): return self.learn.model @property def loss_func(self): return self.learn.loss_func @property def data(self): return self.learn.data def one_batch(self, xb, yb): try: self.xb,self.yb = xb,yb self(&#39;begin_batch&#39;) self.pred = self.model(self.xb) self(&#39;after_pred&#39;) self.loss = self.loss_func(self.pred, self.yb) self(&#39;after_loss&#39;) if not self.in_train: return self.loss.backward() self(&#39;after_backward&#39;) self.opt.step() self(&#39;after_step&#39;) self.opt.zero_grad() except CancelBatchException: self(&#39;after_cancel_batch&#39;) finally: self(&#39;after_batch&#39;) def all_batches(self, dl): self.iters = len(dl) try: for xb,yb in dl: print(&#39;Batch&#39;) self.one_batch(xb, yb) except CancelEpochException: self(&#39;after_cancel_epoch&#39;) def fit(self, epochs, learn): self.epochs,self.learn,self.loss = epochs,learn,tensor(0.) try: for cb in self.cbs: cb.set_runner(self) self(&#39;begin_fit&#39;) for epoch in range(epochs): self.epoch = epoch if not self(&#39;begin_epoch&#39;): self.all_batches(self.data.train_dl) with torch.no_grad(): if not self(&#39;begin_validate&#39;): self.all_batches(self.data.valid_dl) self(&#39;after_epoch&#39;) except CancelTrainException: self(&#39;after_cancel_train&#39;) finally: self(&#39;after_fit&#39;) self.learn = None def __call__(self, cb_name): res = False for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) or res return res . . If you recall from the lecture, a simple CNN is used. The goal was to improve one epoch training validation error as a practice toy problem. I did beat the 0.975 slightly from the video with Dropout in the fully connected part, but further added a Residual block after the normal Conv Part, allowing u. . #collapse_show def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy): if opt_func is None: opt_func = optim.SGD opt = opt_func(model.parameters(), lr=lr) learn = Learner(model, opt, loss_func, data) return learn, Runner(cb_funcs=listify(cbs)) . . #collapse_show def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True): &quot;&quot;&quot;Creates a convolutional layer, with optional batch normalization. &quot;&quot;&quot; layers = [] conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False) layers.append(conv_layer) if batch_norm: layers.append(nn.BatchNorm2d(out_channels)) return nn.Sequential(*layers) . . #collapse_show class Resblock(nn.Module): def __init__(self,conv_dim): super(Resblock,self).__init__() self.conv1 = conv(conv_dim, conv_dim, kernel_size=3, stride=2, batch_norm=True) self.conv2 = conv(conv_dim, conv_dim, kernel_size=3, stride=2, batch_norm=True) def forward(self,x): out1 = F.leaky_relu(self.conv1(x)) out = x + F.leaky_relu(self.conv2(out1)) return out . . #collapse_show def resblocks_create(conv_dim,n_res_blocks): res_layers = [] for l in range(0,n_res_blocks): res_layers.append(Resblock(conv_dim)) return nn.Sequential(*res_layers) . . #collapse_show def conv_rbn(ni, nf, ks=3, stride=2, bn=True, **kwargs): layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn), GeneralRelu(**kwargs)] if bn: layers.append(RunningBatchNorm(nf)) print(**kwargs) return nn.Sequential(*layers) . . #collapse def get_learn_run(nfs, data,conv_dim,n_res_block, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs): model = get_cnn_model(data, nfs, layer,conv_dim,n_res_block, **kwargs) init_cnn(model, uniform=False) return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func) . . #collapse nfs = [8,16,32,64] . . #collapse learn,run = get_learn_run(nfs, data, 64,1,0.4, conv_rbn, cbs=cbfs) . . Sequential( (0): Resblock( (conv1): Sequential( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (conv2): Sequential( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) [Sequential( (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False) (1): GeneralRelu() (2): RunningBatchNorm() ), Sequential( (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): RunningBatchNorm() ), Sequential( (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): RunningBatchNorm() ), Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): RunningBatchNorm() ), Resblock( (conv1): Sequential( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (conv2): Sequential( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ), AdaptiveAvgPool2d(output_size=1), Lambda(), Dropout(p=0.4, inplace=False), Linear(in_features=64, out_features=10, bias=True)] . learn.model . Sequential( (0): Sequential( (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False) (1): GeneralRelu() (2): RunningBatchNorm() ) (1): Sequential( (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): RunningBatchNorm() ) (2): Sequential( (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): RunningBatchNorm() ) (3): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): RunningBatchNorm() ) (4): Resblock( (conv1): Sequential( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (conv2): Sequential( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): AdaptiveAvgPool2d(output_size=1) (6): Lambda() (7): Dropout(p=0.4, inplace=False) (8): Linear(in_features=64, out_features=10, bias=True) ) . #collapse %time run.fit(1, learn) . . train: [34.587845, tensor(0.3369, device=&#39;cuda:0&#39;)] valid: [96743292.928, tensor(0.4051, device=&#39;cuda:0&#39;)] CPU times: user 3min 41s, sys: 1.03 s, total: 3min 42s Wall time: 3min 43s . This solves the small batch size issue! . What can we do in a single epoch? . Now let&#39;s see with a decent batch size what result we can get. . Jump_to lesson 10 video . #collapse data = DataBunch(*get_dls(train_ds, valid_ds, 16), c) . . #collapse learn,run = get_learn_run(nfs, data, 64,1, 0.9, conv_rbn, cbs=cbfs +[partial(ParamScheduler,&#39;lr&#39;, sched_lin(1., 0.2))]) . . Sequential( (0): Resblock( (conv1): Sequential( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (conv2): Sequential( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) [Sequential( (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False) (1): GeneralRelu() (2): RunningBatchNorm() ), Sequential( (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): RunningBatchNorm() ), Sequential( (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): RunningBatchNorm() ), Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): GeneralRelu() (2): RunningBatchNorm() ), Resblock( (conv1): Sequential( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (conv2): Sequential( (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ), AdaptiveAvgPool2d(output_size=1), Lambda(), Dropout(p=0.4, inplace=False), Linear(in_features=64, out_features=10, bias=True)] . #collapse %time run.fit(1, learn) . . Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch Batch train: [0.12176001953125, tensor(0.9641, device=&#39;cuda:0&#39;)] valid: [0.053298565673828124, tensor(0.9842, device=&#39;cuda:0&#39;)] CPU times: user 29.5 s, sys: 327 ms, total: 29.8 s Wall time: 29.5 s .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/12/Batchnorm.html",
            "relUrl": "/jupyter/2020/04/12/Batchnorm.html",
            "date": " • Apr 12, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Fastai Course DL from the Foundations Callbacks and Learning Rate Finder",
            "content": "DL from the foundations Lesson 3 Part 2 . Exceptions for Callbacks . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . #collapse from exp.nb_05 import * . . Jump_to notebook introduction in lesson 10 video . Early stopping . Better callback cancellation . Jump_to lesson 10 video . #collapse x_train,y_train,x_valid,y_valid = get_data() train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) nh,bs = 50,512 c = y_train.max().item()+1 loss_func = F.cross_entropy . . #collapse data = DataBunch(*get_dls(train_ds, valid_ds, bs), c) . . Improving on Lesson 2 : it makes more sense for the __call__to be inside the callback class. We can now set traces, replace the method,... which gives us extra flexibility. Pass just means that a class has same attributes but we can now use a different name for the class. . #collapse_show class Callback(): _order=0 def set_runner(self, run): self.run=run def __getattr__(self, k): return getattr(self.run, k) @property def name(self): name = re.sub(r&#39;Callback$&#39;, &#39;&#39;, self.__class__.__name__) return camel2snake(name or &#39;callback&#39;) def __call__(self, cb_name): f = getattr(self, cb_name, None) if f and f(): return True return False class TrainEvalCallback(Callback): def begin_fit(self): self.run.n_epochs=0. self.run.n_iter=0 def after_batch(self): if not self.in_train: return self.run.n_epochs += 1./self.iters self.run.n_iter += 1 def begin_epoch(self): self.run.n_epochs=self.epoch self.model.train() self.run.in_train=True def begin_validate(self): self.model.eval() self.run.in_train=False class CancelTrainException(Exception): pass class CancelEpochException(Exception): pass class CancelBatchException(Exception): pass . . We can now use exceptions to handle control flow. So for example we can use them to go to the next batch when an exception is called. . #collapse_show class Runner(): def __init__(self, cbs=None, cb_funcs=None): cbs = listify(cbs) for cbf in listify(cb_funcs): cb = cbf() setattr(self, cb.name, cb) cbs.append(cb) self.stop,self.cbs = False,[TrainEvalCallback()]+cbs @property def opt(self): return self.learn.opt @property def model(self): return self.learn.model @property def loss_func(self): return self.learn.loss_func @property def data(self): return self.learn.data def one_batch(self, xb, yb): try: self.xb,self.yb = xb,yb self(&#39;begin_batch&#39;) self.pred = self.model(self.xb) self(&#39;after_pred&#39;) self.loss = self.loss_func(self.pred, self.yb) self(&#39;after_loss&#39;) if not self.in_train: return self.loss.backward() self(&#39;after_backward&#39;) self.opt.step() self(&#39;after_step&#39;) self.opt.zero_grad() except CancelBatchException: self(&#39;after_cancel_batch&#39;) finally: self(&#39;after_batch&#39;) def all_batches(self, dl): self.iters = len(dl) try: for xb,yb in dl: self.one_batch(xb, yb) except CancelEpochException: self(&#39;after_cancel_epoch&#39;) def fit(self, epochs, learn): self.epochs,self.learn,self.loss = epochs,learn,tensor(0.) try: for cb in self.cbs: cb.set_runner(self) self(&#39;begin_fit&#39;) for epoch in range(epochs): self.epoch = epoch if not self(&#39;begin_epoch&#39;): self.all_batches(self.data.train_dl) with torch.no_grad(): if not self(&#39;begin_validate&#39;): self.all_batches(self.data.valid_dl) self(&#39;after_epoch&#39;) except CancelTrainException: self(&#39;after_cancel_train&#39;) finally: self(&#39;after_fit&#39;) self.learn = None def __call__(self, cb_name): res = False for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) or res return res . . #collapse_show learn = create_learner(get_model, loss_func, data) . . #collapse_show class TestCallback(Callback): _order=1 def after_step(self): print(self.n_iter) if self.n_iter&gt;=10: raise CancelTrainException() . . #collapse run = Runner(cb_funcs=TestCallback) . . #collapse run.fit(3, learn) . . 0 1 2 3 4 5 6 7 8 9 10 . Other callbacks . #collapse_show class AvgStatsCallback(Callback): def __init__(self, metrics): self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False) def begin_epoch(self): self.train_stats.reset() self.valid_stats.reset() def after_loss(self): stats = self.train_stats if self.in_train else self.valid_stats with torch.no_grad(): stats.accumulate(self.run) def after_epoch(self): print(self.train_stats) print(self.valid_stats) class Recorder(Callback): def begin_fit(self): self.lrs = [[] for _ in self.opt.param_groups] self.losses = [] def after_batch(self): if not self.in_train: return for pg,lr in zip(self.opt.param_groups,self.lrs): lr.append(pg[&#39;lr&#39;]) self.losses.append(self.loss.detach().cpu()) def plot_lr (self, pgid=-1): plt.plot(self.lrs[pgid]) def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last]) def plot(self, skip_last=0, pgid=-1): losses = [o.item() for o in self.losses] lrs = self.lrs[pgid] n = len(losses)-skip_last plt.xscale(&#39;log&#39;) plt.plot(lrs[:n], losses[:n]) class ParamScheduler(Callback): _order=1 def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,sched_funcs def begin_fit(self): if not isinstance(self.sched_funcs, (list,tuple)): self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups) def set_param(self): assert len(self.opt.param_groups)==len(self.sched_funcs) for pg,f in zip(self.opt.param_groups,self.sched_funcs): pg[self.pname] = f(self.n_epochs/self.epochs) def begin_batch(self): if self.in_train: self.set_param() . . LR Finder . Exceptions can be used for the lr finder for example, when the right lr is found an exception is called and we can stop ! . NB: You may want to also add something that saves the model before running this, and loads it back after running - otherwise you&#39;ll lose your weights! . Jump_to lesson 10 video . #collapse_show class LR_Find(Callback): _order=1 def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10): self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr self.best_loss = 1e9 def begin_batch(self): if not self.in_train: return pos = self.n_iter/self.max_iter lr = self.min_lr * (self.max_lr/self.min_lr) ** pos for pg in self.opt.param_groups: pg[&#39;lr&#39;] = lr def after_step(self): if self.n_iter&gt;=self.max_iter or self.loss&gt;self.best_loss*10: raise CancelTrainException() if self.loss &lt; self.best_loss: self.best_loss = self.loss . . NB: In fastai we also use exponential smoothing on the loss. For that reason we check for best_loss*3 instead of best_loss*10. . #collapse_show learn = create_learner(get_model, loss_func, data) . . #collapse_show run = Runner(cb_funcs=[LR_Find, Recorder]) . . #collapse_show run.fit(2, learn) . . #collapse_show run.recorder.plot(skip_last=5) . . #collapse_show run.recorder.plot_lr() . .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/10/Lesson-three-part-two.html",
            "relUrl": "/jupyter/2020/04/10/Lesson-three-part-two.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Fastai Course DL From the Foundations CUDA Hooks",
            "content": "#collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . #collapse from exp.nb_05b import * torch.set_num_threads(2) . . ConvNet . Jump_to lesson 10 video . #collapse x_train,y_train,x_valid,y_valid = get_data() . . Helper function to quickly normalize with the mean and standard deviation from our training set: . #collapse_show def normalize_to(train, valid): m,s = train.mean(),train.std() return normalize(train, m, s), normalize(valid, m, s) . . #collapse_show x_train,x_valid = normalize_to(x_train,x_valid) train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) . . Let&#39;s check it behaved properly. . #collapse_show x_train.mean(),x_train.std() . . (tensor(3.0614e-05), tensor(1.)) . #collapse_show nh,bs = 50,512 c = y_train.max().item()+1 loss_func = F.cross_entropy data = DataBunch(*get_dls(train_ds, valid_ds, bs), c) . . To refactor layers, it&#39;s useful to have a Lambda layer that can take a basic function and convert it to a layer you can put in nn.Sequential. . NB: if you use a Lambda layer with a lambda function, your model won&#39;t pickle so you won&#39;t be able to save it with PyTorch. So it&#39;s best to give a name to the function you&#39;re using inside your Lambda (like flatten below). . #collapse_show class Lambda(nn.Module): def __init__(self, func): super().__init__() self.func = func def forward(self, x): return self.func(x) def flatten(x): return x.view(x.shape[0], -1) . . This one takes the flat vector of size bs x 784 and puts it back as a batch of images of 28 by 28 pixels: . #collapse_show def mnist_resize(x): return x.view(-1, 1, 28, 28) . . We can now define a simple CNN. With the Lambda class we can now use the resize method in our model directly as well as the flatten method before the fully connected layer. . #collapse_show def get_cnn_model(data): return nn.Sequential( Lambda(mnist_resize), nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), #14 nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 7 nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4 nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2 nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(32,data.c) ) . . #collapse_show model = get_cnn_model(data) . . Basic callbacks from the previous notebook: . #collapse_show cbfs = [Recorder, partial(AvgStatsCallback,accuracy)] . . #collapse_show opt = optim.SGD(model.parameters(), lr=0.4) learn = Learner(model, opt, loss_func, data) run = Runner(cb_funcs=cbfs) . . #collapse_show %time run.fit(1, learn) . . train: [2.06754984375, tensor(0.2895)] valid: [2.14900390625, tensor(0.3016)] CPU times: user 9.03 s, sys: 547 ms, total: 9.58 s Wall time: 3.52 s . CUDA . This took a long time to run, so it&#39;s time to use a GPU. A simple Callback can make sure the model, inputs and targets are all on the same device. . Jump_to lesson 10 video . #collapse_show # Somewhat more flexible way device = torch.device(&#39;cuda&#39;,0) . . #collapse_show class CudaCallback(Callback): def __init__(self,device): self.device=device def begin_fit(self): self.model.to(self.device) def begin_batch(self): self.run.xb,self.run.yb = self.xb.to(self.device),self.yb.to(self.device) . . We can also set the device so we have it as our default : . #collapse_show torch.cuda.set_device(device) . . #collapse_show class CudaCallback(Callback): def begin_fit(self): self.model.cuda() def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda() . . #collapse cbfs.append(CudaCallback) . . #collapse model = get_cnn_model(data) . . #collapse_show opt = optim.SGD(model.parameters(), lr=0.4) learn = Learner(model, opt, loss_func, data) run = Runner(cb_funcs=cbfs) . . #collapse_show %time run.fit(3, learn) . . train: [1.9803146875, tensor(0.2964, device=&#39;cuda:0&#39;)] valid: [0.7705763671875, tensor(0.7300, device=&#39;cuda:0&#39;)] train: [0.4521004296875, tensor(0.8600, device=&#39;cuda:0&#39;)] valid: [0.1957219482421875, tensor(0.9439, device=&#39;cuda:0&#39;)] train: [0.19470791015625, tensor(0.9407, device=&#39;cuda:0&#39;)] valid: [0.1463418701171875, tensor(0.9581, device=&#39;cuda:0&#39;)] CPU times: user 4.12 s, sys: 662 ms, total: 4.78 s Wall time: 5.46 s . Now, that&#39;s definitely faster! . Refactor model . First we can regroup all the conv/relu in a single function. We also set our default values for stride and filter size. . Jump_to lesson 10 video . #collapse_show def conv2d(ni, nf, ks=3, stride=2): return nn.Sequential( nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), nn.ReLU()) . . Another thing is that we can do the mnist resize in a batch transform, that we can do with a Callback. We can pass it a transformation function and store it away. In this case we resize our input and can set it as default with the partial function. . #collapse_show class BatchTransformXCallback(Callback): _order=2 def __init__(self, tfm): self.tfm = tfm def begin_batch(self): self.run.xb = self.tfm(self.xb) def view_tfm(*size): def _inner(x): return x.view(*((-1,)+size)) return _inner . . #collapse_show mnist_view = view_tfm(1,28,28) cbfs.append(partial(BatchTransformXCallback, mnist_view)) . . With the AdaptiveAvgPool, this model can now work on any size input: . #collapse_show nfs = [8,16,32,32] . . We can pass it the above array and configure the network easily that way. . #collapse_show def get_cnn_layers(data, nfs): nfs = [1] + nfs return [ conv2d(nfs[i], nfs[i+1], 5 if i==0 else 3) for i in range(len(nfs)-1) ] + [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)] def get_cnn_model(data, nfs): return nn.Sequential(*get_cnn_layers(data, nfs)) . . And this helper function will quickly give us everything needed to run the training. The kernel size is 5 in the first layer and 3 otherwise. It&#39;s because we want to use a larger filter in order to perform useful computation that reduces the amount of actvations and get useful features that way. . #collapse_show def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy): if opt_func is None: opt_func = optim.SGD opt = opt_func(model.parameters(), lr=lr) learn = Learner(model, opt, loss_func, data) return learn, Runner(cb_funcs=listify(cbs)) . . #collapse model = get_cnn_model(data, nfs) learn,run = get_runner(model, data, lr=0.4, cbs=cbfs) . . #collapse_show model . . Sequential( (0): Sequential( (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2)) (1): ReLU() ) (1): Sequential( (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) (2): Sequential( (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) (3): Sequential( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) (4): AdaptiveAvgPool2d(output_size=1) (5): Lambda() (6): Linear(in_features=32, out_features=10, bias=True) ) . #collapse run.fit(3, learn) . . train: [2.3160621875, tensor(0.1493, device=&#39;cuda:0&#39;)] valid: [2.276257421875, tensor(0.1650, device=&#39;cuda:0&#39;)] train: [1.86434125, tensor(0.3412, device=&#39;cuda:0&#39;)] valid: [0.696328564453125, tensor(0.7789, device=&#39;cuda:0&#39;)] train: [0.5320141015625, tensor(0.8298, device=&#39;cuda:0&#39;)] valid: [0.275061376953125, tensor(0.9157, device=&#39;cuda:0&#39;)] . Hooks . Manual insertion . Let&#39;s say we want to do some telemetry, and want the mean and standard deviation of each activations in the model. First we can do it manually like this: . Jump_to lesson 10 video . #collapse_show class SequentialModel(nn.Module): def __init__(self, *layers): super().__init__() self.layers = nn.ModuleList(layers) self.act_means = [[] for _ in layers] self.act_stds = [[] for _ in layers] def __call__(self, x): for i,l in enumerate(self.layers): x = l(x) self.act_means[i].append(x.data.mean()) self.act_stds [i].append(x.data.std ()) return x def __iter__(self): return iter(self.layers) . . #collapse model = SequentialModel(*get_cnn_layers(data, nfs)) learn,run = get_runner(model, data, lr=0.9, cbs=cbfs) . . #collapse run.fit(2, learn) . . train: [1.74529109375, tensor(0.3883, device=&#39;cuda:0&#39;)] valid: [0.45245498046875, tensor(0.8549, device=&#39;cuda:0&#39;)] train: [0.3530631640625, tensor(0.8905, device=&#39;cuda:0&#39;)] valid: [0.138875048828125, tensor(0.9587, device=&#39;cuda:0&#39;)] . Now we can have a look at the means and stds of the activations at the beginning of training. These look awful rn as we do not use any initalization technique so far here. . Means . #collapse_show for l in model.act_means: plt.plot(l) plt.legend(range(6)); . . Std. Deviation . #collapse_show for l in model.act_stds: plt.plot(l) plt.legend(range(6)); . . First 10 means (of first 10 badges) . #collapse_show for l in model.act_means: plt.plot(l[:10]) plt.legend(range(6)); . . First 10 Std. deviations (of first 10 badges) . #collapse_show for l in model.act_stds: plt.plot(l[:10]) plt.legend(range(6)); . . Pytorch hooks . Hooks are PyTorch object you can add to any nn.Module. A hook will be called when a layer, it is registered to, is executed during the forward pass (forward hook) or the backward pass (backward hook). . Hooks don&#39;t require us to rewrite the model. Hooks are essentially PyTorchs Callbacks. . Jump_to lesson 10 video . #collapse model = get_cnn_model(data, nfs) learn,run = get_runner(model, data, lr=0.5, cbs=cbfs) . . #collapse_show act_means = [[] for _ in model] act_stds = [[] for _ in model] . . A hook is attached to a layer, and needs to have a function that takes three arguments: module, input, output. Here we store the mean and std of the output in the correct position of our list. . #collapse_show def append_stats(i, mod, inp, outp): act_means[i].append(outp.data.mean()) act_stds [i].append(outp.data.std()) . . #collapse_show for i,m in enumerate(model): m.register_forward_hook(partial(append_stats, i)) . . #collapse run.fit(1, learn) . . train: [1.57182515625, tensor(0.4757, device=&#39;cuda:0&#39;)] valid: [0.3583569580078125, tensor(0.8997, device=&#39;cuda:0&#39;)] . #collapse_show for o in act_means: plt.plot(o) plt.legend(range(5)); . . Hook class . We can refactor this in a Hook class. It&#39;s very important to remove the hooks when they are deleted, otherwise there will be references kept and the memory won&#39;t be properly released when your model is deleted. . Jump_to lesson 10 video . #collapse_show def children(m): return list(m.children()) class Hook(): def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self)) def remove(self): self.hook.remove() def __del__(self): self.remove() def append_stats(hook, mod, inp, outp): if not hasattr(hook,&#39;stats&#39;): hook.stats = ([],[]) means,stds = hook.stats means.append(outp.data.mean()) stds .append(outp.data.std()) . . NB: In fastai we use a bool param to choose whether to make it a forward or backward hook. In the above version we&#39;re only supporting forward hooks. . #collapse model = get_cnn_model(data, nfs) learn,run = get_runner(model, data, lr=0.5, cbs=cbfs) . . #collapse_show hooks = [Hook(l, append_stats) for l in children(model[:4])] . . #collapse run.fit(1, learn) . . train: [2.12536453125, tensor(0.2445, device=&#39;cuda:0&#39;)] valid: [1.049569140625, tensor(0.6825, device=&#39;cuda:0&#39;)] . #collapse_show for h in hooks: plt.plot(h.stats[0]) h.remove() plt.legend(range(4)); . . A Hooks class . Let&#39;s design our own class that can contain a list of objects. It will behave a bit like a numpy array in the sense that we can index into it via: . a single index | a slice (like 1:5) | a list of indices | a mask of indices ([True,False,False,True,...]) | . The __iter__ method is there to be able to do things like for x in .... . Jump_to lesson 10 video . #collapse_show class ListContainer(): def __init__(self, items): self.items = listify(items) def __getitem__(self, idx): if isinstance(idx, (int,slice)): return self.items[idx] if isinstance(idx[0],bool): assert len(idx)==len(self) # bool mask return [o for m,o in zip(idx,self.items) if m] return [self.items[i] for i in idx] def __len__(self): return len(self.items) def __iter__(self): return iter(self.items) def __setitem__(self, i, o): self.items[i] = o def __delitem__(self, i): del(self.items[i]) def __repr__(self): res = f&#39;{self.__class__.__name__} ({len(self)} items) n{self.items[:10]}&#39; if len(self)&gt;10: res = res[:-1]+ &#39;...]&#39; return res . . #collapse_show ListContainer(range(10)) . . ListContainer (10 items) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . #collapse ListContainer(range(100)) . . ListContainer (100 items) [0, 1, 2, 3, 4, 5, 6, 7, 8, 9...] . #collapse_show t = ListContainer(range(10)) t[[1,2]], t[[False]*8 + [True,False]] . . ([1, 2], [8]) . We can use it to write a Hooks class that contains several hooks. We will also use it in the next notebook as a container for our objects in the data block API. We add &#39;del&#39; medthod so a hook is removed when it is not used anymore to free up memory. . #collapse_show from torch.nn import init class Hooks(ListContainer): def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms]) def __enter__(self, *args): return self def __exit__ (self, *args): self.remove() def __del__(self): self.remove() def __delitem__(self, i): self[i].remove() super().__delitem__(i) def remove(self): for h in self: h.remove() . . #collapse model = get_cnn_model(data, nfs).cuda() learn,run = get_runner(model, data, lr=0.9, cbs=cbfs) . . #collapse hooks = Hooks(model, append_stats) hooks . . Hooks (7 items) [&lt;__main__.Hook object at 0x7f5af1361390&gt;, &lt;__main__.Hook object at 0x7f5af1361780&gt;, &lt;__main__.Hook object at 0x7f5af1361748&gt;, &lt;__main__.Hook object at 0x7f5af1294198&gt;, &lt;__main__.Hook object at 0x7f5af1294e10&gt;, &lt;__main__.Hook object at 0x7f5af1294748&gt;, &lt;__main__.Hook object at 0x7f5af1294b38&gt;] . #collapse hooks.remove() . . #collapse x,y = next(iter(data.train_dl)) x = mnist_resize(x).cuda() . . #collapse x.mean(),x.std() . . (tensor(0.0050, device=&#39;cuda:0&#39;), tensor(1.0050, device=&#39;cuda:0&#39;)) . #collapse p = model[0](x) p.mean(),p.std() . . (tensor(0.1675, device=&#39;cuda:0&#39;, grad_fn=&lt;MeanBackward0&gt;), tensor(0.3281, device=&#39;cuda:0&#39;, grad_fn=&lt;StdBackward0&gt;)) . #collapse for l in model: if isinstance(l, nn.Sequential): init.kaiming_normal_(l[0].weight) l[0].bias.data.zero_() . . #collapse p = model[0](x) p.mean(),p.std() . . (tensor(0.5189, device=&#39;cuda:0&#39;, grad_fn=&lt;MeanBackward0&gt;), tensor(1.1330, device=&#39;cuda:0&#39;, grad_fn=&lt;StdBackward0&gt;)) . Having given an __enter__ and __exit__ method to our Hooks class, we can use it as a context manager. This makes sure that onces we are out of the with block, all the hooks have been removed and aren&#39;t there to pollute our memory. . #collapse_show with Hooks(model, append_stats) as hooks: run.fit(2, learn) fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks: ms,ss = h.stats ax0.plot(ms[:10]) ax1.plot(ss[:10]) plt.legend(range(6)); fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks: ms,ss = h.stats ax0.plot(ms) ax1.plot(ss) plt.legend(range(6)); . . train: [1.9525890625, tensor(0.3193, device=&#39;cuda:0&#39;)] valid: [1.21211796875, tensor(0.6425, device=&#39;cuda:0&#39;)] train: [0.4085144140625, tensor(0.8712, device=&#39;cuda:0&#39;)] valid: [0.17522093505859376, tensor(0.9452, device=&#39;cuda:0&#39;)] . Other statistics . Let&#39;s store more than the means and stds and plot histograms of our activations now. . Jump_to lesson 10 video . #collapse_show def append_stats(hook, mod, inp, outp): if not hasattr(hook,&#39;stats&#39;): hook.stats = ([],[],[]) means,stds,hists = hook.stats means.append(outp.data.mean().cpu()) stds .append(outp.data.std().cpu()) hists.append(outp.data.cpu().histc(40,0,10)) #histc isn&#39;t implemented on the GPU . . #collapse model = get_cnn_model(data, nfs).cuda() learn,run = get_runner(model, data, lr=0.9, cbs=cbfs) . . #collapse_show for l in model: if isinstance(l, nn.Sequential): init.kaiming_normal_(l[0].weight) l[0].bias.data.zero_() . . #collapse_show with Hooks(model, append_stats) as hooks: run.fit(1, learn) . . train: [2.34095421875, tensor(0.1501, device=&#39;cuda:0&#39;)] valid: [2.290508203125, tensor(0.1028, device=&#39;cuda:0&#39;)] . #collapse_show # Thanks to @ste for initial version of histgram plotting code def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p() . . Jump_to lesson 10 video . #collapse_show fig,axes = plt.subplots(2,2, figsize=(15,6)) for ax,h in zip(axes.flatten(), hooks[:4]): ax.imshow(get_hist(h), origin=&#39;lower&#39;) ax.axis(&#39;off&#39;) plt.tight_layout() . . From the histograms, we can easily get more informations like the min or max of the activations . #collapse_show def get_min(h): h1 = torch.stack(h.stats[2]).t().float() return h1[:2].sum(0)/h1.sum(0) . . #collapse_show fig,axes = plt.subplots(2,2, figsize=(15,6)) for ax,h in zip(axes.flatten(), hooks[:4]): ax.plot(get_min(h)) ax.set_ylim(0,1) plt.tight_layout() . . Generalized ReLU . Now let&#39;s use our model with a generalized ReLU that can be shifted and with maximum value. . Jump_to lesson 10 video . #collapse_show def get_cnn_layers(data, nfs, layer, **kwargs): nfs = [1] + nfs return [layer(nfs[i], nfs[i+1], 5 if i==0 else 3, **kwargs) for i in range(len(nfs)-1)] + [ nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)] def conv_layer(ni, nf, ks=3, stride=2, **kwargs): return nn.Sequential( nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), GeneralRelu(**kwargs)) class GeneralRelu(nn.Module): def __init__(self, leak=None, sub=None, maxv=None): super().__init__() self.leak,self.sub,self.maxv = leak,sub,maxv def forward(self, x): x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x) if self.sub is not None: x.sub_(self.sub) if self.maxv is not None: x.clamp_max_(self.maxv) return x def init_cnn(m, uniform=False): f = init.kaiming_uniform_ if uniform else init.kaiming_normal_ for l in m: if isinstance(l, nn.Sequential): f(l[0].weight, a=0.1) l[0].bias.data.zero_() def get_cnn_model(data, nfs, layer, **kwargs): return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs)) . . #collapse_show def append_stats(hook, mod, inp, outp): if not hasattr(hook,&#39;stats&#39;): hook.stats = ([],[],[]) means,stds,hists = hook.stats means.append(outp.data.mean().cpu()) stds .append(outp.data.std().cpu()) hists.append(outp.data.cpu().histc(40,-7,7)) . . #collapse_show model = get_cnn_model(data, nfs, conv_layer, leak=0.1, sub=0.4, maxv=6.) init_cnn(model) learn,run = get_runner(model, data, lr=0.9, cbs=cbfs) . . #collapse_show with Hooks(model, append_stats) as hooks: run.fit(1, learn) fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks: ms,ss,hi = h.stats ax0.plot(ms[:10]) ax1.plot(ss[:10]) h.remove() plt.legend(range(5)); fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4)) for h in hooks: ms,ss,hi = h.stats ax0.plot(ms) ax1.plot(ss) plt.legend(range(5)); . . train: [0.5544332421875, tensor(0.8234, device=&#39;cuda:0&#39;)] valid: [0.15540943603515625, tensor(0.9536, device=&#39;cuda:0&#39;)] . #collapse_show fig,axes = plt.subplots(2,2, figsize=(15,6)) for ax,h in zip(axes.flatten(), hooks[:4]): ax.imshow(get_hist(h), origin=&#39;lower&#39;) ax.axis(&#39;off&#39;) plt.tight_layout() . . #collapse_show def get_min(h): h1 = torch.stack(h.stats[2]).t().float() return h1[19:22].sum(0)/h1.sum(0) . . #collapse_show fig,axes = plt.subplots(2,2, figsize=(15,6)) for ax,h in zip(axes.flatten(), hooks[:4]): ax.plot(get_min(h)) ax.set_ylim(0,1) plt.tight_layout() . . Jump_to lesson 10 video . #collapse_show def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs): model = get_cnn_model(data, nfs, layer, **kwargs) init_cnn(model, uniform=uniform) return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func) . . #collapse_show sched = combine_scheds([0.5, 0.5], [sched_cos(0.2, 1.), sched_cos(1., 0.1)]) . . #collapse_show learn,run = get_learn_run(nfs, data, 1., conv_layer, cbs=cbfs+[partial(ParamScheduler,&#39;lr&#39;, sched)]) . . #collapse run.fit(8, learn) . . train: [1.31843953125, tensor(0.5775, device=&#39;cuda:0&#39;)] valid: [0.634657666015625, tensor(0.7989, device=&#39;cuda:0&#39;)] train: [0.3491707421875, tensor(0.8969, device=&#39;cuda:0&#39;)] valid: [0.718830908203125, tensor(0.7760, device=&#39;cuda:0&#39;)] train: [0.22474275390625, tensor(0.9335, device=&#39;cuda:0&#39;)] valid: [0.139433447265625, tensor(0.9587, device=&#39;cuda:0&#39;)] train: [0.24052806640625, tensor(0.9311, device=&#39;cuda:0&#39;)] valid: [0.0933504638671875, tensor(0.9738, device=&#39;cuda:0&#39;)] train: [0.087468662109375, tensor(0.9731, device=&#39;cuda:0&#39;)] valid: [0.07542120361328125, tensor(0.9769, device=&#39;cuda:0&#39;)] train: [0.05863072265625, tensor(0.9822, device=&#39;cuda:0&#39;)] valid: [0.06209371337890625, tensor(0.9817, device=&#39;cuda:0&#39;)] train: [0.0425904248046875, tensor(0.9867, device=&#39;cuda:0&#39;)] valid: [0.05970958251953125, tensor(0.9841, device=&#39;cuda:0&#39;)] train: [0.03503335205078125, tensor(0.9896, device=&#39;cuda:0&#39;)] valid: [0.05823819580078125, tensor(0.9834, device=&#39;cuda:0&#39;)] . Uniform init may provide more useful initial weights (normal distribution puts a lot of them at 0). . #collapse learn,run = get_learn_run(nfs, data, 1., conv_layer, uniform=True, cbs=cbfs+[partial(ParamScheduler,&#39;lr&#39;, sched)]) . . #collapse run.fit(8, learn) . . train: [1.212601953125, tensor(0.6339, device=&#39;cuda:0&#39;)] valid: [0.4008196044921875, tensor(0.8711, device=&#39;cuda:0&#39;)] train: [0.4163723046875, tensor(0.8766, device=&#39;cuda:0&#39;)] valid: [0.9500861328125, tensor(0.7101, device=&#39;cuda:0&#39;)] train: [0.31883013671875, tensor(0.9031, device=&#39;cuda:0&#39;)] valid: [0.14645821533203124, tensor(0.9561, device=&#39;cuda:0&#39;)] train: [0.136221357421875, tensor(0.9582, device=&#39;cuda:0&#39;)] valid: [0.09891201171875, tensor(0.9717, device=&#39;cuda:0&#39;)] train: [0.0801023388671875, tensor(0.9752, device=&#39;cuda:0&#39;)] valid: [0.07433181762695312, tensor(0.9799, device=&#39;cuda:0&#39;)] train: [0.0592908984375, tensor(0.9819, device=&#39;cuda:0&#39;)] valid: [0.06670298461914062, tensor(0.9803, device=&#39;cuda:0&#39;)] train: [0.0450291015625, tensor(0.9864, device=&#39;cuda:0&#39;)] valid: [0.0643135009765625, tensor(0.9821, device=&#39;cuda:0&#39;)] train: [0.03759386962890625, tensor(0.9889, device=&#39;cuda:0&#39;)] valid: [0.06419009399414062, tensor(0.9820, device=&#39;cuda:0&#39;)] .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/09/Lesson-three-part-three.html",
            "relUrl": "/jupyter/2020/04/09/Lesson-three-part-three.html",
            "date": " • Apr 9, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Fastai Course DL from the Foundations Callbacks and __dunder__",
            "content": "Deep Learning from the Foundations Lesson 3 Part1 . . In the beginning we recap some lesson 2 concepts (Callbacks, Variants, under special methods) : . Just our imports : . #collapse %load_ext autoreload %autoreload 2 %matplotlib inline . . The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload . #collapse import torch import matplotlib.pyplot as plt . . Jump_to opening comments and overview of lesson 10 . Callbacks . Callbacks as GUI events . Jump_to lesson 10 video Imports : . #collapse import ipywidgets as widgets . . #collapse_show def f(o): print(&#39;hi&#39;) . . From the ipywidget docs: . the button widget is used to handle mouse clicks. The on_click method of the Button can be used to register function to be called when the button is clicked | . #collapse_show w = widgets.Button(description=&#39;Click me&#39;) . . #collapse w . . Now that we created this button we can pass it a function that will execute when pushing the button. It&#39;s a callback using a function pointer ! . #collapse_show w.on_click(f) . . NB: When callbacks are used in this way they are often called &quot;events&quot;. . Did you know what you can create interactive apps in Jupyter with these widgets? Here&#39;s an example from plotly: . . Creating your own callback . Jump_to lesson 10 video . #collapse from time import sleep . . We create a dummy calculation function to show this concept : . #collapse_show def slow_calculation(cb=None): res = 0 for i in range(5): res += i*i sleep(1) if cb: cb(i) return res . . #collapse_show def show_progress(epoch): print(f&quot;Awesome! We&#39;ve finished epoch {epoch}!&quot;) . . We can now use this show_progress fct to use it as a callback ! . #collapse_show slow_calculation(show_progress) . . Awesome! We&#39;ve finished epoch 0! Awesome! We&#39;ve finished epoch 1! Awesome! We&#39;ve finished epoch 2! Awesome! We&#39;ve finished epoch 3! Awesome! We&#39;ve finished epoch 4! . 30 . Lambdas and partials . Jump_to lesson 10 video . We can also define the function with Lambda and use it right away ! . #collapse_show slow_calculation(lambda o: print(f&quot;Awesome! We&#39;ve finished epoch {o}!&quot;)) . . Awesome! We&#39;ve finished epoch 0! Awesome! We&#39;ve finished epoch 1! Awesome! We&#39;ve finished epoch 2! Awesome! We&#39;ve finished epoch 3! Awesome! We&#39;ve finished epoch 4! . 30 . #collapse_show def show_progress(exclamation, epoch): print(f&quot;{exclamation}! We&#39;ve finished epoch {epoch}!&quot;) . . The above function can not be passed, because it uses 2 arguments. So we use Lambda to fix this : . #collapse_show slow_calculation(lambda o: show_progress(&quot;OK I guess&quot;, o)) . . OK I guess! We&#39;ve finished epoch 0! OK I guess! We&#39;ve finished epoch 1! OK I guess! We&#39;ve finished epoch 2! OK I guess! We&#39;ve finished epoch 3! OK I guess! We&#39;ve finished epoch 4! . 30 . It&#39;s better to do it like this, where we pass the function with the exclamation : . #collapse_show import torch.nn def make_show_progress(exclamation): # Leading &quot;_&quot; is generally understood to be &quot;private&quot; def _inner(epoch): print(f&quot;{exclamation}! We&#39;ve finished epoch {epoch}!&quot;) return _inner . . def logsumexp(x): m = x.max(-1)[0] print(&#39;m shape : {}&#39;.format(m.shape)) return m + (x-m[:,None]).exp().sum(-1).log() def log_softmax(x): return x - x.logsumexp(-1,keepdim=True) . sm_pred = [10,11] sm_pred = torch.FloatTensor(sm_pred) a = log_softmax(sm_pred) print(a) . tensor([-1.3133, -0.3133]) . def progress(epochs,cb=None): for i in range(epochs): res = torch.FloatTensor([i,i+1]) if cb : cb(i,res) return res . def show_random_loss(): def _inner(epoch,loss): print(&quot;Loss&quot;,loss) a = log_softmax(loss) print(&quot;log-soft&quot;,a) print(f&quot;The loss in epoch {epoch} is : {log_softmax(loss)}&quot;) return _inner . progress(10,show_random_loss()) . Loss tensor([0., 1.]) log-soft tensor([-1.3133, -0.3133]) The loss in epoch 0 is : tensor([-1.3133, -0.3133]) Loss tensor([1., 2.]) log-soft tensor([-1.3133, -0.3133]) The loss in epoch 1 is : tensor([-1.3133, -0.3133]) Loss tensor([2., 3.]) log-soft tensor([-1.3133, -0.3133]) The loss in epoch 2 is : tensor([-1.3133, -0.3133]) Loss tensor([3., 4.]) log-soft tensor([-1.3133, -0.3133]) The loss in epoch 3 is : tensor([-1.3133, -0.3133]) Loss tensor([4., 5.]) log-soft tensor([-1.3133, -0.3133]) The loss in epoch 4 is : tensor([-1.3133, -0.3133]) Loss tensor([5., 6.]) log-soft tensor([-1.3133, -0.3133]) The loss in epoch 5 is : tensor([-1.3133, -0.3133]) Loss tensor([6., 7.]) log-soft tensor([-1.3133, -0.3133]) The loss in epoch 6 is : tensor([-1.3133, -0.3133]) Loss tensor([7., 8.]) log-soft tensor([-1.3133, -0.3133]) The loss in epoch 7 is : tensor([-1.3133, -0.3133]) Loss tensor([8., 9.]) log-soft tensor([-1.3133, -0.3133]) The loss in epoch 8 is : tensor([-1.3133, -0.3133]) Loss tensor([ 9., 10.]) log-soft tensor([-1.3133, -0.3133]) The loss in epoch 9 is : tensor([-1.3133, -0.3133]) . tensor([ 9., 10.]) . slow_calculation(show_random_loss(log_softmax(sm_pred))) . The loss in epoch 0 is : tensor([-435., 0., -430.]) The loss in epoch 1 is : tensor([-435., 0., -430.]) The loss in epoch 2 is : tensor([-435., 0., -430.]) The loss in epoch 3 is : tensor([-435., 0., -430.]) The loss in epoch 4 is : tensor([-435., 0., -430.]) . 30 . #collapse_show slow_calculation(make_show_progress(&quot;Nice!&quot;)) . . Nice!! We&#39;ve finished epoch 0! Nice!! We&#39;ve finished epoch 1! Nice!! We&#39;ve finished epoch 2! Nice!! We&#39;ve finished epoch 3! Nice!! We&#39;ve finished epoch 4! . 30 . Obviously we can also do it like this with f2 containing the closure : . #collapse_show f2 = make_show_progress(&quot;Terrific&quot;) . . #collapse slow_calculation(f2) . . Terrific! We&#39;ve finished epoch 0! Terrific! We&#39;ve finished epoch 1! Terrific! We&#39;ve finished epoch 2! Terrific! We&#39;ve finished epoch 3! Terrific! We&#39;ve finished epoch 4! . 30 . #collapse slow_calculation(make_show_progress(&quot;Amazing&quot;)) . . Amazing! We&#39;ve finished epoch 0! Amazing! We&#39;ve finished epoch 1! Amazing! We&#39;ve finished epoch 2! Amazing! We&#39;ve finished epoch 3! Amazing! We&#39;ve finished epoch 4! . 30 . #collapse from functools import partial . . We can also use partial to use the argmuent passed always as given. This means we can only pass the epoch argument now. We can also store it in f2 again like before. . #collapse_show slow_calculation(partial(show_progress, &quot;OK I guess&quot;)) . . OK I guess! We&#39;ve finished epoch 0! OK I guess! We&#39;ve finished epoch 1! OK I guess! We&#39;ve finished epoch 2! OK I guess! We&#39;ve finished epoch 3! OK I guess! We&#39;ve finished epoch 4! . 30 . #collapse f2 = partial(show_progress, &quot;OK I guess&quot;) . . Callbacks as callable classes . Jump_to lesson 10 video . Most of the time we want to use a class, so we store our exclamation and epoch args in class attributes : . #collapse_show class ProgressShowingCallback(): def __init__(self, exclamation=&quot;Awesome&quot;): self.exclamation = exclamation def __call__(self, epoch): print(f&quot;{self.exclamation}! We&#39;ve finished epoch {epoch}!&quot;) . . #collapse_show cb = ProgressShowingCallback(&quot;Just super&quot;) . . call is a magic name, will be called when you take an object and treat it like a function : . #collapse_show cb(&quot;hi&quot;) . . Just super! We&#39;ve finished epoch hi! . #collapse slow_calculation(cb) . . Just super! We&#39;ve finished epoch 0! Just super! We&#39;ve finished epoch 1! Just super! We&#39;ve finished epoch 2! Just super! We&#39;ve finished epoch 3! Just super! We&#39;ve finished epoch 4! . 30 . Multiple callback funcs; *args and **kwargs . Jump_to lesson 10 video . All the things that are positional arguments end up in a tuple (args) and all the keyword arguments (kwargs) are stored as a dict. This is used to wrap other classes/objects, **kwargs can be passed off to the subclasses for example. . #collapse_show def f(*args, **kwargs): print(f&quot;args: {args}; kwargs: {kwargs}&quot;) . . #collapse_show f(3, &#39;a&#39;, thing1=&quot;hello&quot;) . . args: (3, &#39;a&#39;); kwargs: {&#39;thing1&#39;: &#39;hello&#39;} . NB: We&#39;ve been guilty of over-using kwargs in fastai - it&#39;s very convenient for the developer, but is annoying for the end-user unless care is taken to ensure docs show all kwargs too. kwargs can also hide bugs (because it might not tell you about a typo in a param name). In R there&#39;s a very similar issue (R uses ... for the same thing), and matplotlib uses kwargs a lot too. . Let&#39;s go back to our function from the start, adding a callback before and after the calculation. This is a good use for args and kwargs, due to it&#39;s flexibility : . #collapse_show def slow_calculation(cb=None): res = 0 for i in range(5): if cb: cb.before_calc(i) res += i*i sleep(1) if cb: cb.after_calc(i, val=res) return res . . #collapse_show class PrintStepCallback(): def __init__(self): pass #when removing args and kwargs here it won&#39;t work def before_calc(self, *args, **kwargs): print(f&quot;About to start&quot;) def after_calc (self, *args, **kwargs): print(f&quot;Done step&quot;) . . #collapse_show slow_calculation(PrintStepCallback()) . . About to start Done step About to start Done step About to start Done step About to start Done step About to start Done step . 30 . We can now use this with epoch and val to print our details : . #collapse_show class PrintStatusCallback(): def __init__(self): pass def before_calc(self, epoch, **kwargs): print(f&quot;About to start: {epoch}&quot;) def after_calc (self, epoch, val, **kwargs): print(f&quot;After {epoch}: {val}&quot;) . . #collapse_show slow_calculation(PrintStatusCallback()) . . About to start: 0 After 0: 0 About to start: 1 After 1: 1 About to start: 2 After 2: 5 About to start: 3 After 3: 14 About to start: 4 After 4: 30 . 30 . Modifying behavior . Jump_to lesson 10 video . We can now use this to implement early stopping. . #collapse_show def slow_calculation(cb=None): res = 0 for i in range(5): if cb and hasattr(cb,&#39;before_calc&#39;): cb.before_calc(i) res += i*i sleep(1) if cb and hasattr(cb,&#39;after_calc&#39;): if cb.after_calc(i, res): print(&quot;stopping early&quot;) break return res . . #collapse_show class PrintAfterCallback(): def after_calc (self, epoch, val): print(f&quot;After {epoch}: {val}&quot;) if val&gt;10: return True . . #collapse_show slow_calculation(PrintAfterCallback()) . . After 0: 0 After 1: 1 After 2: 5 After 3: 14 stopping early . 14 . We can now use implement a class, this will allow our callback to modify the values inside the class. . #collapse_show class SlowCalculator(): def __init__(self, cb=None): self.cb,self.res = cb,0 def callback(self, cb_name, *args): if not self.cb: return cb = getattr(self.cb,cb_name, None) if cb: return cb(self, *args) def calc(self): for i in range(5): self.callback(&#39;before_calc&#39;, i) self.res += i*i sleep(1) if self.callback(&#39;after_calc&#39;, i): print(&quot;stopping early&quot;) break . . Using dunder method __call__ we can do it like this : . #collapse_show class SlowCalculator(): def __init__(self, cb=None): self.cb,self.res = cb,0 def __call__(self, cb_name, *args): if not self.cb: return cb = getattr(self.cb,cb_name, None) if cb: return cb(self, *args) def calc(self): for i in range(5): self.callback(&#39;before_calc&#39;, i) self.res += i*i sleep(1) if self(&#39;after_calc&#39;, i): print(&quot;stopping early&quot;) break . . #collapse_show class ModifyingCallback(): def after_calc (self, calc, epoch): print(f&quot;After {epoch}: {calc.res}&quot;) if calc.res&gt;10: return True if calc.res&lt;3: calc.res = calc.res*2 . . #collapse_show calculator = SlowCalculator(ModifyingCallback()) calculator.calc() calculator.res . . After 0: 0 After 1: 1 After 2: 6 After 3: 15 stopping early . 15 . __dunder__ thingies . Anything that looks like __this__ is, in some way, special. Python, or some library, can define some functions that they will call at certain documented times. For instance, when your class is setting up a new object, python will call __init__. These are defined as part of the python data model. . For instance, if python sees +, then it will call the special method __add__. If you try to display an object in Jupyter (or lots of other places in Python) it will call __repr__. . Jump_to lesson 10 video . #collapse_show class SloppyAdder(): def __init__(self,o): self.o=o def __add__(self,b): return SloppyAdder(self.o + b.o + 0.01) def __repr__(self): return str(self.o) . . #collapse_show a = SloppyAdder(1) b = SloppyAdder(2) a+b . . 3.01 . Special methods you should probably know about (see data model link above) are: . __getitem__ | __getattr__ | __setattr__ | __del__ | __init__ | __new__ | __enter__ | __exit__ | __len__ | __repr__ | __str__ | . Variance and stuff . Variance . Variance is the average of how far away each data point is from the mean. E.g.: . Jump_to lesson 10 video . #collapse t = torch.tensor([1.,2.,4.,18]) . . #collapse m = t.mean(); m . . tensor(6.2500) . #collapse (t-m).mean() . . tensor(0.) . Oops. We can&#39;t do that. Because by definition the positives and negatives cancel out. So we can fix that in one of (at least) two ways: . #collapse (t-m).pow(2).mean() . . tensor(47.1875) . #collapse (t-m).abs().mean() . . tensor(5.8750) . But the first of these is now a totally different scale, since we squared. So let&#39;s undo that at the end. . #collapse (t-m).pow(2).mean().sqrt() . . tensor(6.8693) . They&#39;re still different. Why? . Note that we have one outlier (18). In the version where we square everything, it makes that much bigger than everything else. . (t-m).pow(2).mean() is refered to as variance. It&#39;s a measure of how spread out the data is, and is particularly sensitive to outliers. . When we take the sqrt of the variance, we get the standard deviation. Since it&#39;s on the same kind of scale as the original data, it&#39;s generally more interpretable. However, since sqrt(1)==1, it doesn&#39;t much matter which we use when talking about unit variance for initializing neural nets. . (t-m).abs().mean() is referred to as the mean absolute deviation. It isn&#39;t used nearly as much as it deserves to be, because mathematicians don&#39;t like how awkward it is to work with. But that shouldn&#39;t stop us, because we have computers and stuff. . Here&#39;s a useful thing to note about variance: . #collapse_show (t-m).pow(2).mean(), (t*t).mean() - (m*m) . . (tensor(47.1875), tensor(47.1875)) . You can see why these are equal if you want to work thru the algebra. Or not. . But, what&#39;s important here is that the latter is generally much easier to work with. In particular, you only have to track two things: the sum of the data, and the sum of squares of the data. Whereas in the first form you actually have to go thru all the data twice (once to calculate the mean, once to calculate the differences). . Let&#39;s go steal the LaTeX from Wikipedia: . $$ operatorname{E} left[X^2 right] - operatorname{E}[X]^2$$ . Covariance and correlation . Here&#39;s how Wikipedia defines covariance: . $$ operatorname{cov}(X,Y) = operatorname{E}{ big[(X - operatorname{E}[X])(Y - operatorname{E}[Y]) big]}$$ . Jump_to lesson 10 video . #collapse t . . tensor([ 1., 2., 4., 18.]) . Let&#39;s see that in code. So now we need two vectors. . #collapse # `u` is twice `t`, plus a bit of randomness u = t*2 u *= torch.randn_like(t)/10+0.95 plt.scatter(t, u); . . #collapse prod = (t-t.mean())*(u-u.mean()); prod . . tensor([ 55.9552, 41.3348, 9.6900, 290.1150]) . #collapse prod.mean() . . tensor(99.2737) . #collapse v = torch.randn_like(t) plt.scatter(t, v); . . #collapse ((t-t.mean())*(v-v.mean())).mean() . . tensor(1.0830) . It&#39;s generally more conveniently defined like so: . $$ operatorname{E} left[X Y right] - operatorname{E} left[X right] operatorname{E} left[Y right]$$ . #collapse cov = (t*v).mean() - t.mean()*v.mean(); cov . . tensor(1.0830) . From now on, you&#39;re not allowed to look at an equation (or especially type it in LaTeX) without also typing it in Python and actually calculating some values. Ideally, you should also plot some values. . Finally, here is the Pearson correlation coefficient: . $$ rho_{X,Y}= frac{ operatorname{cov}(X,Y)}{ sigma_X sigma_Y}$$ . #collapse_show cov / (t.std() * v.std()) . . tensor(0.1165) . It&#39;s just a scaled version of the same thing. Question: Why is it scaled by standard deviation, and not by variance or mean or something else? . Softmax . Here&#39;s our final logsoftmax definition: . Jump_to lesson 10 video . #collapse_show def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log() . . which is: . $$ hbox{logsoftmax(x)}_{i} = x_{i} - log sum_{j} e^{x_{j}}$$ . And our cross entropy loss is: $$- log(p_{i})$$ . Softmax is only a good idea, when our data (e.g. image) has only one (at least one) example, due to the highest output being far higher in Softmax due to the e function. In these cases binomial : $e^{x}/(1+e^{x})$ . Browsing source code . Jump_to lesson 10 video . Jump to tag/symbol by with (with completions) | Jump to current tag | Jump to library tags | Go back | Search | Outlining / folding | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/07/Callbacks-dunder.html",
            "relUrl": "/jupyter/2020/04/07/Callbacks-dunder.html",
            "date": " • Apr 7, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Delving Deep into Rectifiers a summary",
            "content": "Delving Deep into Rectifiers: . Surpassing Human-Level Performance on ImageNet Classification a summary of the paper . What did the authors want to achieve ? . Achieve above human level performance on vision (ImageNet) | Train deep Networks with rectifier functions (ReLu,LReLu,PReLU) effectively | Improve accuracy by enabling the training of deeper and larger networks | . Key elements . The key elements are a different kind of rectifier activation function called PReLu, which is very similar to LReLu as well as a different initalization technique called Kaiming/He init which improves upon the fact that Xavier initalization does not consider the non-linearities of ReLu kind functions | . PReLU . . As we can see PReLu looks a lot like LReLu, having a negative slope a when x &lt; 0, however this slope is not fixed in the beginning but learned by introducing a few hyperparameters | Due to the number of extra hyperparams being equal to the number of channels, no additional risk of overfitting is introduced | PReLu seems to keep more information in early layers and becomes more discriminative in deeper stages due to being able to model more non-linear functions | . Kaiming Initalization . The problem with Xavier init, is that it does not take into account the non-linearities of rectifier funcions, therefore a new init technique is derived by taking these activation functions into account, for the forward pass the following is derived : . Based on the response of a conv layer, which is computed by : $y_{l} = W_{l}*x_{l}+b_{l}$ ,with $x$ being a $ n = k^{2}*c$ vector ($k*k$ co-located pixels, in $c$ channels) and $W_{l}$ being a $d$ x $n$ matrix, where $d$ is the num of filters . | The elements in $W_{l}$ and $x_{l}$ are assumed to be independent from each other and share the same distribution, $W_{l}$ and $x_{l}$ are also independet from each other it follows : $Var[y_{l}] = n_{l} *Var[w_{l}*x_{l}] $ . | We let $w_{l}$ have zero mean, the variance of the product of independent variables gives us : $Var[y_{l}] = n_{l} *Var[w_{l}]*Var[x_{l}] $ , which leads to $Var[y_{l}] = n_{l} *Var[w_{l}]*E[x_{l}^{2}] $ . | $E[x_{l}^{2}]$ is the expectation of the square of $x_{l}$, we notice that $E[x_{l}^{2}] neq Var[x_{l}]$ unless $x_{l}$ has 0 mean (Random variability) , which is not the case for ReLu : $x_{l} = max(0,y_{l-1})$ . | if $w_{l-1}$ is symmetric around 0 and $b_{l-1}=0$, it follows that $y_{l-1}$ is a symmetric distribution around zero. This means that $E[x_{l}^{2}]=0.5 * Var[y_{l-1}]$ when the activation is ReLu thus : $Var[y_{l}] = 0.5 * n_{l} *Var[w_{l}]*Var[y_{l-1}] $ . | when we have L layers we have : . $Var[y_{l}] = Var[y_{1}] * prod^{L}_{l=2} (0.5 * n_{l} *Var[w_{l}])$ . | the initalization should not magnify the magnitude of the inputs signals, this is achieved by applying a proper scalar : . | . $0.5 * n_{l} *Var[w_{l}] = 1, forall {l}$ (ReLu case) . $0.5 *(1+a^{2}) * n_{l} *Var[w_{l}] = 1, forall {l}$ (PReLu case) . | . =&gt; this distribution is a 0-mean Gaussian with a std of $ sqrt{2/n_{l}}$, which is also adopted in the first layer . For the backward pass the same function applies, with $n_{l}=k_{l}^{2}*d_{l-1} = k_{l}^{2}*c_{l}$ replaced by $ tilde{n}=k_{l}^{2}*d_{l}$ : . $0.5 * tilde{n} *Var[w_{l}] = 1, forall {l}$ (ReLu) . $0.5 *(1+a^{2}) * tilde{n} *Var[w_{l}] = 1, forall {l}$ (PReLu case) . | . &quot;This means that if the initialization properly scales the backward signal, then this is also the case for the forward signal; and vice versa. For all models in this paper, both forms can make them converge.&quot; . Implementation Details . The standard hyperparms are as follows : Weight decay is 0.0005 | Momentum is 0.9. | Dropout (50%) is used in the first two fc layers | Minibatch size is fixed as 128 | The learning rates are 1e-2, 1e-3,and 1e-4, and is switched when the error plateaus | Number of epochs : 80 | simple variant of Krizhevsky’s method is used to run Multi-GPUs, the GPUs are synched before the first fc layer to run backprop/forward pass on one of the GPUs (3.8x speedup using 4 GPUs, and a 6.0x speedup using 8 GPUs) | . | The PReLU hyperparameters (slopes) are trained with Backprop, the authors proposed the following : no weight decay is used | the slopes ai are initialized as 0.25 | the slopes aiare not constrained, even without regularization aiis rarely larger than 1 | . | . Results and Conclusion . . PRelu reduces top-1 error by 1.05% and top-5 error by 0.23% (@scale 384), when the large model A is used | Kaiming init allows training deep rectifier networks and converges, this allows them to reduce the error to below human level 4.94% compared to 5.1%, you should check out how the human benchmark was established by checking out Andrej Karpathy&#39;s blog on this | It has to be noted however that this is largely due to the fine grained details that can be learned by NNs, if a prediction is incorrect humans still mostly guess the right category (for example vehicle) while NNs can be completely off. So superhuman performance is only achieved in detecting fine grained classes. This can be confirmed when training on the Pascal VOC dataset. | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/jupyter/2020/04/05/Delving-Deep-into-rectifiers.html",
            "relUrl": "/jupyter/2020/04/05/Delving-Deep-into-rectifiers.html",
            "date": " • Apr 5, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Dense Net",
            "content": "Dense Net paper summary . . What did they try to accomplish ? . improve CNNs : fight the vanishing gradient problem, improve regularization, remove redundancy (redundant layers/neurons) existing in current CNNs (like ResNets), which in turn recudes the number of parameters | . Key elements . Concat and Dense conectivity . concatenating feature maps, instead of using the classic ResNet skip connection function : . . . =&gt; lth layer is used as input to (l+1)th layer =&gt; xl = Hl(xl-1) . Dense Nets concatenate feature maps of the same size, which means it has L*(L+1)/(2) connections instead of L in a normal network, where L is the number of layers. Consequently every Dense Net layer has access to the feature maps of the preceeding layers : | . . The activation function Hl is a composite function with 3x3 convolutions, Batch Norm and ReLu activations. . Pooling /Transition Layers . When the size of feature maps changes, concatenation is not viable. The network is divided into several Dense Blocks, in between those 2x2 average pooling with 1x1 conv filters and batch norm are applied forming a “transition layer”. . Growth rate . The growth rate k is a hyperparameter which regulates how much a layer contributes to the global state. If each composite function Hl produces k feature maps, the lth layer has k0 + k * (l-1) feature-maps, where k0 is the number of channels in the input layer. It has to be noted that DenseNets use narrow layers, with k=12. . Bottleneck layers . To reduce the amount of input channels (for compute efficiency), bottleneck layers are used with 1x1 convs before the 3x3 convs applied. . Compression . Compression is used to reduce the number of feature maps at transition layers, if a dense block contains m feature maps, the transition layer will generate a*m feature maps,where 0 &lt; a &lt;= 1 with a = 0.5 in most cases. . Implementation Details . Kaiming/He init. is used | Zero padding is used @ each Dense block | Global pooling after last Dense block, with Softmax activation | 3 Dense blocks are used with all datasets except for ImageNet | Weight decay of 10e-4 | Nesterov momentum of 0.9 | ImageNet implementation uses 7x7 convs instead of 3x3 | . Results and Conclusion . . Bottleneck impact decreases with depth of the network | not the same regularization issues as with ResNets1 | Dense Net BC with 15.3 Million params outperforms much larger Fractal Net (comparable to ResNet-1001), with DenseNet having 90% fewer parameters | a DenseNet with as much compute complexity (FLOPS) as ResNet-50 performs on par with ResNet-101 | DenseNet with 0.8 Million parameters performs as good as ResNet with 10.2 Millon parameters | Deep Supervision is achieved with a single classifier. This provides easier loss functions and doesn’t need a multi classifier (like Inception). | The intuition behind the good performance of DenseNets : architecture style is similar to a ResNet trained with stochastic depth, that means redundant layers are dropped from the beginning allowing smaller Networks | . References that are interesting to follow . DenseNets Implentation Github | ResNets paper | Fractal Nets paper | Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/04/Dense-Net.html",
            "relUrl": "/markdown/2020/04/04/Dense-Net.html",
            "date": " • Apr 4, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Fastai DL from the Foundations Softmax, NLL, Training, Module, Dataloader, Sampler, Callbacks",
            "content": "Lesson 2 How to train your model . Loss . For our Classification task, we will be using Cross Entropy Loss (also called Log Loss) ourselves. We define a simple Linear Model for our task. We will be using the Pytorch functions, as we already implemented those in Lesson 1 : . class Model(nn.Module): def __init__(self,in_dim,nh,out_dim): super().__init__() self.layers = [nn.Linear(in_dim,nh), nn.ReLu(),nn.Linear(nh,out_dim)] def __call__(self,x): for i in self.layers: x = l(x) return x . Since we are using Softmax, we need to compute it’s output first : . In practice we need it’s log, the code is simple : . def log_softmax(x) : return (x.exp()/x.exp().sum(-1,keepdim=True))).log() . Using simple log-math . log(a/b) = log(a) - log(b) . Which leads to in pseudo code : . log(x.exp()) - log(x.exp().sum()) . We can simplify this log_softmax function, like so : . def log_softmax(x) : return x - x.exp().sum(-1,keepdim=True).log() . . Using numpy integer array indexing we can compute our negative log likelihood like so by passing our softmax output : . def negative_log_likelihood(input,target): return -input[range(target.shape[0]), target].mean() . However we can compute the log of the sum of exponentials in a more stable way to avoid an overflow of big activations, with the LogSumExp trick : . def logsumexp(exp): a= x.max(-1)[0] #maximum of x(j) return a + (x-a[:,None]).exp().sum(-1).log() . Updating our softmax again, this leads to : . def log_softmax(x): return x - x.logsumexp(-1,keepdim=True) . x.logsumexp() is the Pytorch function in this case. In order to compare our function with Pytorch, we can use . test_near(logsumexp(pred), pred.logsumexp(-1)) . test_near will throw an AssertionError if they are not equal to each other. . Now we succesfully implemented F.cross_entropy(pred,y_train), which is made out of F.log_softmax and F.nll_loss . The accuracy can be calculated with : . def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean() . Training Loop . Basically the training loop repeats over the following steps: . get the output of the model on a batch of inputs | compare the output to the labels we have and compute a loss | calculate the gradients of the loss with respect to every parameter of the model | update said parameters with those gradients to make them a little bit better | . Now we can implement our Training Loop : . for epoch in range(epochs): for i in range((n-1)//bs + 1): # slice dataset in batches start_i = i*bs end_i = start_i+bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] loss = loss_func(model(xb), yb) loss.backward() with torch.no_grad(): for l in model.layers: if hasattr(l, &#39;weight&#39;): l.weight -= l.weight.grad * lr l.bias -= l.bias.grad * lr l.weight.grad.zero_() l.bias .grad.zero_() . This looks kind of messy. Since our parameters can all be stored in a model class, we can loop over them and update them easily. However we need to implement a dummy Module first : . Module and improved training loop . class DummyModule(): def __init__(self, n_in, nh, n_out): self._modules = {} self.l1 = nn.Linear(n_in,nh) self.l2 = nn.Linear(nh,n_out) def __setattr__(self,k,v): #this is called everytime self is assigned if not k.startswith(&quot;_&quot;): self._modules[k] = v # just checks if it doesn&#39;t start with _ to avoid calling python _modules recursively super().__setattr__(k,v) #super class is python object def __repr__(self): return f&#39;{self._modules}&#39; def parameters(self): for l in self._modules.values(): for p in l.parameters(): yield p . for simplicity we can now use the Pytorch Module . class Model(nn.Module): def __init__(self,layers): super().__init__() #initalizes self._modules self.layers = layers for i,l in enumerate(self.layers) : self.add_module(f&#39;layer_{i}&#39;,l) def __call__(): for layer in self.layers: x = l(x) return x . now we can call the training more conveniently : . for epoch in range(epochs): for i in range((n-1)//bs + 1): # slice dataset in batches start_i = i*bs end_i = start_i+bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] loss = loss_func(model(xb), yb) loss.backward() with torch.no_grad() for param in model.parameters(): param -= lr * param.grad . We can make it even easier with nn.ModuleList to recreate nn.Sequential. . class SequentialModel(nn.Module): def __init__(self, layers): super().__init__() self.layers = nn.ModuleList(layers) def __call__(self, x): for l in self.layers : x = l(x) return x . Let’s replace our previous manually coded optimization step: . with torch.no_grad(): for p in model.parameters(): p -= p.grad * lr model.zero_grad() . and instead use just: . opt.step() opt.zero_grad() . Optimizer . By creating our own Optimizer Function . class Optimizer(): def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr def step(self): with torch.no_grad(): for p in self.params: p -= p.grad * lr def zero_grad(self): for p in self.params: p.grad.data.zero_() . PyTorch already provides optimizers, like optim.SGD and optim.Adam, which also handles more stuff. . Now we can further simplify our Training loop : . for epoch in range(epochs): for i in range((n-1)//bs + 1): start_i = i*bs end_i = start_i+bs xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() . When implementing stuff yourself, it’s always good to put some tests in. Like checking the accuracy for example. . Dataset and DataLoader . Dataset . We can further simplify this by converting . xb = x_train[start_i:end_i] yb = y_train[start_i:end_i] . with a Dataset Class : . class Dataset(): def __init__(self,x,y): self.x,self.y = x,y def __len__(self): return len(self.x) def __getitem__(self,i): return self.x[i], self.y[i] train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid) . Now we can call our loop like this : . for e in range(epochs): for i in range((n-1)//bs+1): x,y = train_ds[i*bs:i*bs+bs] pred = model(x) loss = loss_func(pred,y) loss.backward() opt.step() opt.zero_grad() . DataLoader . We can make this even easier with a Dataloader to iterate over our Dataset automatically. . class Dataloader : def __init__(self, ds, bs): self.ds,self.bs = ds,bs def __iter__(self): for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs] . yield is used, in order to produce a series of values over time. We can use the Dataloader with next and iter : . train_dl = DataLoader(train_ds, bs) valid_dl = DataLoader(valid_ds, bs) xb,yb = next(iter(valid_dl)) . now we can put our train loop in a wonderful function : . def fit(): for epoch in range(epochs): for xb,yb in train_dl: pred = model(xb) loss = loss_func(pred, yb) loss.backward() opt.step() opt.zero_grad() . Sampler . In order to have a random order, we can implement a Sampler class : . class Sampler(): def __init__(self, ds, bs, shuffle=False): self.n,self.bs,self.shuffle = len(ds),bs,shuffle def __iter__(self): self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n) for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs] . Now we can change our DataLoader to include our sampler as an argument. . def collate(b): xs,ys = zip(*b) return torch.stack(xs),torch.stack(ys) class DataLoader(): def __init__(self, ds, sampler, collate_fn=collate): self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn def __iter__(self): for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s]) . collate stacks tensors, we can also add padding, etc … PyTorch does the same thing as well, but also adds num_workers which can be used to start several threads and run more efficiently : . train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate,num_workers=num_workers) valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate,num_workers=num_workers) . With Val Set . Using best practices we should add a val set to store our best model and test during training : . def fit(epochs, model, loss_func, opt, train_dl, valid_dl): for epoch in range(epochs): # Handle batchnorm / dropout model.train() # print(model.training) for xb,yb in train_dl: loss = loss_func(model(xb), yb) loss.backward() opt.step() opt.zero_grad() model.eval() # print(model.training) with torch.no_grad(): tot_loss,tot_acc = 0.,0. for xb,yb in valid_dl: pred = model(xb) tot_loss += loss_func(pred, yb) tot_acc += accuracy (pred,yb) nv = len(valid_dl) print(epoch, tot_loss/nv, tot_acc/nv) return tot_loss/nv, tot_acc/nv . I divided it in to a train and val Function for better readability : . def train(model, loss_func, opt, train_dl): model.train() for xb,yb in train_dl : loss = loss_func(model(xb),yb) loss.backward() opt.step() opt.zero_grad() def val(epoch,model, loss_func, valid_dl): model.eval() with torch.no_grad(): total_loss, total_acc = 0.,0. for xb,yb in valid_dl: total_loss += loss_func(model(xb),yb) total_acc += accuracy(model(xb),yb) iterations = len(valid_dl) print(epoch, total_loss/iterations, total_acc/iterations) return total_loss, total_acc, iterations def fit(epochs,model,loss_func,opt,train_dl,valid_dl): for epoch in range(epochs) : train(model,loss_func,opt,train_dl) loss,acc, nv = val(epoch,model,loss_func,valid_dl) return loss/(nv), acc/(nv) . Powerful Training Loops with Callbacks . In order to customize out training loop in many ways (regularization techniques, visualization, early stopping,…), we want to be able to do so easily without having to write a huge loop function all the time that is hard to read and update. For this fastai uses something called callbacks : . . Databunch . This can be used to store our info and doesn’t have any real logic. . class DataBunch(): def __init__(self, train_dl, valid_dl, c=None): self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c @property def train_ds(self): return self.train_dl.dataset @property def valid_ds(self): return self.valid_dl.dataset data = DataBunch(*get_dls(train_ds, valid_ds, bs), c) #c is max y value #further storage wrappers def get_model(data, lr=0.5, nh=50): m = data.train_ds.x.shape[1] model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c)) return model, optim.SGD(model.parameters(), lr=lr) class Learner(): def __init__(self, model, opt, loss_func, data): self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data . With our previous training loop modified a bit we can add Callbacks now, : . def one_batch(xb, yb, cb): if not cb.begin_batch(xb,yb): return loss = cb.learn.loss_func(cb.learn.model(xb), yb) if not cb.after_loss(loss): return loss.backward() if cb.after_backward(): cb.learn.opt.step() if cb.after_step(): cb.learn.opt.zero_grad() def all_batches(dl, cb): for xb,yb in dl: one_batch(xb, yb, cb) if cb.do_stop(): return def fit(epochs, learn, cb): if not cb.begin_fit(learn): return for epoch in range(epochs): if not cb.begin_epoch(epoch): continue all_batches(learn.data.train_dl, cb) if cb.begin_validate(): with torch.no_grad(): all_batches(learn.data.valid_dl, cb) if cb.do_stop() or not cb.after_epoch(): break cb.after_fit() . add Callbacks . class Callback(): def begin_fit(self, learn): self.learn = learn return True def after_fit(self): return True def begin_epoch(self, epoch): self.epoch=epoch return True def begin_validate(self): return True def after_epoch(self): return True def begin_batch(self, xb, yb): self.xb,self.yb = xb,yb return True def after_loss(self, loss): self.loss = loss return True def after_backward(self): return True def after_step(self): return True . Callback Hander . class CallbackHandler(): def __init__(self,cbs=None): self.cbs = cbs if cbs else [] def begin_fit(self, learn): self.learn,self.in_train = learn,True learn.stop = False res = True for cb in self.cbs: res = res and cb.begin_fit(learn) return res def after_fit(self): res = not self.in_train for cb in self.cbs: res = res and cb.after_fit() return res def begin_epoch(self, epoch): self.learn.model.train() self.in_train=True res = True for cb in self.cbs: res = res and cb.begin_epoch(epoch) return res def begin_validate(self): self.learn.model.eval() self.in_train=False res = True for cb in self.cbs: res = res and cb.begin_validate() return res def after_epoch(self): res = True for cb in self.cbs: res = res and cb.after_epoch() return res def begin_batch(self, xb, yb): res = True for cb in self.cbs: res = res and cb.begin_batch(xb, yb) return res def after_loss(self, loss): res = self.in_train for cb in self.cbs: res = res and cb.after_loss(loss) return res def after_backward(self): res = True for cb in self.cbs: res = res and cb.after_backward() return res def after_step(self): res = True for cb in self.cbs: res = res and cb.after_step() return res def do_stop(self): try: return self.learn.stop finally: self.learn.stop = False . Callback Test . class TestCallback(Callback): def begin_fit(self,learn): super().begin_fit(learn) self.n_iters = 0 return True def after_step(self): self.n_iters += 1 print(self.n_iters) if self.n_iters&gt;=10: self.learn.stop = True return True . These methods are checked for in our one_batch function and are executed in our training loop. . Callback Simplification . We do on not need to implement each function seperately by using a call function : . def __call__(self, cb_name): for cb in sorted(self.cbs, key=lambda x: x._order): f = getattr(cb, cb_name, None) if f and f(): return True return False . This allows is to recreate the Calbacks in a better way : . #export import re _camel_re1 = re.compile(&#39;(.)([A-Z][a-z]+)&#39;) _camel_re2 = re.compile(&#39;([a-z0-9])([A-Z])&#39;) def camel2snake(name): s1 = re.sub(_camel_re1, r&#39; 1_ 2&#39;, name) return re.sub(_camel_re2, r&#39; 1_ 2&#39;, s1).lower() class Callback(): _order=0 def set_runner(self, run): self.run=run def __getattr__(self, k): return getattr(self.run, k) @property def name(self): name = re.sub(r&#39;Callback$&#39;, &#39;&#39;, self.__class__.__name__) return camel2snake(name or &#39;callback&#39;) #export class TrainEvalCallback(Callback): def begin_fit(self): self.run.n_epochs=0. self.run.n_iter=0 def after_batch(self): if not self.in_train: return self.run.n_epochs += 1./self.iters self.run.n_iter += 1 def begin_epoch(self): self.run.n_epochs=self.epoch self.model.train() self.run.in_train=True def begin_validate(self): self.model.eval() self.run.in_train=False class TestCallback(Callback): def after_step(self): if self.train_eval.n_iters&gt;=10: return True #export class Runner(): def __init__(self, cbs=None, cb_funcs=None): cbs = listify(cbs) for cbf in listify(cb_funcs): cb = cbf() setattr(self, cb.name, cb) cbs.append(cb) self.stop,self.cbs = False,[TrainEvalCallback()]+cbs @property def opt(self): return self.learn.opt @property def model(self): return self.learn.model @property def loss_func(self): return self.learn.loss_func @property def data(self): return self.learn.data def one_batch(self, xb, yb): self.xb,self.yb = xb,yb if self(&#39;begin_batch&#39;): return self.pred = self.model(self.xb) if self(&#39;after_pred&#39;): return self.loss = self.loss_func(self.pred, self.yb) if self(&#39;after_loss&#39;) or not self.in_train: return self.loss.backward() if self(&#39;after_backward&#39;): return self.opt.step() if self(&#39;after_step&#39;): return self.opt.zero_grad() def all_batches(self, dl): self.iters = len(dl) for xb,yb in dl: if self.stop: break self.one_batch(xb, yb) self(&#39;after_batch&#39;) self.stop=False def fit(self, epochs, learn): self.epochs,self.learn = epochs,learn try: for cb in self.cbs: cb.set_runner(self) if self(&#39;begin_fit&#39;): return for epoch in range(epochs): self.epoch = epoch if not self(&#39;begin_epoch&#39;): self.all_batches(self.data.train_dl) with torch.no_grad(): if not self(&#39;begin_validate&#39;): self.all_batches(self.data.valid_dl) if self(&#39;after_epoch&#39;): break finally: self(&#39;after_fit&#39;) self.learn = None . In order to track our stats we will add a third Callback to see them, it will make us of a class that computes these. . class AvgStats(): def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train def reset(self): self.tot_loss,self.count = 0.,0 self.tot_mets = [0.] * len(self.metrics) @property def all_stats(self): return [self.tot_loss.item()] + self.tot_mets @property def avg_stats(self): return [o/self.count for o in self.all_stats] def __repr__(self): if not self.count: return &quot;&quot; return f&quot;{&#39;train&#39; if self.in_train else &#39;valid&#39;}: {self.avg_stats}&quot; def accumulate(self, run): bn = run.xb.shape[0] self.tot_loss += run.loss * bn self.count += bn for i,m in enumerate(self.metrics): self.tot_mets[i] += m(run.pred, run.yb) * bn class AvgStatsCallback(Callback): def __init__(self, metrics): self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False) def begin_epoch(self): self.train_stats.reset() self.valid_stats.reset() def after_loss(self): stats = self.train_stats if self.in_train else self.valid_stats with torch.no_grad(): stats.accumulate(self.run) def after_epoch(self): print(self.train_stats) print(self.valid_stats) . As this Callback alread tracks all our Stats, we can easily cerate a new Callback to save the best model based on validation loss at a given epoch and introduce early stopping. This can be done by inheriting from AvgStatsCallback which already has handy begin_epoch and after_loss functions that we can use. . class Early_save(AvgStatsCallback): def __init__(self, metrics,early_stopping_iter,loss_stop): self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False) self.lowest_val_loss = float(&quot;inf&quot;) self.cur_val_loss = float(&quot;inf&quot;) self.early_stopping_array = deque(maxlen=3) self.early_stopping_iter = 3 self.loss_stop = loss_stop def save_model(self): self.cur_val_loss, self.cur_val_acc = self.valid_stats.avg_stats if self.cur_val_loss &lt; self.lowest_val_loss : self.lowest_val_loss = self.cur_val_loss torch.save(learn.model.state_dict(),&quot;best_model.pt&quot;) print(&quot;Saving Model with val loss of :{:.4f}&quot;.format(self.lowest_val_loss)) def early_stopping(self): self.early_stopping_array.append(self.cur_val_loss) if (self.early_stopping_array.maxlen == len(self.early_stopping_array)): diff = 0.0 for i in range(0,len(self.early_stopping_array)-1): #check diff between losses diff += abs(self.early_stopping_array[i]-self.early_stopping_array[i+1]) diff /= (len(self.early_stopping_array)-1) if(diff &lt; self.loss_stop): return &quot;stop&quot; def after_epoch(self): print(self.train_stats) print(self.valid_stats) self.save_model() if self.early_stopping()==&quot;stop&quot;: return True print(self.valid_stats.avg_stats[1]) . It keeps track of the last 3 losses and will stop training if the loss difference is too small. It will also save a model that performs best on validation with the handy torch.save function. . The Class now also keeps track of the lowest validation loss overall and can save the best model based on validation loss. Early stopping was implemented by tracking the last n elements, with n=early_stopping_iter in this case. We are storing it in a deque data structures. The early_stopping function will return a string that will then lead to our after_epoch function returning True which will stop training, as we have : . if self(&#39;after_epoch&#39;): break . in our training loop. . Now we can finally call our methods and start trainig. . learn = Learner(*get_model(data), loss_func, data) stats = AvgStatsCallback([accuracy]) run = Runner(cbs=stats) run.fit(2, learn) loss,acc = stats.valid_stats.avg_stats . Using partial we can create a Function that can create a callback function : . from functools import partial acc_cbf = partial(AvgStatsCallback,accuracy) run = Runner(cb_funcs=acc_cbf) . This way we can create Callback funcs easily, e.g. by using a list. . Annealing . We define two new callbacks: the Recorder to save track of the loss and our scheduled learning rate, and a ParamScheduler that can schedule any hyperparameter as long as it’s registered in the state_dict of the optimizer. . class Recorder(Callback): def begin_fit(self): self.lrs,self.losses = [],[] def after_batch(self): if not self.in_train: return self.lrs.append(self.opt.param_groups[-1][&#39;lr&#39;]) self.losses.append(self.loss.detach().cpu()) def plot_lr (self): plt.plot(self.lrs) def plot_loss(self): plt.plot(self.losses) class ParamScheduler(Callback): _order=1 def __init__(self, pname, sched_func): self.pname,self.sched_func = pname,sched_func def set_param(self): for pg in self.opt.param_groups: pg[self.pname] = self.sched_func(self.n_epochs/self.epochs) def begin_batch(self): if self.in_train: self.set_param() def sched_lin(start, end): def _inner(start, end, pos): return start + pos*(end-start) return partial(_inner, start, end) def annealer(f): def _inner(start, end): return partial(f, start, end) return _inner @annealer def sched_lin(start, end, pos): return start + pos*(end-start) . The Decorator annealer passes sched_lin in annealer and replaces sched_lin() definition with what annealer returns. . other scheduler funcs . @annealer def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2 @annealer def sched_no(start, end, pos): return start @annealer def sched_exp(start, end, pos): return start * (end/start) ** pos def cos_1cycle_anneal(start, high, end): return [sched_cos(start, high), sched_cos(high, end)] #This monkey-patch is there to be able to plot tensors torch.Tensor.ndim = property(lambda x: len(x.shape)) . plot them . annealings = &quot;NO LINEAR COS EXP&quot;.split() a = torch.arange(0, 100) p = torch.linspace(0.01,1,100) fns = [sched_no, sched_lin, sched_cos, sched_exp] for fn, t in zip(fns, annealings): f = fn(2, 1e-2) plt.plot(a, [f(o) for o in p], label=t) plt.legend() . Combine Schedulers with a function : . def combine_scheds(pcts, scheds): assert sum(pcts) == 1. pcts = tensor([0] + listify(pcts)) assert torch.all(pcts &gt;= 0) pcts = torch.cumsum(pcts, 0) def _inner(pos): idx = (pos &gt;= pcts).nonzero().max() actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx]) return scheds[idx](actual_pos) return _inner sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) . Here is an example: use 30% of the budget to go from 0.3 to 0.6 following a cosine, then the last 70% of the budget to go from 0.6 to 0.2, still following a cosine. Train for a long time @ high lr, then switch to lower lr with cosine 1 cycle schedules. . Now we can train with our Callbacks and cosine scheduling. . cbfs = [Recorder, partial(AvgStatsCallback,accuracy), partial(ParamScheduler, &#39;lr&#39;, sched)] learn = create_learner(get_model_func(0.3), loss_func, data) run = Runner(cb_funcs=cbfs) run.fit(3, learn) . These Callbacks can also be used to move compute to our GPU ! .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/02/DL-From-Foundations-Part2.html",
            "relUrl": "/markdown/2020/04/02/DL-From-Foundations-Part2.html",
            "date": " • Apr 2, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Fastai DL from the Foundations Matmul, Initalization, ReLU, Backprop, MSE",
            "content": "Fastai DL from the Foundations Lesson 1 . The idea of this Repo is to manage and document the Code for the fastai Deep Learning from the foundations course and include Papers and Summaries of them when it is helpful to do so. The course focuses on building large components of the fastai library and Pytorch from scratch to allow deep understanding of the fastai and Pytorch frameworks, which enables the creation of own algorithms and makes debugging easier. . . The Code is based on the code in the fastai course (v3), check out their repo which also includes part 1 which is realy focused on the practical side of things. Thank you to Jeremy, Rachel and the fastai Team for this great course. . In order to understand the material of part 2, you should be familiar with the following concepts, depending on each category : . Fundamentals . Affine functions &amp; non-linearities | Parameters &amp; activations | Random weight initalization and transfer learning | Stochastic gradient descent, Momentum and ADAM (a combination of RMSprop and Momentum) | Regularization techniques, specifically batch norm, dropout, weight decay and data augmentation | Embeddings Vision . | Image classification and Regression Lesson 1 | Image classification and Regression Lesson 2 | Conv Nets | Residual and dense blocks | Segmentation : U-Net | GANs NLP . | Language models &amp; NLP Tabular Data . | Continious &amp; categorical variables | Collaborative filtering | . ## Lesson 1 . As we already know DL is mainly based on Linear Algebra, so let’s implement some simple Matrix Multiplication ! We already know that np.matmul can be used for this, bur let’s do it ourselves. . def matmul(a,b): ar,ac = a.shape # n_rows * n_cols br,bc = b.shape assert ac==br #check for right dimensions =&gt; output dim is ar,bc c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): for k in range(ac): # or br c[i,j] += a[i,k] * b[k,j] return c . This is a very simple and inefficient implementation, which runs in 572 ms on my CPU with matrix dimensions 5x784 multiplied by 784x10. As expected the output array has 5 rows, if we used MNIST (50k) rows onw forward pass would take more than an hour which is unacceptable. . To improve this we can pass the Code down to a lower level language (Pytorch uses ATen a Tensor library for this). This can be done with elementwise multiplication (also works on Tensors with rank &gt; 1) : . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): c[i,j] = (a[i,:] * b[:,j]).sum() #row by column return c . . This is essentially using the above formula and executing it in C code, with a runtime of : 802 µs ± 60.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each). Which is about 714 times faster than the first implementation ! Wooho we are done ! . Broadcasting . Hold on not so fast ! We can still do better by removing the inner loop with Broadcasting. Broadcasting “broadcasts” the smaller array across the larger one, so they have compatible shapes, operations are vecorized so that loops are executed in C without any overhead. You can see the broadcasted version of a vector by calling : . &lt;smaller_array&gt;.expand_as(&lt;larger_array&gt;) . after expansion you can call : . &lt;smaller_array&gt;.storage() . and you will see that no additional memory is needed. With this our matmul looks like this : . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): c[i] = (a[i ].unsqueeze(-1) * b).sum(dim=0) #unsqueeze is used to unsqueeze a to rank 2 return c . This code executes in 172 µs ± 14.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each). Which means we are 3325.81 faster than in the beginning, nice. . Einstein Summation . A compact representation to combine products and sums in a general way. . def matmul(a,b): return torch.einsum(&#39;ik,kj-&gt;ij&#39;, a, b) . This speeds up the code a little more (factor 3575 compared to the start), but more improvements can be made. . It uses this string “mini language” (sort of like regex) to specify the multiply, it is a little bit annoying and languages like Swift will hopefully allow us to get rid of this. . Pytorch Op . Pushes the code to BLAS, Hardware optimized code. We can not specify this with Pytorch, with Swift this could be optimized by the programmer more easily. A classic operation is the @ operator, it can do more than matmul (such as Batch wise, Tensor Reductions,…). . Matmul Summary . Algorithm Runtime on CPU Factor improvement . Naive Loops | 572 ms | 1 | . Loops + elementwise row/column multiply | 802 µs | 714 | . Brodacasting | 172 µs | 3326 | . Einstein Summation | 160 µs | 3575 | . Pytorch’s function (uses HW specific BLAS) | 86 µs | 6651 | . Now let’s use it to init our weights and code RELU . Init . We create a 2layer Net, with a hidden layer of size 50. . m is the 2nd dimension size of our input. . nh = 50 w1 = torch.randn(m,nh)/math.sqrt(m) b1 = torch.zeros(nh) w2 = torch.randn(nh,1)/math.sqrt(nh) b2 = torch.zeros(1) . Randn gives us weights with mean 0 and std of 1. Just using random numbers the mean and std of our output vector will be way off. In order to avoid this we divide by sqrt(m), which will keep our output mean and std in bounds. . Another common initalization method is Xavier Initalization. Check out Andrej Karpathy’s lecture (starting at about 45:00) for a good explanation Even more advanced methods like Fixup initalization can be used. The authors of the paper are able to learn deep nets (up to 10k layers) as stable without normalization when using Fixup. . Problem : if the variance halves each layer, the activation will vanish after some time, leading to dead neurons. . Due to this the 2015 ImageNet ResNet winners, see 2.2 in the paper suggested this : Up to that point init was done with random weights from Gaussian distributions, which used fixed std deviations (for example 0.01). These methods however did not allow deeper models (more than 8 layers) to converge in most cases. Due to this, in the older days models like VGG16 had to train the first 8 layers at first, in order to then initalize the next ones. As we can imagine this takes longer to train, but also may lead to a poorer local optimum. Unfortunately the Xavier init paper does not talk about non-linarities, but should not be used with ReLu like functions, as the ReLu function will half the distribution (values smaller than zero are = 0) at every step. . . Looking at the distributions in the plots, you can see that the rapid decrease of the std. deviation leads to ReLu neurons activating less and less. . The Kaiming init paper investigates the variance at each layer and ends up suggesting the following : . essentially it just adds the 2 in the numerator to avoid the halfing of the variance due at each step. . A direct comparison in the paper on a 22 layer model shows the benefit, even though Xavier converges as well, Kaiming init does so significantly faster. With a deeper 30-layer model the advantage of Kaiming is even more evident. . Kaiming init code : . w1 = torch.randn(m,nh)*math.sqrt(2/m) . ReLu can be implemented easily, it clamps values below 0 and is linear otherwise : . def relu(x): return x.clamp_min(0.) . Leaky ReLu avoids 0-ing the gradient by using a small negative slope below 0 (0.01 usually). . Therfore Kaiming init with ReLU can be implemented like this : . w1 = torch.randn(m,nh)*math.sqrt(2./m ) t1 = relu(lin(x_valid, w1, b1)) t1.mean(),t1.std() . Info . The Pytorch source code in the tutorial for torch.nn.Conv2d uses a kaiming init with : . init.kaiming_uniform_(self.weight, a=math.sqrt(5)) . .sqrt(5) was an original bug from the Lua Torch and was fixed now ! . Loss Function MSE . Now that we have done almost one forward pass,we still need to implement an error function. MSE Error, popular for regression tasks, can be implemented like this : . def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() . .squeeze() is used to get rid of a trainiling (,1) in this case. . Gradient and Backward Pass . Mathematically the Backward Pass uses the chain rule to compute all of the gradients. . In order to Backprop effectively, we need to calc the gradients of all of our components. In our case these are our loss, activation functions (only ReLu, which is easy) and our linear layers. . I suggest CS 231n by Andrej Karpathy for mathematical explanation of Backprop. . Let’s start with MSE : . def mse_grad(inp, targ): # grad of loss with respect to output of previous layer inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0] . Relu : . def relu_grad(inp, out): # grad of relu with respect to input activations inp.g = (inp&gt;0).float() * out.g . Very simple, the gradient is either 0 or 1. In the Leaky Relu Case it’s either -0.01 or 1. . Linear Layers . def lin_grad(inp, out, w, b): # grad of matmul with respect to input inp.g = out.g @ w.t() #matrix prod with the transpose w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0) b.g = out.g.sum(0) . Forward and Backward Pass . def forward_and_backward(inp, targ): # forward pass: l1 = inp @ w1 + b1 l2 = relu(l1) out = l2 @ w2 + b2 # we don&#39;t actually need the loss in backward! loss = mse(out, targ) # backward pass, just reverse order: mse_grad(out, targ) lin_grad(l2, out, w2, b2) relu_grad(l1, l2) lin_grad(inp, l1, w1, b1) . In order to check our results, we can use Pytorch : . We can control our results with Pytorch auto_grad() function . xt2 = x_train.clone().requires_grad_(True) w12 = w1.clone().requires_grad_(True) w22 = w2.clone().requires_grad_(True) b12 = b1.clone().requires_grad_(True) b22 = b2.clone().requires_grad_(True) . .requires_grad(True) turns a tensor in to an autograd so it can keep track of each step . Refactor . It’s always good to refactor our code. This can be done by creating classes and using our functions. One for forward and one for backward pass. . class Relu(): def __call__(self, inp): self.inp = inp self.out = inp.clamp_min(0.)-0.5 return self.out def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g . class Lin(): def __init__(self, w, b): self.w,self.b = w,b def __call__(self, inp): self.inp = inp self.out = inp@self.w + self.b return self.out def backward(self): self.inp.g = self.out.g @ self.w.t() # Creating a giant outer product, just to sum it, is inefficient! self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0) self.b.g = self.out.g.sum(0) . class Mse(): def __call__(self, inp, targ): self.inp = inp self.targ = targ self.out = (inp.squeeze() - targ).pow(2).mean() return self.out def backward(self): self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0] . Lastly we create a model class. . class Model(): def __init__(self, w1, b1, w2, b2): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . The execution times is to slow and we want to avoid the call() declarations so we define a module class . further Refactor . class Module(): def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&#39;not implemented&#39;) def backward(self): self.bwd(self.out, *self.args) . class Relu(Module): def forward(self, inp): return inp.clamp_min(0.)-0.5 def bwd(self, out, inp): inp.g = (inp&gt;0).float() * out.g . class Lin(Module): def __init__(self, w, b): self.w,self.b = w,b def forward(self, inp): return inp@self.w + self.b def bwd(self, out, inp): inp.g = out.g @ self.w.t() self.w.g = torch.einsum(&quot;bi,bj-&gt;ij&quot;, inp, out.g) self.b.g = out.g.sum(0) . class Mse(Module): def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean() def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0] . class Model(): def __init__(self): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . Now we can call the forward and backprop passes for our model easily. . Summary . To summarize we implemented nn.Linear and nn.Module and will be able to write the train loop next lesson ! .",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/04/01/DL-From-Foundations-Part1.html",
            "relUrl": "/markdown/2020/04/01/DL-From-Foundations-Part1.html",
            "date": " • Apr 1, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://cedric-perauer.github.io/DL_from_Foundations/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://cedric-perauer.github.io/DL_from_Foundations/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

}